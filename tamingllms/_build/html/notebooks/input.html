<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

      <title>5. Managing Input Data</title>
    
          <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../_static/theme.css " type="text/css" />
          <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-thebe.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/scripts/sphinx-book-theme.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../_static/design-tabs.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
        <script async="async" src="../_static/sphinx-thebe.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../_static/theme-vendors.js"></script> -->
      <script src="../_static/theme.js" defer></script>
    
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="6. Safety" href="safety.html" />
  <link rel="prev" title="4. Structured Output" href="structured_output.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../markdown/toc.html" class="home-link">
    
      <span class="site-name">Taming LLMs</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



  
    <div class="nav-item">
      <a href="https://tamingllm.substack.com/"
        class="nav-link external">
          Newsletter <outboundlink></outboundlink>
      </a>
    </div>
  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



  
    <div class="nav-item">
      <a href="https://tamingllm.substack.com/"
        class="nav-link external">
          Newsletter <outboundlink></outboundlink>
      </a>
    </div>
  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../markdown/toc.html#taming-llms">taming llms</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="../markdown/preface.html" class="reference internal ">Preface</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../markdown/intro.html" class="reference internal ">About the Book</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="evals.html" class="reference internal ">The Evals Gap</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="structured_output.html" class="reference internal ">Structured Output</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">Managing Input Data</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#introduction" class="reference internal">Introduction</a></li>
                
                  <li class="toctree-l2"><a href="#parsing-documents" class="reference internal">Parsing Documents</a></li>
                
                  <li class="toctree-l2"><a href="#retrieval-augmented-generation" class="reference internal">Retrieval-Augmented Generation</a></li>
                
                  <li class="toctree-l2"><a href="#case-studies" class="reference internal">Case Studies</a></li>
                
                  <li class="toctree-l2"><a href="#conclusion" class="reference internal">Conclusion</a></li>
                
                  <li class="toctree-l2"><a href="#references" class="reference internal">References</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="safety.html" class="reference internal ">Safety</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="alignment.html" class="reference internal ">Preference-Based Alignment</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="local.html" class="reference internal ">Local LLMs in Practice</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="cost.html" class="reference internal ">The Falling Cost Paradox</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../markdown/toc.html">Docs</a> &raquo;</li>
    
    <li><span class="section-number">5. </span>Managing Input Data</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="structured_output.html"
       title="previous chapter">← <span class="section-number">4. </span>Structured Output</a>
  </li>
  <li class="next">
    <a href="safety.html"
       title="next chapter"><span class="section-number">6. </span>Safety →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section class="tex2jax_ignore mathjax_ignore" id="managing-input-data">
<span id="input"></span><h1><a class="toc-backref" href="#id215" role="doc-backlink"><span class="section-number">5. </span>Managing Input Data</a><a class="headerlink" href="#managing-input-data" title="Permalink to this heading">¶</a></h1>
<blockquote class="epigraph">
<div><p>One home run is much better than two doubles.</p>
<p class="attribution">—Steve Jobs</p>
</div></blockquote>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#managing-input-data" id="id215">Managing Input Data</a></p>
<ul>
<li><p><a class="reference internal" href="#introduction" id="id216">Introduction</a></p></li>
<li><p><a class="reference internal" href="#parsing-documents" id="id217">Parsing Documents</a></p>
<ul>
<li><p><a class="reference internal" href="#markitdown" id="id218">MarkItDown</a></p></li>
<li><p><a class="reference internal" href="#docling" id="id219">Docling</a></p></li>
<li><p><a class="reference internal" href="#frameworks-based-parsing" id="id220">Frameworks-Based Parsing</a></p></li>
<li><p><a class="reference internal" href="#structured-data-extraction" id="id221">Structured Data Extraction</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#retrieval-augmented-generation" id="id222">Retrieval-Augmented Generation</a></p></li>
<li><p><a class="reference internal" href="#case-studies" id="id223">Case Studies</a></p>
<ul>
<li><p><a class="reference internal" href="#case-study-i-content-chunking-with-contextual-linking" id="id224">Case Study I: Content Chunking with Contextual Linking</a></p>
<ul>
<li><p><a class="reference internal" href="#generating-long-form-content" id="id225">Generating long-form content</a></p></li>
<li><p><a class="reference internal" href="#discussion" id="id226">Discussion</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#case-study-ii-github-rag" id="id227">Case Study II: Github RAG</a></p></li>
<li><p><a class="reference internal" href="#case-study-iii-quiz-generation-with-citations" id="id228">Case Study III: Quiz Generation with Citations</a></p>
<ul>
<li><p><a class="reference internal" href="#use-case" id="id229">Use Case</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id230">Implementation</a></p></li>
<li><p><a class="reference internal" href="#example-usage" id="id231">Example Usage</a></p></li>
<li><p><a class="reference internal" href="#id14" id="id232">Discussion</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion" id="id233">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#references" id="id234">References</a></p></li>
</ul>
</li>
</ul>
</nav>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This Chapter is Work-in-Progress.</p>
</div>
<section id="introduction">
<h2><a class="toc-backref" href="#id216" role="doc-backlink"><span class="section-number">5.1. </span>Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>Large Language Models face several critical challenges in effectively processing input data. While advances in long-context language models (LCLMs) <span id="id1">[<a class="reference internal" href="#id104" title="Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more? 2024. URL: https://arxiv.org/abs/2406.13121, arXiv:2406.13121.">Lee <em>et al.</em>, 2024</a>]</span> have expanded the amount of information these systems can process simultaneously, significant challenges remain in managing and effectively utilizing extended inputs.</p>
<p>LLMs are sensitive to input formatting and structure, requiring careful data preparation to achieve optimal results <span id="id2">[<a class="reference internal" href="#id146" title="Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag: html is better than plain text for modeling retrieved knowledge in rag systems. 2024. URL: https://arxiv.org/abs/2411.02959, arXiv:2411.02959.">Tan <em>et al.</em>, 2024</a>]</span>. They operate with knowledge cutoffs, providing potentially stale or outdated information that may not reflect current reality and demonstrate problems with temporal knowledge accuracy <span id="id3">[<a class="reference internal" href="#id149" title="Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang. Knowledge of knowledge: exploring known-unknowns uncertainty with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, 6416–6432. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.findings-acl.383, doi:10.18653/v1/2024.findings-acl.383.">Amayuelas <em>et al.</em>, 2024</a>]</span>. LLMs also struggle with less common but important information showing a systematic loss of long-tail knowledge <span id="id4">[<a class="reference internal" href="#id150" title="Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic forgetting in language models via implicit inference. In The Twelfth International Conference on Learning Representations. 2024. URL: https://openreview.net/forum?id=VrHiF2hsrm.">Kotha <em>et al.</em>, 2024</a>]</span>.</p>
<p>Motivated by these challenges, this chapter explores two key components:</p>
<ol class="arabic simple">
<li><p>Data Parsing: Parsing documents into a unified format that is suitable for LLMs to process.</p></li>
<li><p>Retrieval Augmentation: Augmenting LLMs with the ability to retrieve relevant, recent, and specialized information.</p></li>
</ol>
<p>In data parsing, we will explore some useful open source tools that help transform data into LLM-compatible formats, demonstrating their impact through a case study of structured information extraction from complex PDFs. In a second case study, we will introduce some chunking strategies to help LLMs process long inputs and implement a particular technique called Chunking with Contextual Linking the enables contextually relevant chunk processing.</p>
<p>In retrieval augmentation, we will explore how to enhance LLMs with semantic search capabilities for incorporating external context using RAGs (Retrieval Augmented Generation). Through a detailed case study, we build a RAG system for querying live codebases, illustrating methods to bridge static model knowledge with dynamic information requirements.</p>
<p>In our last case study, we build a quiz generator using a LLM with large context window. We will explore some additional relevant techniques such as prompt caching and response verification through citations.</p>
<p>By the chapter’s conclusion, readers will possess relevant knowledge of input data management strategies for LLMs and practical expertise in selecting and implementing appropriate approaches and tools for specific use cases.</p>
</section>
<section id="parsing-documents">
<h2><a class="toc-backref" href="#id217" role="doc-backlink"><span class="section-number">5.2. </span>Parsing Documents</a><a class="headerlink" href="#parsing-documents" title="Permalink to this heading">¶</a></h2>
<p>Building robust data ingestion and preprocessing pipelines is essential for any LLM application. This section explores tools and frameworks that streamline input data processing, in particular for parsing purposes, providing a unified interface for converting diverse data formats into standardized representations that LLMs can effectively process. By abstracting away format-specific complexities, they allow developers to focus on core application logic rather than parsing implementation details while maximizing the performance of the LLM.</p>
<p>We will cover open source tools and frameworks that provide parsing capabilities for a wide range of data formats. And we will demonstrate how some of these tools can be used to extract structured information from complex PDFs also discussing how the quality of the parser can impact LLM’s performance.</p>
<section id="markitdown">
<h3><a class="toc-backref" href="#id218" role="doc-backlink"><span class="section-number">5.2.1. </span>MarkItDown</a><a class="headerlink" href="#markitdown" title="Permalink to this heading">¶</a></h3>
<p>MarkItDown is a Python package and CLI too developed by the Microsoft AutoGen team for converting various file formats to Markdown. It supports a wide range of formats including PDF, PowerPoint, Word, Excel, images (with OCR and EXIF metadata), audio (with transcription), HTML, and other text-based formats making it a useful tool for document indexing and LLM-based applications.</p>
<p>Key features:</p>
<ul class="simple">
<li><p>Simple command-line and Python API interfaces</p></li>
<li><p>Support for multiple file formats</p></li>
<li><p>Optional LLM integration for enhanced image descriptions</p></li>
<li><p>Batch processing capabilities</p></li>
<li><p>Docker support for containerized usage</p></li>
</ul>
<p>Sample usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">markitdown</span> <span class="kn">import</span> <span class="n">MarkItDown</span>

<span class="n">md</span> <span class="o">=</span> <span class="n">MarkItDown</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">md</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;test.xlsx&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">text_content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="docling">
<h3><a class="toc-backref" href="#id219" role="doc-backlink"><span class="section-number">5.2.2. </span>Docling</a><a class="headerlink" href="#docling" title="Permalink to this heading">¶</a></h3>
<p>Docling is a Python package developed by IBM Research for parsing and converting documents into various formats. It provides advanced document understanding capabilities with a focus on maintaining document structure and formatting.</p>
<p>Key features:</p>
<ul class="simple">
<li><p>Support for multiple document formats (PDF, DOCX, PPTX, XLSX, Images, HTML, etc.)</p></li>
<li><p>Advanced PDF parsing including layout analysis and table extraction</p></li>
<li><p>Unified document representation format</p></li>
<li><p>Integration with LlamaIndex and LangChain</p></li>
<li><p>OCR support for scanned documents</p></li>
<li><p>Simple CLI interface</p></li>
</ul>
<p>Sample usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">docling.document_converter</span> <span class="kn">import</span> <span class="n">DocumentConverter</span>

<span class="n">converter</span> <span class="o">=</span> <span class="n">DocumentConverter</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;document.pdf&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">document</span><span class="o">.</span><span class="n">export_to_markdown</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="frameworks-based-parsing">
<h3><a class="toc-backref" href="#id220" role="doc-backlink"><span class="section-number">5.2.3. </span>Frameworks-Based Parsing</a><a class="headerlink" href="#frameworks-based-parsing" title="Permalink to this heading">¶</a></h3>
</section>
<section id="structured-data-extraction">
<h3><a class="toc-backref" href="#id221" role="doc-backlink"><span class="section-number">5.2.4. </span>Structured Data Extraction</a><a class="headerlink" href="#structured-data-extraction" title="Permalink to this heading">¶</a></h3>
<p>A common use case where document parsing matters is to structured data extraction from documents, particularly in the presence of complex formatting and layout. In this case study, we will extract the economic forecasts from Merrill Lynch’s CIO Capital Market Outlook released on December 16, 2024 <span id="id5">[<a class="reference internal" href="#id105" title="Merrill Lynch. Chief investment officer capital market outlook. CIO Weekly Letter, 2024. URL: https://olui2.fs.ml.com/publish/content/application/pdf/gwmol/me-cio-weekly-letter.pdf.">Merrill Lynch, 2024</a>]</span>. We will focus on page 7 of this document, which contains several economic variables organized in a mix of tables, text and images (see <a class="reference internal" href="#forecast"><span class="std std-numref">Fig. 5.1</span></a>)</p>
<figure class="align-center" id="forecast">
<a class="reference internal image-reference" href="../_images/forecast.png"><img alt="Forecast" src="../_images/forecast.png" style="width: 897.5px; height: 953.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.1 </span><span class="caption-text">Forecast</span><a class="headerlink" href="#forecast" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FORECAST_FILE_PATH</span> <span class="o">=</span> <span class="s2">&quot;../data/input/forecast.pdf&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>First, we will use MarkItDown to extract the text content from the document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">markitdown</span> <span class="kn">import</span> <span class="n">MarkItDown</span>

<span class="n">md</span> <span class="o">=</span> <span class="n">MarkItDown</span><span class="p">()</span>
<span class="n">result_md</span> <span class="o">=</span> <span class="n">md</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">FORECAST_FILE_PATH</span><span class="p">)</span><span class="o">.</span><span class="n">text_content</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we will do the same with Docling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">docling.document_converter</span> <span class="kn">import</span> <span class="n">DocumentConverter</span>

<span class="n">converter</span> <span class="o">=</span> <span class="n">DocumentConverter</span><span class="p">()</span>
<span class="n">forecast_result_docling</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">source</span><span class="p">)</span><span class="o">.</span><span class="n">document</span><span class="o">.</span><span class="n">export_to_markdown</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>How similar are the two results? We can use use Levenshtein distance to measure the similarity between the two results. We will also calculate a naive score using the <code class="docutils literal notranslate"><span class="pre">SequenceMatcher</span></code> from the <code class="docutils literal notranslate"><span class="pre">difflib</span></code> package, which is a simple measure of the similarity between two strings based on the number of matches in the longest common subsequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">Levenshtein</span>
<span class="k">def</span> <span class="nf">levenshtein_similarity</span><span class="p">(</span><span class="n">text1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">text2</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate normalized Levenshtein distance</span>
<span class="sd">    Returns value between 0 (completely different) and 1 (identical)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">Levenshtein</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">text2</span><span class="p">))</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">distance</span> <span class="o">/</span> <span class="n">max_len</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">difflib</span> <span class="kn">import</span> <span class="n">SequenceMatcher</span>
<span class="k">def</span> <span class="nf">simple_similarity</span><span class="p">(</span><span class="n">text1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">text2</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate similarity ratio using SequenceMatcher</span>
<span class="sd">    Returns value between 0 (completely different) and 1 (identical)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">SequenceMatcher</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span><span class="o">.</span><span class="n">ratio</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">levenshtein_similarity</span><span class="p">(</span><span class="n">forecast_result_md</span><span class="p">,</span> <span class="n">forecast_result_docling</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.13985705461925346
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simple_similarity</span><span class="p">(</span><span class="n">forecast_result_md</span><span class="p">,</span> <span class="n">forecast_result_docling</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.17779960707269155
</pre></div>
</div>
</div>
</div>
<p>It turns out that the two results are quite different, with a similarity score of about 13.98% and 17.77% for Levenshtein and <code class="docutils literal notranslate"><span class="pre">SequenceMatcher</span></code> respectively.</p>
<p>Docling’s result is a quite readable markdown displaying key economic variables and their forecasts. Conversely, MarkItDown’s result is a bit messy and hard to read but the information is there just not in a structured format. Does it matter? That’s what we will explore next.</p>
<p><strong>Docling’s result</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">forecast_result_docling</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference internal" href="#id6"><span class="std std-numref">Fig. 5.2</span></a> shows part of the parsed result from Docling.</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="../_images/docling.png"><img alt="Docling's result" src="../_images/docling.png" style="width: 1106.3999999999999px; height: 661.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.2 </span><span class="caption-text">Docling’s parsed result</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>MarkItDown’s result</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">forecast_result_md</span><span class="p">[:</span><span class="mi">500</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference internal" href="#id7"><span class="std std-numref">Fig. 5.3</span></a> shows part of the parsed result from MarkItDown.</p>
<figure class="align-center" id="id7">
<a class="reference internal image-reference" href="../_images/markitdown.png"><img alt="MarkItDown's parsed result" src="../_images/markitdown.png" style="width: 1287.0px; height: 567.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.3 </span><span class="caption-text">MarkItDown’s parsed result</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Now, let’s focus on the economic forecasts. In particular, we are interested in extracting the CIO’s 2025E forecasts.</p>
<figure class="align-center" id="forecast2025">
<a class="reference internal image-reference" href="../_images/2025.png"><img alt="Forecast 2025" src="../_images/2025.png" style="width: 1010.25px; height: 293.85px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.4 </span><span class="caption-text">Forecast 2025</span><a class="headerlink" href="#forecast2025" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We will define a <code class="docutils literal notranslate"><span class="pre">Forecast</span></code> pydantic model to represent an economic forecast composed of a <code class="docutils literal notranslate"><span class="pre">financial_variable</span></code> and a <code class="docutils literal notranslate"><span class="pre">financial_forecast</span></code>. We will also define a <code class="docutils literal notranslate"><span class="pre">EconForecast</span></code> pydantic model to represent the list of economic forecasts we want to extract from the document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="k">class</span> <span class="nc">Forecast</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">financial_variable</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">financial_forecast</span><span class="p">:</span> <span class="nb">float</span>
<span class="k">class</span> <span class="nc">EconForecast</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">forecasts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Forecast</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We write a simple function to extract the economic forecasts from the document using an LLM model (with structured output) with the following prompt template, where <code class="docutils literal notranslate"><span class="pre">extract_prompt</span></code> is kind of data the user would like to extract and <code class="docutils literal notranslate"><span class="pre">doc</span></code> is the input document to analyze.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BASE_PROMPT</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: You are an expert at structured data extraction. </span>
<span class="s2">    TASK: Extract the following data </span><span class="si">{</span><span class="n">extract_prompt</span><span class="si">}</span><span class="s2"> from input DOCUMENT</span>
<span class="s2">    FORMAT: The output should be a JSON object with &#39;financial_variable&#39; as key and &#39;financial_forecast&#39; as value.</span>
<span class="s2">    &quot;&quot;&quot;</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BASE_PROMPT</span><span class="si">}</span><span class="s2"> </span><span class="se">\n\n</span><span class="s2"> DOCUMENT: </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>  <span class="n">doc</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EconForecast</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract data of a financial document using an LLM model.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        doc: The financial document text to analyze</span>
<span class="sd">        client: The LLM model to use for analysis</span>
<span class="sd">        extract_prompt: The prompt to use for extraction</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        EconForecasts object containing sentiment analysis results</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">BASE_PROMPT</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: You are an expert at structured data extraction. </span>
<span class="s2">    TASK: Extract the following data </span><span class="si">{</span><span class="n">extract_prompt</span><span class="si">}</span><span class="s2"> from input DOCUMENT</span>
<span class="s2">    FORMAT: The output should be a JSON object with &#39;financial_variable&#39; as key and &#39;financial_forecast&#39; as value.</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BASE_PROMPT</span><span class="si">}</span><span class="s2"> </span><span class="se">\n\n</span><span class="s2"> DOCUMENT: </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">doc</span><span class="p">}</span>
        <span class="p">],</span>
        <span class="n">response_format</span><span class="o">=</span><span class="n">EconForecast</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">parsed</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Load environment variables from .env file</span>
<span class="n">load_dotenv</span><span class="p">(</span><span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The user then calls the <code class="docutils literal notranslate"><span class="pre">extract_from_doc</span></code> function simply defining that “Economic Forecasts for 2025E” is the data they would like to extract from the document. We perform the extraction twice, once with MarkItDown and once with Docling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">extract_prompt</span> <span class="o">=</span> <span class="s2">&quot;Economic Forecasts for 2025E&quot;</span>
<span class="n">md_financials</span> <span class="o">=</span> <span class="n">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">,</span> <span class="n">forecast_result_md</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
<span class="n">docling_financials</span> <span class="o">=</span> <span class="n">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">,</span> <span class="n">forecast_result_docling</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The response is an <code class="docutils literal notranslate"><span class="pre">EconForecast</span></code> object containing a list of <code class="docutils literal notranslate"><span class="pre">Forecast</span></code> objects, as defined in the pydantic model. We can then convert the response to a pandas DataFrame for easier comparison.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">md_financials</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>EconForecast(forecasts=[Forecast(financial_variable=&#39;Real global GDP (% y/y annualized)&#39;, financial_forecast=3.2), Forecast(financial_variable=&#39;Real U.S. GDP (% q/q annualized)&#39;, financial_forecast=2.4), Forecast(financial_variable=&#39;CPI inflation (% y/y)&#39;, financial_forecast=2.5), Forecast(financial_variable=&#39;Core CPI inflation (% y/y)&#39;, financial_forecast=3.0), Forecast(financial_variable=&#39;Unemployment rate (%)&#39;, financial_forecast=4.3), Forecast(financial_variable=&#39;Fed funds rate, end period (%)&#39;, financial_forecast=3.88)])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_md_forecasts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">f</span><span class="o">.</span><span class="n">financial_variable</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">financial_forecast</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">md_financials</span><span class="o">.</span><span class="n">forecasts</span><span class="p">],</span> 
                      <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;Forecast&#39;</span><span class="p">])</span>
<span class="n">df_docling_forecasts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">f</span><span class="o">.</span><span class="n">financial_variable</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">financial_forecast</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">docling_financials</span><span class="o">.</span><span class="n">forecasts</span><span class="p">],</span> 
                      <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;Forecast&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_md_forecasts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Variable</th>
      <th>Forecast</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Real global GDP (% y/y annualized)</td>
      <td>3.20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Real U.S. GDP (% q/q annualized)</td>
      <td>2.40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CPI inflation (% y/y)</td>
      <td>2.50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Core CPI inflation (% y/y)</td>
      <td>3.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Unemployment rate (%)</td>
      <td>4.30</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Fed funds rate, end period (%)</td>
      <td>3.88</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_docling_forecasts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Variable</th>
      <th>Forecast</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Real global GDP (% y/y annualized)</td>
      <td>3.20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Real U.S. GDP (% q/q annualized)</td>
      <td>2.40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CPI inflation (% y/y)</td>
      <td>2.50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Core CPI inflation (% y/y)</td>
      <td>3.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Unemployment rate (%)</td>
      <td>4.30</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Fed funds rate, end period (%)</td>
      <td>3.88</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The results from MarkItDown and Docling are identical and accurately match the true values from the document. This demonstrates that despite MarkItDown’s output appearing less readable from a human perspective, both approaches enabled the LLM to successfully extract the economic forecast data with equal accuracy, in this particular case.</p>
<p>Now, let’s focus on the asset class weightings. We will extract the asset class weightings from the document and compare the results from MarkItDown and Docling. The information now is presented in a quite different structure. The CIO view information is represented in a spectrum from starting with “Underweight”, passing through “Neutral” and reaching “Overweight”. The actual view is marked by some colored dots in the chart. Let’s see if we can extract this information from the document.</p>
<figure class="align-center" id="asset-class">
<a class="reference internal image-reference" href="../_images/asset_class.png"><img alt="Asset Class Weightings" src="../_images/asset_class.png" style="width: 575.0px; height: 739.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.5 </span><span class="caption-text">Asset Class Weightings</span><a class="headerlink" href="#asset-class" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The user will simply define the following data to extract: “Asset Class Weightings (as of 12/3/2024) in a scale from -2 to 2”. In that way, we expect that “Underweight” will be mapped to -2, “Neutral” to 0 and “Overweight” to 2 with some values in between.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">extract_prompt</span> <span class="o">=</span> <span class="s2">&quot;Asset Class Weightings (as of 12/3/2024) in a scale from -2 to 2&quot;</span>
<span class="n">asset_class_docling</span> <span class="o">=</span> <span class="n">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">,</span> <span class="n">forecast_result_docling</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
<span class="n">asset_class_md</span> <span class="o">=</span> <span class="n">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">,</span> <span class="n">forecast_result_md</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_md</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">f</span><span class="o">.</span><span class="n">financial_variable</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">financial_forecast</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">asset_class_md</span><span class="o">.</span><span class="n">forecasts</span><span class="p">],</span> 
                 <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;Forecast&#39;</span><span class="p">])</span>
<span class="n">df_docling</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">f</span><span class="o">.</span><span class="n">financial_variable</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">financial_forecast</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">asset_class_docling</span><span class="o">.</span><span class="n">forecasts</span><span class="p">],</span> 
                 <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;Forecast&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Now we construct a DataFrame to compare the results from MarkItDown and Docling with an added “true_value” column containing the true values from the document, which we extracted manually from the chart.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create DataFrame with specified columns</span>
<span class="n">df_comparison</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;variable&#39;</span><span class="p">:</span> <span class="n">df_docling</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="s1">&#39;markitdown&#39;</span><span class="p">:</span> <span class="n">df_md</span><span class="p">[</span><span class="s1">&#39;Forecast&#39;</span><span class="p">],</span>
    <span class="s1">&#39;docling&#39;</span><span class="p">:</span> <span class="n">df_docling</span><span class="p">[</span><span class="s1">&#39;Forecast&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Drop last row</span>
    <span class="s1">&#39;true_value&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">display</span><span class="p">(</span><span class="n">df_comparison</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variable</th>
      <th>markitdown</th>
      <th>docling</th>
      <th>true_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Global Equities</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>U.S. Large Cap Growth</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>U.S. Large Cap Value</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>U.S. Small Cap Growth</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>U.S. Small Cap Value</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>International Developed</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Emerging Markets</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Global Fixed Income</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>U.S. Governments</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>U.S. Mortgages</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>U.S. Corporates</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>International Fixed Income</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>High Yield</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>U.S. Investment-grade</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Tax Exempt U.S. High Yield Tax Exempt</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate accuracy for markitdown and docling</span>
<span class="n">markitdown_accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_comparison</span><span class="p">[</span><span class="s1">&#39;markitdown&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">df_comparison</span><span class="p">[</span><span class="s1">&#39;true_value&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">docling_accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_comparison</span><span class="p">[</span><span class="s1">&#39;docling&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">df_comparison</span><span class="p">[</span><span class="s1">&#39;true_value&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Markitdown accuracy: </span><span class="si">{</span><span class="n">markitdown_accuracy</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Docling accuracy: </span><span class="si">{</span><span class="n">docling_accuracy</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Markitdown accuracy: 53.33%
Docling accuracy: 93.33%
</pre></div>
</div>
</div>
</div>
<p>Docling performs significantly better at 93.33% accuracy missing only one value. MarkItDown achieves 53.33% accuracy, struggling with nuanced asset class weightings. In this case, Docling’s structured parsed output did help the LLM to extract the information more accurately compared to MarkItDown’s unstructured output. Hence, in this case, the strategy used to parse the data did impact the LLM’s ability to extract the information. A more robust analysis would run data extraction on a large sample data a number of repeated runs to estimate error rates.</p>
<p>What if we want to systematically extract all tables from the document? We can use Docling to do that by simply accessing the <code class="docutils literal notranslate"><span class="pre">tables</span></code> attribute of the <code class="docutils literal notranslate"><span class="pre">DocumentConverter</span></code> object.</p>
<p>By doing that, we observe that Docling extracted 7 tables from the document. Exporting tables from top down and left to right in order of appearance in the document.
Below, we can see the first table successfully extracted for Equities forecasts, the second one for Fixed Income forecasts as well as the last table, which contains CIO Equity Sector Views.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">docling.document_converter</span> <span class="kn">import</span> <span class="n">DocumentConverter</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">convert_and_export_tables</span><span class="p">(</span><span class="n">file_path</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert document and export tables to DataFrames.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        file_path: Path to input document</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        List of pandas DataFrames containing the tables</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">doc_converter</span> <span class="o">=</span> <span class="n">DocumentConverter</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    
    <span class="n">conv_res</span> <span class="o">=</span> <span class="n">doc_converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    
    <span class="n">tables</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Export tables</span>
    <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">conv_res</span><span class="o">.</span><span class="n">document</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="n">table_df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">export_to_dataframe</span><span class="p">()</span>
        <span class="n">tables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">table_df</span><span class="p">)</span>

    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Document converted in </span><span class="si">{</span><span class="n">end_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">tables</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert and export tables</span>
<span class="n">tables</span> <span class="o">=</span> <span class="n">convert_and_export_tables</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">FORECAST_FILE_PATH</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">tables</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Total Return in USD (%).Current</th>
      <th>Total Return in USD (%).WTD</th>
      <th>Total Return in USD (%).MTD</th>
      <th>Total Return in USD (%).YTD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>DJIA</td>
      <td>43,828.06</td>
      <td>-1.8</td>
      <td>-2.3</td>
      <td>18.4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NASDAQ</td>
      <td>19,926.72</td>
      <td>0.4</td>
      <td>3.7</td>
      <td>33.7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S&amp;P 500</td>
      <td>6,051.09</td>
      <td>-0.6</td>
      <td>0.4</td>
      <td>28.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>S&amp;P 400 Mid Cap</td>
      <td>3,277.20</td>
      <td>-1.6</td>
      <td>-2.6</td>
      <td>19.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Russell 2000</td>
      <td>2,346.90</td>
      <td>-2.5</td>
      <td>-3.5</td>
      <td>17.3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>MSCI World</td>
      <td>3,817.24</td>
      <td>-1.0</td>
      <td>0.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>MSCI EAFE</td>
      <td>2,319.05</td>
      <td>-1.5</td>
      <td>0.2</td>
      <td>6.4</td>
    </tr>
    <tr>
      <th>7</th>
      <td>MSCI Emerging Markets</td>
      <td>1,107.01</td>
      <td>0.3</td>
      <td>2.7</td>
      <td>10.6</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Total Return in USD (%).Current</th>
      <th>Total Return in USD (%).WTD</th>
      <th>Total Return in USD (%).MTD</th>
      <th>Total Return in USD (%).YTD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Corporate &amp; Government</td>
      <td>4.66</td>
      <td>-1.34</td>
      <td>-0.92</td>
      <td>1.94</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Agencies</td>
      <td>4.54</td>
      <td>-0.58</td>
      <td>-0.31</td>
      <td>3.35</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Municipals</td>
      <td>3.55</td>
      <td>-0.87</td>
      <td>-0.54</td>
      <td>1.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>U.S. Investment Grade Credit</td>
      <td>4.79</td>
      <td>-1.38</td>
      <td>-0.93</td>
      <td>1.97</td>
    </tr>
    <tr>
      <th>4</th>
      <td>International</td>
      <td>5.17</td>
      <td>-1.40</td>
      <td>-0.90</td>
      <td>3.20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>High Yield</td>
      <td>7.19</td>
      <td>-0.22</td>
      <td>0.20</td>
      <td>8.87</td>
    </tr>
    <tr>
      <th>6</th>
      <td>90 Day Yield</td>
      <td>4.32</td>
      <td>4.39</td>
      <td>4.49</td>
      <td>5.33</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2 Year Yield</td>
      <td>4.24</td>
      <td>4.10</td>
      <td>4.15</td>
      <td>4.25</td>
    </tr>
    <tr>
      <th>8</th>
      <td>10 Year Yield</td>
      <td>4.40</td>
      <td>4.15</td>
      <td>4.17</td>
      <td>3.88</td>
    </tr>
    <tr>
      <th>9</th>
      <td>30 Year Yield</td>
      <td>4.60</td>
      <td>4.34</td>
      <td>4.36</td>
      <td>4.03</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">tables</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sector</th>
      <th>CIO View.</th>
      <th>CIO View.Underweight</th>
      <th>CIO View.Neutral</th>
      <th>CIO View.</th>
      <th>CIO View.Overweight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Utilities</td>
      <td>slight over weight green   </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>Financials</td>
      <td>slight over weight green   </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>Healthcare</td>
      <td>slight over weight green   </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>Consumer  Discretionary</td>
      <td>Slight over weight green  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>Information  Technology</td>
      <td>Neutral yellow  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>5</th>
      <td>Communication  Services</td>
      <td>Neutral yellow  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>6</th>
      <td>Industrials</td>
      <td>Neutral yellow  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>7</th>
      <td>Real Estate</td>
      <td>Neutral yellow  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>8</th>
      <td>Energy</td>
      <td>slight underweight orange  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>9</th>
      <td>Materials</td>
      <td>slight underweight orange  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>10</th>
      <td>Consumer  Staples</td>
      <td>underweight red</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Coming back to MarkItDown, one interesting feature to explore is the ability to extract information from images by passing an image capable LLM model to its constructor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">md_llm</span> <span class="o">=</span> <span class="n">MarkItDown</span><span class="p">(</span><span class="n">llm_client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span> <span class="n">llm_model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">md_llm</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;../data/input/forecast.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the description we obtain from the image of our input document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">text_content</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<h1 class="rubric" id="description">Description:</h1>
<p><strong>Markets in Review: Economic Forecasts and Asset Class Weightings (as of 12/13/2024)</strong></p>
<p>This detailed market overview presents key performance metrics and economic forecasts as of December 13, 2024.</p>
<p><strong>Equities Overview:</strong></p>
<ul class="simple">
<li><p><strong>Total Returns:</strong> Highlights returns for major indices such as the DJIA (18.4% YTD), NASDAQ (33.7% YTD), and S&amp;P 500 (28.6% YTD), showcasing strong performance across the board.</p></li>
<li><p><strong>Forecasts:</strong> Economic indicators reveal a projected real global GDP growth of 3.1%, with inflation rates expected to stabilize around 2.2% in 2025. Unemployment rates are anticipated to remain low at 4.4%.</p></li>
</ul>
<p><strong>Fixed Income:</strong></p>
<ul class="simple">
<li><p>Focuses on various segments, including Corporate &amp; Government bonds, which offer an annualized return of 4.66% and indicate shifting trends in interest rates over 2-Year (4.25%) and 10-Year (4.03%) bonds.</p></li>
</ul>
<p><strong>Commodities &amp; Currencies:</strong></p>
<ul class="simple">
<li><p>Commodities such as crude oil and gold show varied performance, with oil increasing by 4.8% and gold prices sitting at $2,648.23 per ounce.</p></li>
<li><p>Currency metrics highlight the Euro and USD trends over the past year.</p></li>
</ul>
<p><strong>S&amp;P Sector Returns:</strong></p>
<ul class="simple">
<li><p>A quick reference for sector performance indicates a significant 2.5% return in Communication Services, while other sectors like Consumer Staples and Materials display minor fluctuations.</p></li>
</ul>
<p><strong>CIO Asset Class Weightings:</strong></p>
<ul class="simple">
<li><p>Emphasizes strategic asset allocation recommendations which are crucial for an investor’s portfolio. Underweight positions in U.S. Small Cap Growth and International Developed contrast with overweight positions in certain sectors such as Utilities and Financials, signaling tactical shifts based on ongoing economic assessments.</p></li>
</ul>
<p><strong>Note:</strong> This summary is sourced from BofA Global Research and aims to provide a comprehensive view of current market conditions and forecasts to assist investors in making informed decisions.</p>
</div>
</div>
<hr class="docutils" />
<p>Overall, the description is somewhat accurate but contains a few inaccuracies including:</p>
<ul class="simple">
<li><p>For the sector weightings, the description states there are “underweight positions in U.S. Small Cap Growth” but looking at the Asset Class Weightings chart, U.S. Small Cap Growth actually shows an overweight position (green circle).</p></li>
<li><p>The description mentions “overweight positions in certain sectors such as Utilities and Financials” but looking at the CIO Equity Sector Views, both these sectors show neutral positions, not overweight positions.</p></li>
<li><p>For fixed income, the description cites a “10-Year (4.03%)” yield, but the image shows the 30-Year Yield at 4.03%, while the 10-Year Yield is actually 4.40%.</p></li>
</ul>
<p>Arguably, the description’s inaccuracies could be a consequence of the underlying LLM model’s inability to process the image. Further research is needed to determine if this is the case.</p>
</section>
</section>
<section id="retrieval-augmented-generation">
<h2><a class="toc-backref" href="#id222" role="doc-backlink"><span class="section-number">5.3. </span>Retrieval-Augmented Generation</a><a class="headerlink" href="#retrieval-augmented-generation" title="Permalink to this heading">¶</a></h2>
<p>RAG is a technique that allows LLMs to retrieve information from a knowledge base to answer questions. It is a popular technique for building LLM applications that require knowledge-intensive tasks <span id="id8">[<a class="reference internal" href="#id102" title="Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. 2021. URL: https://arxiv.org/abs/2005.11401, arXiv:2005.11401.">Lewis <em>et al.</em>, 2021</a>]</span>.</p>
<p>RAG utilizes a retrieval system to fetch external knowledge and augment the LLM. It has proved effective in mitigating hallucinations of LLMs <span id="id9">[<a class="reference internal" href="#id151" title="Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. When do LLMs need retrieval augmentation? mitigating LLMs' overconfidence helps retrieval augmentation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, 11375–11388. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.findings-acl.675, doi:10.18653/v1/2024.findings-acl.675.">Ni <em>et al.</em>, 2024</a>, <a class="reference internal" href="#id145" title="Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large language models. In Proceedings of the ACM Web Conference 2024, WWW '24, 1453-1463. New York, NY, USA, 2024. Association for Computing Machinery. URL: https://doi.org/10.1145/3589334.3645481, doi:10.1145/3589334.3645481.">Zhou <em>et al.</em>, 2024</a>]</span>.</p>
</section>
<section id="case-studies">
<h2><a class="toc-backref" href="#id223" role="doc-backlink"><span class="section-number">5.4. </span>Case Studies</a><a class="headerlink" href="#case-studies" title="Permalink to this heading">¶</a></h2>
<p>This section presents three case studies that demonstrate practical solutions to common LLM limitations:</p>
<p>First, Content Chunking with Contextual Linking showcases how intelligent chunking strategies can overcome both context window and output token limitations. This case study illustrates techniques for breaking down and reassembling content while maintaining coherence, enabling the generation of high-quality long-form outputs despite model constraints.</p>
<p>Second, a Retrieval Augmented Generation case study addresses the challenge of stale or outdated model knowledge. By implementing semantic search over a GitHub repository, this example demonstrates how to augment LLM responses with current, accurate information - allowing users to query and receive up-to-date answers about code repository contents.</p>
<p>Third, the final case study builds a Quiz generator with citations. This case study explores some additional input management techniques that become particularly useful when long context window is available. This includes implementing prompt caching for efficiency and adding citations to enhance response accuracy and verifiability. These approaches show how to maximize the benefits of larger context models while maintaining response quality.</p>
<section id="case-study-i-content-chunking-with-contextual-linking">
<h3><a class="toc-backref" href="#id224" role="doc-backlink"><span class="section-number">5.4.1. </span>Case Study I: Content Chunking with Contextual Linking</a><a class="headerlink" href="#case-study-i-content-chunking-with-contextual-linking" title="Permalink to this heading">¶</a></h3>
<p>Content chunking with contextual linking is a technique to break down long-form content into smaller, manageable chunks while keeping chunk-specific context. This approach tackles three problems:</p>
<ol class="arabic simple">
<li><p>The LLM’s inability to process long inputs to do context-size limits</p></li>
<li><p>The LLM’s inability to generate long-form content due to the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limitation.</p></li>
<li><p>The LLM’s inability to maintain coherence and context when generating responses per chunks</p></li>
</ol>
<p>Here, we exemplify this technique by following these steps:</p>
<ol class="arabic simple">
<li><p><strong>Chunking the Content</strong>: The input content is split into smaller chunks. This allows the LLM to process each chunk individually, focusing on generating a complete and detailed response for that specific section of the input.</p></li>
<li><p><strong>Maintaining Context</strong>: Each chunk is linked with contextual information from the previous chunks. This helps in maintaining the flow and coherence of the content across multiple chunks.</p></li>
<li><p><strong>Generating Linked Prompts</strong>: For each chunk, a prompt is generated that includes the chunk’s content and its context. This prompt is then used to generate the output for that chunk.</p></li>
<li><p><strong>Combining the Outputs</strong>: The outputs of all chunks are combined to form the final long-form content.</p></li>
</ol>
<p>Let’s examine an example implementation of this technique.</p>
<section id="generating-long-form-content">
<h4><a class="toc-backref" href="#id225" role="doc-backlink"><span class="section-number">5.4.1.1. </span>Generating long-form content</a><a class="headerlink" href="#generating-long-form-content" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Goal: Generate a long-form report analyzing a company’s financial statement.</p></li>
<li><p>Input: A company’s 10K SEC filing.</p></li>
</ul>
<figure class="align-center" id="content-chunking-with-contextual-linking">
<a class="reference internal image-reference" href="../_images/diagram1.png"><img alt="Content Chunking with Contextual Linking" src="../_images/diagram1.png" style="width: 819.0px; height: 1725.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.6 </span><span class="caption-text">Content Chunking with Contextual Linking Schematic Representation.</span><a class="headerlink" href="#content-chunking-with-contextual-linking" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The diagram in <a class="reference internal" href="#content-chunking-with-contextual-linking"><span class="std std-numref">Fig. 5.6</span></a> illustrates the process we will follow for handling long-form content generation with Large Language Models through “Content Chunking with Contextual Linking.” It shows how input content is first split into manageable chunks using a chunking function (e.g. <code class="docutils literal notranslate"><span class="pre">CharacterTextSplitter</span></code> with <code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> tokenizer), then each chunk is processed sequentially while maintaining context from previous chunks. For each chunk, the system updates the context, generates a dynamic prompt with specific parameters, makes a call to the LLM chain, and stores the response. After all chunks are processed, the individual responses are combined with newlines to create the final report, effectively working around the token limit constraints of LLMs while maintaining coherence across the generated content.</p>
<p><strong>Step 1: Chunking the Content</strong></p>
<p>There are different methods for chunking, and each of them might be appropriate for different situations. However, we can broadly group chunking strategies in two types:</p>
<ul>
<li><p><strong>Fixed-size Chunking</strong>: This is the most common and straightforward approach to chunking. We simply decide the number of tokens in our chunk and, optionally, whether there should be any overlap between them. In general, we will want to keep some overlap between chunks to make sure that the semantic context doesn’t get lost between chunks. Fixed-sized chunking may be a reasonable path in many common cases. Compared to other forms of chunking, fixed-sized chunking is computationally cheap and simple to use since it doesn’t require the use of any specialied techniques or libraries.</p></li>
<li><p><strong>Content-aware Chunking</strong>: These are a set of methods for taking advantage of the nature of the content we’re chunking and applying more sophisticated chunking to it. Examples include:</p>
<ul class="simple">
<li><p><strong>Sentence Splitting</strong>: Many models are optimized for embedding sentence-level content. Naturally, we would use sentence chunking, and there are several approaches and tools available to do this, including naive splitting (e.g. splitting on periods), NLTK, and spaCy.</p></li>
<li><p><strong>Recursive Chunking</strong>: Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators.</p></li>
<li><p><strong>Semantic Chunking</strong>: This is a class of methods that leverages embeddings to extract the semantic meaning present in your data, creating chunks that are made up of sentences that talk about the same theme or topic.</p></li>
</ul>
<p>Here, we will utilize <code class="docutils literal notranslate"><span class="pre">langchain</span></code> for a content-aware sentence-splitting strategy for chunking. Langchain offers several text splitters <span id="id10">[<a class="reference internal" href="#id53" title="LangChain. Text splitters - langchain documentation. https://python.langchain.com/docs/how_to/#text-splitters, 2024. Accessed: 12/07/2024.">LangChain, 2024</a>]</span> such as JSON-, Markdown- and HTML-based or split by token. We will use the <code class="docutils literal notranslate"><span class="pre">CharacterTextSplitter</span></code> with <code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> as our tokenizer to count the number of tokens per chunk which we can use to ensure that we do not surpass the input token limit of our model.</p>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_chunks</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Split input text into chunks of specified size with specified overlap.</span>

<span class="sd">    Args:</span>
<span class="sd">        text (str): The input text to be chunked.</span>
<span class="sd">        chunk_size (int): The maximum size of each chunk in tokens.</span>
<span class="sd">        chunk_overlap (int): The number of tokens to overlap between chunks.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list: A list of text chunks.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>

    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="o">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">chunk_overlap</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Step 2: Writing the Base Prompt Template</strong></p>
<p>We will write a base prompt template which will serve as a foundational structure for all chunks, ensuring consistency in the instructions and context provided to the language model. The template includes the following parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">role</span></code>: Defines the role or persona the model should assume.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: Provides the background information or context for the task.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">instruction</span></code>: Specifies the task or action the model needs to perform.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_text</span></code>: Contains the actual text input that the model will process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">requirements</span></code>: Lists any specific requirements or constraints for the output.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="k">def</span> <span class="nf">get_base_prompt_template</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    
    <span class="n">base_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: </span><span class="si">{role}</span>
<span class="s2">    CONTEXT: </span><span class="si">{context}</span>
<span class="s2">    INSTRUCTION: </span><span class="si">{instruction}</span>
<span class="s2">    INPUT: </span><span class="si">{input}</span>
<span class="s2">    REQUIREMENTS: </span><span class="si">{requirements}</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">base_prompt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prompt</span>
</pre></div>
</div>
</div>
</div>
<p>We will write a simple function that returns an <code class="docutils literal notranslate"><span class="pre">LLMChain</span></code> which is a simple <code class="docutils literal notranslate"><span class="pre">langchain</span></code> construct that allows you to chain together a combination of prompt templates, language models and output parsers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">ChatLiteLLM</span>

<span class="k">def</span> <span class="nf">get_llm_chain</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an LLMChain instance using langchain.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt_template (str): The prompt template to use.</span>
<span class="sd">        model_name (str): The name of the model to use.</span>
<span class="sd">        temperature (float): The temperature setting for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        llm_chain: An instance of the LLMChain.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
    <span class="kn">import</span> <span class="nn">os</span>

    <span class="c1"># Load environment variables from .env file</span>
    <span class="n">load_dotenv</span><span class="p">()</span>
    
    <span class="n">api_key_label</span> <span class="o">=</span> <span class="n">model_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;_API_KEY&quot;</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatLiteLLM</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">api_key_label</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">prompt_template</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">llm_chain</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Step 3: Constructing Dynamic Prompt Parameters</strong></p>
<p>Now, we will write a function (<code class="docutils literal notranslate"><span class="pre">get_dynamic_prompt_template</span></code>) that constructs prompt parameters dynamically for each chunk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="k">def</span> <span class="nf">get_dynamic_prompt_params</span><span class="p">(</span><span class="n">prompt_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> 
                            <span class="n">part_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                            <span class="n">total_parts</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                            <span class="n">chat_context</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                            <span class="n">chunk</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct prompt template dynamically per chunk while maintaining the chat context of the response generation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        prompt_params (Dict): Original prompt parameters</span>
<span class="sd">        part_idx (int): Index of current conversation part</span>
<span class="sd">        total_parts (int): Total number of conversation parts</span>
<span class="sd">        chat_context (str): Chat context from previous parts</span>
<span class="sd">        chunk (str): Current chunk of text to be processed</span>
<span class="sd">    Returns:</span>
<span class="sd">        str: Dynamically constructed prompt template with part-specific params</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dynamic_prompt_params</span> <span class="o">=</span> <span class="n">prompt_params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="c1"># saves the chat context from previous parts</span>
    <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chat_context</span>
    <span class="c1"># saves the current chunk of text to be processed as input</span>
    <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunk</span>
    
    <span class="c1"># Add part-specific instructions</span>
    <span class="k">if</span> <span class="n">part_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># Introduction part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating the Introduction part of a long report.</span>
<span class="s2">        Don&#39;t cover any topics yet, just define the scope of the report.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    <span class="k">elif</span> <span class="n">part_idx</span> <span class="o">==</span> <span class="n">total_parts</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># Conclusion part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating the last part of a long report. </span>
<span class="s2">        For this part, first discuss the below INPUT. Second, write a &quot;Conclusion&quot; section summarizing the main points discussed given in CONTEXT.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># Main analysis part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating part </span><span class="si">{</span><span class="n">part_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">total_parts</span><span class="si">}</span><span class="s2"> parts of a long report.</span>
<span class="s2">        For this part, analyze the below INPUT.</span>
<span class="s2">        Organize your response in a way that is easy to read and understand either by creating new or merging with previously created structured sections given in CONTEXT.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    
    <span class="k">return</span> <span class="n">dynamic_prompt_params</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Step 4: Generating the Report</strong></p>
<p>Finally, we will write a function that generates the actual report by calling the <code class="docutils literal notranslate"><span class="pre">LLMChain</span></code> with the dynamically updated prompt parameters for each chunk and concatenating the results at the end.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_report</span><span class="p">(</span><span class="n">input_content</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">llm_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                    <span class="n">role</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">requirements</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                    <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># stores the parts of the report, each generated by an individual LLM call</span>
    <span class="n">report_parts</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="c1"># split the input content into chunks</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">get_chunks</span><span class="p">(</span><span class="n">input_content</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">)</span>
    <span class="c1"># initialize the chat context with the input content</span>
    <span class="n">chat_context</span> <span class="o">=</span> <span class="n">input_content</span>
    <span class="c1"># number of parts to be generated</span>
    <span class="n">num_parts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="n">prompt_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="n">role</span><span class="p">,</span> <span class="c1"># user-provided</span>
        <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dinamically updated per part</span>
        <span class="s2">&quot;instruction&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dynamically updated per part</span>
        <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dynamically updated per part</span>
        <span class="s2">&quot;requirements&quot;</span><span class="p">:</span> <span class="n">requirements</span> <span class="c1">#user-priovided</span>
    <span class="p">}</span>

    <span class="c1"># get the LLMChain with the base prompt template</span>
    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">get_llm_chain</span><span class="p">(</span><span class="n">get_base_prompt_template</span><span class="p">(),</span> 
                                 <span class="n">llm_model_name</span><span class="p">)</span>

    <span class="c1"># dynamically update prompt_params per part</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating </span><span class="si">{</span><span class="n">num_parts</span><span class="si">}</span><span class="s2"> report parts&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
        <span class="n">dynamic_prompt_params</span> <span class="o">=</span> <span class="n">get_dynamic_prompt_params</span><span class="p">(</span>
            <span class="n">prompt_params</span><span class="p">,</span>
            <span class="n">part_idx</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
            <span class="n">total_parts</span><span class="o">=</span><span class="n">num_parts</span><span class="p">,</span>
            <span class="n">chat_context</span><span class="o">=</span><span class="n">chat_context</span><span class="p">,</span>
            <span class="n">chunk</span><span class="o">=</span><span class="n">chunk</span>
        <span class="p">)</span>
        
        <span class="c1"># invoke the LLMChain with the dynamically updated prompt parameters</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">llm_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dynamic_prompt_params</span><span class="p">)</span>

        <span class="c1"># update the chat context with the cummulative response</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">chat_context</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chat_context</span> <span class="o">=</span> <span class="n">chat_context</span> <span class="o">+</span> <span class="n">response</span>
            
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated part </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_parts</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">report_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

    <span class="n">report</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">report_parts</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">report</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Example Usage</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the text from sample 10K SEC filing</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/apple.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the chunk and chunk overlap size</span>
<span class="n">MAX_CHUNK_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">MAX_CHUNK_OVERLAP</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">report</span> <span class="o">=</span> <span class="n">generate_report</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">llm_model_name</span><span class="o">=</span><span class="s2">&quot;gemini/gemini-1.5-flash-latest&quot;</span><span class="p">,</span> 
                           <span class="n">role</span><span class="o">=</span><span class="s2">&quot;Financial Analyst&quot;</span><span class="p">,</span> 
                           <span class="n">requirements</span><span class="o">=</span><span class="s2">&quot;The report should be in a readable, structured format, easy to understand and follow. Focus on finding risk factors and market moving insights.&quot;</span><span class="p">,</span>
                           <span class="n">chunk_size</span><span class="o">=</span><span class="n">MAX_CHUNK_SIZE</span><span class="p">,</span> 
                           <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">MAX_CHUNK_OVERLAP</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the generated report to a local file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/apple_report.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read and display the generated report</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/apple_report.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">report_content</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span>

<span class="c1"># Display first and last 10% of the report content</span>
<span class="n">report_lines</span> <span class="o">=</span> <span class="n">report_content</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="n">total_lines</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">report_lines</span><span class="p">)</span>
<span class="n">quarter_lines</span> <span class="o">=</span> <span class="n">total_lines</span> <span class="o">//</span> <span class="mi">10</span>

<span class="n">top_portion</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">report_lines</span><span class="p">[:</span><span class="n">quarter_lines</span><span class="p">])</span>
<span class="n">bottom_portion</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">report_lines</span><span class="p">[</span><span class="o">-</span><span class="n">quarter_lines</span><span class="p">:])</span>

<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">top_portion</span><span class="si">}</span><span class="se">\n\n</span><span class="s2"> (...) </span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{</span><span class="n">bottom_portion</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<p><strong>Introduction</strong></p>
<p>This report provides a comprehensive analysis of Apple Inc.’s financial performance and position for the fiscal year ended September 28, 2024, as disclosed in its Form 10-K filing with the United States Securities and Exchange Commission.  The analysis will focus on identifying key risk factors impacting Apple’s business, evaluating its financial health, and uncovering market-moving insights derived from the provided data.  The report will delve into Apple’s various segments, product lines, and services, examining their performance and contributions to overall financial results.  Specific attention will be paid to identifying trends, potential challenges, and opportunities for future growth.  The analysis will also consider the broader macroeconomic environment and its influence on Apple’s operations and financial outlook.  Finally, the report will incorporate relevant information from Apple’s definitive proxy statement for its 2025 annual meeting of shareholders, as incorporated by reference in the Form 10-K.</p>
<p><strong>PART 2: Key Risk Factors and Market-Moving Insights</strong></p>
<p>This section analyzes key risk factors disclosed in Apple Inc.’s 2024 Form 10-K, focusing on their potential impact on financial performance and identifying potential market-moving insights.  The analysis is structured around the major risk categories identified in the filing.</p>
<p><strong>2.1 Dependence on Third-Party Developers:</strong></p>
<p>Apple’s success is heavily reliant on the continued support and innovation of third-party software developers.  The Form 10-K highlights several critical aspects of this dependence:</p>
<ul class="simple">
<li><p><strong>Market Share Vulnerability:</strong> Apple’s relatively smaller market share in smartphones, personal computers, and tablets compared to competitors (Android, Windows, gaming consoles) could discourage developers from prioritizing Apple’s platform, leading to fewer high-quality apps and potentially impacting customer purchasing decisions.  This is a significant risk, especially given the rapid pace of technological change.  A decline in app availability or quality could negatively impact sales and market share.  <strong>Market-moving insight:</strong>  Monitoring developer activity and app quality across competing platforms is crucial for assessing this risk.  Any significant shift in developer focus away from iOS could be a negative market signal.</p></li>
<li><p><strong>App Store Dynamics:</strong> While Apple allows developers to retain most App Store revenue, its commission structure and recent changes (e.g., complying with the Digital Markets Act (DMA) in the EU) introduce uncertainty.  Changes to the App Store’s policies or fee structures could materially affect Apple’s revenue and profitability.  <strong>Market-moving insight:</strong>  Closely monitoring regulatory developments (especially concerning the DMA) and their impact on App Store revenue is essential.  Any significant changes to Apple’s App Store policies or revenue streams could trigger market reactions.</p></li>
<li><p><strong>Content Acquisition and Creation:</strong> Apple’s reliance on third-party digital content providers for its services introduces risks related to licensing agreements, competition, and pricing.  The cost of producing its own digital content is also increasing due to competition for talent and subscribers.  Failure to secure or create appealing content could negatively impact user engagement and revenue.  <strong>Market-moving insight:</strong>  Analyzing the success of Apple’s original content initiatives and the renewal rates of third-party content agreements will provide insights into this risk.</p></li>
</ul>
<p><strong>2.2 Operational Risks:</strong></p>
<p>(…)</p>
<p>The reconciliation of segment operating income to consolidated operating income reveals that research and development (R&amp;D) and other corporate expenses significantly impact overall profitability.  While increased R&amp;D is generally positive, it reduces short-term profits.  The geographical breakdown of net sales and long-lived assets further emphasizes the concentration of Apple’s business in the U.S. and China.  <strong>Market-moving insight:</strong>  Continued weakness in the Greater China market, sustained flat iPhone sales, or any significant changes in R&amp;D spending should be closely monitored for their potential impact on Apple’s financial performance and investor sentiment.</p>
<p><strong>5.4 Auditor’s Report and Internal Controls:</strong></p>
<p>The auditor’s report expresses an unqualified opinion on Apple’s financial statements and internal control over financial reporting.  However, it identifies uncertain tax positions as a critical audit matter.  The significant amount of unrecognized tax benefits ($22.0 billion) and the complexity involved in evaluating these positions highlight a substantial risk.  Management’s assessment of these positions involves significant judgment and relies on interpretations of complex tax laws.  Apple’s management also asserts that its disclosure controls and procedures are effective.  <strong>Market-moving insight:</strong>  Any changes in tax laws, unfavorable rulings on uncertain tax positions, or weaknesses in internal controls could materially affect Apple’s financial results and investor confidence.</p>
<p><strong>Conclusion</strong></p>
<p>This report provides a comprehensive analysis of Apple Inc.’s financial performance and position for fiscal year 2024.  While Apple maintains a strong financial position with substantial cash reserves and a robust capital return program, several key risk factors could significantly impact its future performance.  These risks include:</p>
<ul class="simple">
<li><p><strong>Dependence on third-party developers:</strong>  A shift in developer focus away from iOS or changes to the App Store’s policies could negatively impact Apple’s revenue and profitability.</p></li>
<li><p><strong>Operational risks:</strong>  Employee retention challenges, reseller dependence, and cybersecurity threats pose significant operational risks.</p></li>
<li><p><strong>Legal and regulatory risks:</strong>  Ongoing antitrust litigation, the Digital Markets Act (DMA) compliance, and data privacy regulations introduce substantial legal and regulatory uncertainties.</p></li>
<li><p><strong>Financial risks:</strong>  Volatility in sales and profit margins, foreign exchange rate fluctuations, credit risk, and tax risks could impact Apple’s financial performance.</p></li>
<li><p><strong>Supply chain concentration:</strong>  Apple’s reliance on a concentrated network of outsourcing partners, primarily located in a few Asian countries, and dependence on single or limited sources for certain custom components, exposes the company to significant supply chain risks.</p></li>
<li><p><strong>Uncertain tax positions:</strong>  The significant amount of unrecognized tax benefits represents a substantial uncertainty that could materially affect Apple’s financial results.</p></li>
</ul>
<p>Despite these risks, Apple’s strong liquidity position, continued growth in its Services segment, and robust capital return program provide a degree of resilience.  However, investors and analysts should closely monitor the market-moving insights identified throughout this report, including developer activity, regulatory developments, regional economic conditions, supply chain stability, and the resolution of uncertain tax positions, to assess their potential impact on Apple’s future performance and valuation.  The significant short-term obligations, while manageable given Apple’s cash position, highlight the need for continued financial discipline and effective risk management.  A deeper, more granular analysis of the financial statements and notes is recommended for a more complete assessment.</p>
</div>
</div>
</section>
<hr class="docutils" />
<section id="discussion">
<h4><a class="toc-backref" href="#id226" role="doc-backlink"><span class="section-number">5.4.1.2. </span>Discussion</a><a class="headerlink" href="#discussion" title="Permalink to this heading">¶</a></h4>
<p>Results from the generated report present a few interesting aspects:</p>
<ul class="simple">
<li><p><strong>Coherence</strong>: The generated report demonstrates an apparent level of coherence. The sections are logically structured, and the flow of information is smooth. Each part of the report builds upon the previous sections, providing a comprehensive analysis of Apple Inc.’s financial performance and key risk factors. The use of headings and subheadings helps in maintaining clarity and organization throughout the document.</p></li>
<li><p><strong>Adherence to Instructions</strong>: The LLM followed the provided instructions effectively. The report is in a readable, structured format, and it focuses on identifying risk factors and market-moving insights as requested. The analysis is detailed and covers various aspects of Apple’s financial performance, including revenue segmentation, profitability, liquidity, and capital resources. The inclusion of market-moving insights adds value to the report, aligning with the specified requirements.</p></li>
</ul>
<p>Despite the seemingly good quality of the results, there are some limitations to consider:</p>
<ul class="simple">
<li><p><strong>Depth of Analysis</strong>: While the report covers a wide range of topics, the depth of analysis in certain sections may not be as comprehensive as a human expert’s evaluation. Some nuances and contextual factors might be overlooked by the LLM. Splitting the report into multiple parts helps in mitigating this issue.</p></li>
<li><p><strong>Chunking Strategy</strong>: The current approach splits the text into chunks based on size, which ensures that each chunk fits within the model’s token limit. However, this method may disrupt the logical flow of the document, as sections of interest might be split across multiple chunks. An alternative approach could be “structured” chunking, where the text is divided based on meaningful sections or topics. This would preserve the coherence of each section, making it easier to follow and understand. Implementing structured chunking requires additional preprocessing to identify and segment the text appropriately, but it can significantly enhance the readability and logical flow of the generated report.</p></li>
</ul>
<p>Here, we implemented a simple strategy to improve the coherence in output generation given a multi-part chunked input. Many other strategies are possible. One related technique worth mentioning is Anthropic’s Contextual Retrieval <span id="id11">[<a class="reference internal" href="#id147" title="Anthropic. Introducing contextual retrieval. 09 2024a. URL: https://www.anthropic.com/news/contextual-retrieval.">Anthropic, 2024a</a>]</span>. The approach, as shown in <a class="reference internal" href="#anth-contextual"><span class="std std-numref">Fig. 5.7</span></a>, employs an LLM itself to generate relevant context per chunk before passing these two pieces of information together to the LLM. This process was proposed in the context of RAGs to enhance its retrieval capabilities but can be applied more generally to improve output generation.</p>
<figure class="align-center" id="anth-contextual">
<a class="reference internal image-reference" href="../_images/anth_contextual.png"><img alt="Anthropic Contextual Linking" src="../_images/anth_contextual.png" style="width: 545.5px; height: 359.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.7 </span><span class="caption-text">Anthropic Contextual Linking <span id="id12">[<a class="reference internal" href="#id147" title="Anthropic. Introducing contextual retrieval. 09 2024a. URL: https://www.anthropic.com/news/contextual-retrieval.">Anthropic, 2024a</a>]</span>.</span><a class="headerlink" href="#anth-contextual" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="case-study-ii-github-rag">
<h3><a class="toc-backref" href="#id227" role="doc-backlink"><span class="section-number">5.4.2. </span>Case Study II: Github RAG</a><a class="headerlink" href="#case-study-ii-github-rag" title="Permalink to this heading">¶</a></h3>
</section>
<section id="case-study-iii-quiz-generation-with-citations">
<h3><a class="toc-backref" href="#id228" role="doc-backlink"><span class="section-number">5.4.3. </span>Case Study III: Quiz Generation with Citations</a><a class="headerlink" href="#case-study-iii-quiz-generation-with-citations" title="Permalink to this heading">¶</a></h3>
<p>In this case study, we will build a Quiz generator with citations that explores additional input management techniques particularly useful with long context windows. The implementation includes prompt caching for efficiency and citation tracking to enhance accuracy and verifiability. We will use Gemini 1.5 Pro as our LLM model, which has a context window of 2M tokens.</p>
<section id="use-case">
<h4><a class="toc-backref" href="#id229" role="doc-backlink"><span class="section-number">5.4.3.1. </span>Use Case</a><a class="headerlink" href="#use-case" title="Permalink to this heading">¶</a></h4>
<p>Let’s assume you are a Harvard student enrolled in GOV 1039 “The Birth of Modern Democracy” (see <a class="reference internal" href="#harvard-class"><span class="std std-numref">Fig. 5.8</span></a>), you face a daunting reading list for next Tuesday’s class on Rights. The readings include foundational documents like the Magna Carta, Declaration of Independence, and US Bill of Rights, each with specific sections to analyze.</p>
<figure class="align-center" id="harvard-class">
<a class="reference internal image-reference" href="../_images/harvard.png"><img alt="Harvard Class" src="../_images/harvard.png" style="width: 691.0px; height: 435.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.8 </span><span class="caption-text">Harvard’s Democratic Theory Class</span><a class="headerlink" href="#harvard-class" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Instead of trudging through these dense historical texts sequentially, we would like to:</p>
<ul class="simple">
<li><p>Extract key insights and connections between these documents, conversationally.</p></li>
<li><p>Engage with the material through a quiz format.</p></li>
<li><p>Add citations to help with verifying answers.</p></li>
</ul>
</section>
<section id="implementation">
<h4><a class="toc-backref" href="#id230" role="doc-backlink"><span class="section-number">5.4.3.2. </span>Implementation</a><a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
<p>The full implementation is available at Book’s <a class="reference external" href="https://github.com/souzatharsis/tamingLLMs/tamingllms/notebooks/src/gemini_duo.py">Github repository</a>. Here, we will cover the most relevant parts of the implementation.</p>
<p><strong>Client Class</strong></p>
<p>First, we will define the <code class="docutils literal notranslate"><span class="pre">Client</span></code> class which will provide the key interface users will interact with. It has the following summarized interface:</p>
<ul class="simple">
<li><p>Initialization:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">__init__(knowledge_base:</span> <span class="pre">List[str]</span> <span class="pre">=</span> <span class="pre">[])</span></code>: Initialize with optional list of URLs as knowledge base</p></li>
</ul>
</li>
<li><p>Core Methods:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">add_knowledge_base(urls:</span> <span class="pre">List[str])</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code>: Add URLs to the knowledge base</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add(urls:</span> <span class="pre">List[str])</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code>: Extract content from URLs and add to conversation input</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">msg(msg:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">&quot;&quot;,</span> <span class="pre">add_citations:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False)</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code>: Enables users to send messages to the client</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">quiz(add_citations:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">num_questions:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">10)</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code>: Generate a quiz based on full input memory</p></li>
</ul>
</li>
<li><p>Key Attributes:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">knowledge_base</span></code>: List of URLs providing foundation knowledge</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input</span></code>: Current input being studied (short-term memory)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_memory</span></code>: Cumulative input + knowledge base (long-term memory)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response</span></code>: Latest response from LLM</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response_memory</span></code>: Cumulative responses (long-term memory)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">urls_memory</span></code>: Cumulative list of processed URLs</p></li>
</ul>
</li>
</ul>
<p><strong>Corpus-in-Context Prompting</strong></p>
<p>The <code class="docutils literal notranslate"><span class="pre">add()</span></code> method is key since it is used to add content to the client. It takes a list of URLs and extracts the content from each URL using a content extractor (using MarkitDown). The content is then added to the conversation input memory in a way that enables citations using the “Corpus-in-Context” (CIC) Prompting <span id="id13">[<a class="reference internal" href="#id104" title="Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more? 2024. URL: https://arxiv.org/abs/2406.13121, arXiv:2406.13121.">Lee <em>et al.</em>, 2024</a>]</span>.</p>
<p><a class="reference internal" href="#cic"><span class="std std-numref">Fig. 5.9</span></a> shows how CIC format is used to enable citations. It inserts a corpus into the prompt. Each candidate citable part (e.g., passage, chapter) in a corpus is assigned a unique identifier (ID) that can be referenced as needed for that task.</p>
<figure class="align-center" id="cic">
<a class="reference internal image-reference" href="../_images/cic.png"><img alt="CIC Format" src="../_images/cic.png" style="width: 830.5px; height: 361.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.9 </span><span class="caption-text">Example of Corpus-in-Context Prompting for retrieval.</span><a class="headerlink" href="#cic" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>CiC prompting leverages LLM’s capacity to follow instructions by carefully annotating the corpus with document IDs. It benefits from a strong, capable models to retrieve over large corpora provided in context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">urls</span> <span class="o">=</span> <span class="n">urls</span>

        <span class="c1"># Add new content to input following CIC format to enable citations</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">urls_memory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="n">content</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extractor</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">text_content</span>
            <span class="n">formatted_content</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;ID: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_id</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">content</span><span class="si">}</span><span class="s2"> | END ID: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_id</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">+=</span> <span class="n">formatted_content</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">reference_id</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="c1"># Update memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_memory</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span>
</pre></div>
</div>
<p>The method <code class="docutils literal notranslate"><span class="pre">add_knowledge_base()</span></code> is a simple wrapper around the <code class="docutils literal notranslate"><span class="pre">add()</span></code> method. It is used to add URLs to the knowledge base, which are later cached by the LLM model as we will see later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">add_knowledge_base</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">urls</span><span class="p">)</span>
</pre></div>
</div>
<p>Later, when the user sends a message to the client, the <code class="docutils literal notranslate"><span class="pre">msg()</span></code> method is used to generate a response  while enabling citations. <code class="docutils literal notranslate"><span class="pre">self.content_generator</span></code> is an instance of our LLM model, which we will go through next.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">msg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">add_citations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">add_citations</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="n">msg</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2"> For key statements, add Input ID to the response.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">content_generator</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">input_content</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">,</span>
            <span class="n">user_instructions</span><span class="o">=</span><span class="n">msg</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">response_memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_memory</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">text</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">text</span>
</pre></div>
</div>
<p><strong>Prompt Caching</strong></p>
<p>LLM-based applications often involve repeatedly passing the same input tokens to a model, which can be inefficient and costly. Context caching addresses this by allowing you to cache input tokens after their first use and reference them in subsequent requests. This approach significantly reduces costs compared to repeatedly sending the same token corpus, especially at scale.</p>
<p>In our application, the user might passes a large knowledge base to the client that can be referenced multiple times by smaller user requests. Our <code class="docutils literal notranslate"><span class="pre">Client</span></code> class is composed of a <code class="docutils literal notranslate"><span class="pre">LLMBackend</span></code> class that takes the <code class="docutils literal notranslate"><span class="pre">input_memory</span></code> containing the entire knowledge base and any additional user added content.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLMBackend</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_memory</span><span class="p">)</span>
</pre></div>
</div>
<p>In our <code class="docutils literal notranslate"><span class="pre">LLMBackend</span></code> Class, we leverage prompt caching on input tokens and uses them for subsequent requests.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LLMBackend</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">cache_ttl</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">60</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="n">caching</span><span class="o">.</span><span class="n">CachedContent</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">display_name</span><span class="o">=</span><span class="s1">&#39;due_knowledge_base&#39;</span><span class="p">,</span> <span class="c1"># used to identify the cache</span>
            <span class="n">system_instruction</span><span class="o">=</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compose_prompt</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">conversation_config</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">ttl</span><span class="o">=</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="n">cache_ttl</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">genai</span><span class="o">.</span><span class="n">GenerativeModel</span><span class="o">.</span><span class="n">from_cached_content</span><span class="p">(</span><span class="n">cached_content</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Quiz Generation</strong></p>
<p>Coming back to our <code class="docutils literal notranslate"><span class="pre">Client</span></code> class, we implement the <code class="docutils literal notranslate"><span class="pre">quiz()</span></code> method to generate a quiz based on the full input memory, i.e. the initial knowledge base and any additional user added content.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">quiz()</span></code> method returns a <code class="docutils literal notranslate"><span class="pre">Quiz</span></code> instance which behind the scenes caches input tokens. The user later can invoke its <code class="docutils literal notranslate"><span class="pre">generate()</span></code> method to generate a quiz passing the user instructions in <code class="docutils literal notranslate"><span class="pre">msg</span></code> parameter, as we will see later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">quiz</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">add_citations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">num_questions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a quiz instance based on full input memory.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quiz_instance</span> <span class="o">=</span> <span class="n">Quiz</span><span class="p">(</span>
                         <span class="nb">input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_memory</span><span class="p">,</span>
                         <span class="n">add_citations</span><span class="o">=</span><span class="n">add_citations</span><span class="p">,</span>
                         <span class="n">num_questions</span><span class="o">=</span><span class="n">num_questions</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">quiz_instance</span>
</pre></div>
</div>
<p>We write a simple prompt template for quiz generation:</p>
<blockquote>
<div><p>ROLE:</p>
<ul class="simple">
<li><p>You are a Harvard Professor providing a quiz.
INSTRUCTIONS:</p></li>
<li><p>Generate a quiz with {num_questions} questions based on the input.</p></li>
<li><p>The quiz should be multi-choice.</p></li>
<li><p>Answers should be provided at the end of the quiz.</p></li>
<li><p>Questions should have broad coverage of the input including multiple Input IDs.</p></li>
<li><p>Level of difficulty is advanced/hard.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">{citations}</span></code></p></li>
</ul>
<p>STRUCTURE:</p>
<ul class="simple">
<li><p>Sequence of questions and alternatives.</p></li>
<li><p>At the end provide the correct answers.</p></li>
</ul>
</div></blockquote>
<p>where, <code class="docutils literal notranslate"><span class="pre">{citations}</span></code> instructs the model to add CiC citations to the response if user requests it.</p>
</section>
<section id="example-usage">
<h4><a class="toc-backref" href="#id231" role="doc-backlink"><span class="section-number">5.4.3.3. </span>Example Usage</a><a class="headerlink" href="#example-usage" title="Permalink to this heading">¶</a></h4>
<p><strong>Dataset</strong></p>
<p>First, we will define our knowledge base.</p>
<ul class="simple">
<li><p>Harvard Class: <a class="reference external" href="https://scholar.harvard.edu/files/dlcammack/files/gov_1039_syllabus.pdf">GOV 1039 Syllabus</a></p></li>
<li><p>Class / Topic: “Rights”</p></li>
<li><p>Reading List:</p>
<ul>
<li><p>ID 1. The Declaration of Independence of the United States of America</p></li>
<li><p>ID 2. The United States Bill of Rights</p></li>
<li><p>ID 3. John F. Kennedy’s Inaugural Address</p></li>
<li><p>ID 4. Lincoln’s Gettysburg Address</p></li>
<li><p>ID 5. The United States Constitution</p></li>
<li><p>ID 6. Give Me Liberty or Give Me Death</p></li>
<li><p>ID 7. The Mayflower Compact</p></li>
<li><p>ID 8. Abraham Lincoln’s Second Inaugural Address</p></li>
<li><p>ID 9. Abraham Lincoln’s First Inaugural Address</p></li>
</ul>
</li>
</ul>
<p>We will take advantage of Project Gutenberg’s to create our knowledge base.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kb</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;https://www.gutenberg.org/cache/epub/</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">/pg</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.txt&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>We will import our module <code class="docutils literal notranslate"><span class="pre">gemini_duo</span></code> as <code class="docutils literal notranslate"><span class="pre">genai_duo</span></code> and initialize the <code class="docutils literal notranslate"><span class="pre">Client</span></code> class with our knowledge base.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gemini_duo</span> <span class="k">as</span> <span class="nn">genai_duo</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span><span class="p">,</span> <span class="n">display</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">duo</span> <span class="o">=</span> <span class="n">genai_duo</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">knowledge_base</span><span class="o">=</span><span class="n">kb</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>At this point, we converted each book into markdown using MarkitDown and cached the content in our LLM model. We can access how many tokens we have cached in our LLM model by looking at the <code class="docutils literal notranslate"><span class="pre">usage_metadata</span></code> attribute of the Gemini’s model response. At this point, we have cached at total of 38470 tokens.</p>
<p>Now, we can add references to our knowledge base at anytime by calling the <code class="docutils literal notranslate"><span class="pre">add()</span></code> method. We add the following references:</p>
<ol class="arabic simple">
<li><p>The Magna Carta</p></li>
<li><p>William Shap McKechnie on Magna Carta book</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">study_references</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://www.gutenberg.org/cache/epub/10000/pg10000.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;https://www.gutenberg.org/cache/epub/65363/pg65363.txt&quot;</span><span class="p">]</span>

<span class="n">duo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">study_references</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can instantiate a <code class="docutils literal notranslate"><span class="pre">Quiz</span></code> object and generate a quiz based on the full input memory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quiz</span> <span class="o">=</span> <span class="n">duo</span><span class="o">.</span><span class="n">quiz</span><span class="p">(</span><span class="n">add_citations</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">quiz</span><span class="o">.</span><span class="n">generate</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference internal" href="#quiz"><span class="std std-numref">Fig. 5.10</span></a> shows a sample quiz with citations. Marked in yellow are the citations which refer to the input IDs of the resources we added to the model.</p>
<figure class="align-center" id="quiz">
<a class="reference internal image-reference" href="../_images/quiz.png"><img alt="Quiz with Citations" src="../_images/quiz.png" style="width: 891.0px; height: 772.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.10 </span><span class="caption-text">Sample Quiz with Citations.</span><a class="headerlink" href="#quiz" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="id14">
<h4><a class="toc-backref" href="#id232" role="doc-backlink"><span class="section-number">5.4.3.4. </span>Discussion</a><a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h4>
<p>The experiment demonstrated the ability to build a knowledge base from multiple sources while leveraging prompt caching for efficiency and generate quizzes with citations for verifiability. The system successfully ingested content from Project Gutenberg texts, including historical documents like the Magna Carta, and used them to create interactive educational content.</p>
<p>However, several limitations emerged during this process:</p>
<ol class="arabic simple">
<li><p>Memory Management: The system currently loads all content into memory, which could become problematic with larger knowledge bases. A more scalable approach might involve chunking or streaming the content.</p></li>
<li><p>Citation Quality: While the system provides citations, they lack specificity - pointing to entire documents rather than specific passages or page numbers. This limits the ability to fact-check or verify specific claims.</p></li>
<li><p>Content Verification: While citations are provided, the system is not guaranteed to provide factual information. This could lead to potential hallucinations or misinterpretations.</p></li>
</ol>
<p>While limitations are present in this simple example, the case study highlights that not always complex systems are needed. Alternative simple strategies should be preferred when possible, particularly if capable, long-context window models are available and fit within the application requirements.</p>
</section>
</section>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id233" role="doc-backlink"><span class="section-number">5.5. </span>Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="CC BY-NC-SA 4.0" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" /></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@misc</span><span class="p">{</span><span class="n">tharsistpsouza2024tamingllms</span><span class="p">,</span>
  <span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">Tharsis</span> <span class="n">T</span><span class="o">.</span> <span class="n">P</span><span class="o">.</span> <span class="n">Souza</span><span class="p">},</span>
  <span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">Taming</span> <span class="n">LLMs</span><span class="p">:</span> <span class="n">A</span> <span class="n">Practical</span> <span class="n">Guide</span> <span class="n">to</span> <span class="n">LLM</span> <span class="n">Pitfalls</span> <span class="k">with</span> <span class="n">Open</span> <span class="n">Source</span> <span class="n">Software</span><span class="p">},</span>
  <span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2024</span><span class="p">},</span>
  <span class="n">chapter</span> <span class="o">=</span> <span class="p">{</span><span class="n">Managing</span> <span class="n">Input</span> <span class="n">Data</span><span class="p">},</span>
  <span class="n">journal</span> <span class="o">=</span> <span class="p">{</span><span class="n">GitHub</span> <span class="n">repository</span><span class="p">},</span>
  <span class="n">url</span> <span class="o">=</span> <span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">souzatharsis</span><span class="o">/</span><span class="n">tamingLLMs</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id234" role="doc-backlink"><span class="section-number">5.6. </span>References</a><a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id15">
<div class="citation" id="id149" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">AWP+24</a><span class="fn-bracket">]</span></span>
<p>Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang. Knowledge of knowledge: exploring known-unknowns uncertainty with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, <em>Findings of the Association for Computational Linguistics: ACL 2024</em>, 6416–6432. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2024.findings-acl.383">https://aclanthology.org/2024.findings-acl.383</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2024.findings-acl.383">doi:10.18653/v1/2024.findings-acl.383</a>.</p>
</div>
<div class="citation" id="id150" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">KSR24</a><span class="fn-bracket">]</span></span>
<p>Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic forgetting in language models via implicit inference. In <em>The Twelfth International Conference on Learning Representations</em>. 2024. URL: <a class="reference external" href="https://openreview.net/forum?id=VrHiF2hsrm">https://openreview.net/forum?id=VrHiF2hsrm</a>.</p>
</div>
<div class="citation" id="id104" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LCD+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id13">2</a>)</span>
<p>Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more? 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2406.13121">https://arxiv.org/abs/2406.13121</a>, <a class="reference external" href="https://arxiv.org/abs/2406.13121">arXiv:2406.13121</a>.</p>
</div>
<div class="citation" id="id102" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">LPP+21</a><span class="fn-bracket">]</span></span>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a>, <a class="reference external" href="https://arxiv.org/abs/2005.11401">arXiv:2005.11401</a>.</p>
</div>
<div class="citation" id="id151" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">NBGC24</a><span class="fn-bracket">]</span></span>
<p>Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. When do LLMs need retrieval augmentation? mitigating LLMs' overconfidence helps retrieval augmentation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, <em>Findings of the Association for Computational Linguistics: ACL 2024</em>, 11375–11388. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2024.findings-acl.675">https://aclanthology.org/2024.findings-acl.675</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2024.findings-acl.675">doi:10.18653/v1/2024.findings-acl.675</a>.</p>
</div>
<div class="citation" id="id146" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">TDW+24</a><span class="fn-bracket">]</span></span>
<p>Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag: html is better than plain text for modeling retrieved knowledge in rag systems. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2411.02959">https://arxiv.org/abs/2411.02959</a>, <a class="reference external" href="https://arxiv.org/abs/2411.02959">arXiv:2411.02959</a>.</p>
</div>
<div class="citation" id="id145" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">ZLJ+24</a><span class="fn-bracket">]</span></span>
<p>Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large language models. In <em>Proceedings of the ACM Web Conference 2024</em>, WWW '24, 1453–1463. New York, NY, USA, 2024. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3589334.3645481">https://doi.org/10.1145/3589334.3645481</a>, <a class="reference external" href="https://doi.org/10.1145/3589334.3645481">doi:10.1145/3589334.3645481</a>.</p>
</div>
<div class="citation" id="id147" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Anthropic4a<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id12">2</a>)</span>
<p>Anthropic. Introducing contextual retrieval. 09 2024a. URL: <a class="reference external" href="https://www.anthropic.com/news/contextual-retrieval">https://www.anthropic.com/news/contextual-retrieval</a>.</p>
</div>
<div class="citation" id="id53" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">LangChain24</a><span class="fn-bracket">]</span></span>
<p>LangChain. Text splitters - langchain documentation. <a class="reference external" href="https://python.langchain.com/docs/how_to/#text-splitters">https://python.langchain.com/docs/how_to/#text-splitters</a>, 2024. Accessed: 12/07/2024.</p>
</div>
<div class="citation" id="id105" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">MerrillLynch24</a><span class="fn-bracket">]</span></span>
<p>Merrill Lynch. Chief investment officer capital market outlook. CIO Weekly Letter, 2024. URL: <a class="reference external" href="https://olui2.fs.ml.com/publish/content/application/pdf/gwmol/me-cio-weekly-letter.pdf">https://olui2.fs.ml.com/publish/content/application/pdf/gwmol/me-cio-weekly-letter.pdf</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="structured_output.html"
       title="previous chapter">← <span class="section-number">4. </span>Structured Output</a>
  </li>
  <li class="next">
    <a href="safety.html"
       title="next chapter"><span class="section-number">6. </span>Safety →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; Copyright Tharsis T. P. Souza, 2024.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.9.1.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>