<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

      <title>5. Managing Input Data</title>
    
          <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../_static/theme.css " type="text/css" />
          <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-thebe.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/scripts/sphinx-book-theme.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../_static/design-tabs.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
        <script async="async" src="../_static/sphinx-thebe.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../_static/theme-vendors.js"></script> -->
      <script src="../_static/theme.js" defer></script>
    
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="6. Safety" href="safety.html" />
  <link rel="prev" title="4. Structured Output" href="structured_output.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../markdown/toc.html" class="home-link">
    
      <span class="site-name">Taming LLMs</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



  
    <div class="nav-item">
      <a href="https://tamingllm.substack.com/"
        class="nav-link external">
          Newsletter <outboundlink></outboundlink>
      </a>
    </div>
  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



  
    <div class="nav-item">
      <a href="https://tamingllm.substack.com/"
        class="nav-link external">
          Newsletter <outboundlink></outboundlink>
      </a>
    </div>
  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../markdown/toc.html#taming-llms">taming llms</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="../markdown/preface.html" class="reference internal ">Preface</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../markdown/intro.html" class="reference internal ">About the Book</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="evals.html" class="reference internal ">The Evals Gap</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="structured_output.html" class="reference internal ">Structured Output</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">Managing Input Data</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#introduction" class="reference internal">Introduction</a></li>
                
                  <li class="toctree-l2"><a href="#parsing-documents" class="reference internal">Parsing Documents</a></li>
                
                  <li class="toctree-l2"><a href="#retrieval-augmented-generation" class="reference internal">Retrieval-Augmented Generation</a></li>
                
                  <li class="toctree-l2"><a href="#a-note-on-frameworks" class="reference internal">A Note on Frameworks</a></li>
                
                  <li class="toctree-l2"><a href="#case-studies" class="reference internal">Case Studies</a></li>
                
                  <li class="toctree-l2"><a href="#conclusion" class="reference internal">Conclusion</a></li>
                
                  <li class="toctree-l2"><a href="#references" class="reference internal">References</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="safety.html" class="reference internal ">Safety</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="alignment.html" class="reference internal ">Preference-Based Alignment</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="local.html" class="reference internal ">Local LLMs in Practice</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="cost.html" class="reference internal ">The Falling Cost Paradox</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../markdown/toc.html">Docs</a> &raquo;</li>
    
    <li><span class="section-number">5. </span>Managing Input Data</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="structured_output.html"
       title="previous chapter">← <span class="section-number">4. </span>Structured Output</a>
  </li>
  <li class="next">
    <a href="safety.html"
       title="next chapter"><span class="section-number">6. </span>Safety →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section class="tex2jax_ignore mathjax_ignore" id="managing-input-data">
<span id="input"></span><h1><a class="toc-backref" href="#id292" role="doc-backlink"><span class="section-number">5. </span>Managing Input Data</a><a class="headerlink" href="#managing-input-data" title="Permalink to this heading">¶</a></h1>
<blockquote class="epigraph">
<div><p>One home run is much better than two doubles.</p>
<p class="attribution">—Steve Jobs</p>
</div></blockquote>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#managing-input-data" id="id292">Managing Input Data</a></p>
<ul>
<li><p><a class="reference internal" href="#introduction" id="id293">Introduction</a></p></li>
<li><p><a class="reference internal" href="#parsing-documents" id="id294">Parsing Documents</a></p>
<ul>
<li><p><a class="reference internal" href="#markitdown" id="id295">MarkItDown</a></p></li>
<li><p><a class="reference internal" href="#docling" id="id296">Docling</a></p></li>
<li><p><a class="reference internal" href="#structured-data-extraction" id="id297">Structured Data Extraction</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#retrieval-augmented-generation" id="id298">Retrieval-Augmented Generation</a></p>
<ul>
<li><p><a class="reference internal" href="#rag-pipeline" id="id299">RAG Pipeline</a></p>
<ul>
<li><p><a class="reference internal" href="#preparing-the-knowledge-base" id="id300">Preparing the Knowledge Base</a></p></li>
<li><p><a class="reference internal" href="#vector-database" id="id301">Vector Database</a></p></li>
<li><p><a class="reference internal" href="#reranking" id="id302">Reranking</a></p></li>
<li><p><a class="reference internal" href="#llms-with-rag" id="id303">LLMs with RAG</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#challenges-and-limitations" id="id304">Challenges and Limitations</a></p></li>
<li><p><a class="reference internal" href="#will-rags-exist-in-the-future" id="id305">Will RAGs exist in the future?</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#a-note-on-frameworks" id="id306">A Note on Frameworks</a></p></li>
<li><p><a class="reference internal" href="#case-studies" id="id307">Case Studies</a></p>
<ul>
<li><p><a class="reference internal" href="#case-study-i-content-chunking-with-contextual-linking" id="id308">Case Study I: Content Chunking with Contextual Linking</a></p>
<ul>
<li><p><a class="reference internal" href="#generating-long-form-content" id="id309">Generating long-form content</a></p></li>
<li><p><a class="reference internal" href="#discussion" id="id310">Discussion</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#case-study-ii-quiz-generation-with-citations" id="id311">Case Study II: Quiz Generation with Citations</a></p>
<ul>
<li><p><a class="reference internal" href="#use-case" id="id312">Use Case</a></p></li>
<li><p><a class="reference internal" href="#implementation" id="id313">Implementation</a></p></li>
<li><p><a class="reference internal" href="#example-usage" id="id314">Example Usage</a></p></li>
<li><p><a class="reference internal" href="#id39" id="id315">Discussion</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion" id="id316">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#references" id="id317">References</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="introduction">
<h2><a class="toc-backref" href="#id293" role="doc-backlink"><span class="section-number">5.1. </span>Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>While advances in long-context language models (LCs) <span id="id1">[<a class="reference internal" href="#id278" title="Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more? 2024. URL: https://arxiv.org/abs/2406.13121, arXiv:2406.13121.">Lee <em>et al.</em>, 2024</a>]</span> have expanded the amount of information these systems can process, significant challenges remain in managing and effectively utilizing extended data inputs:</p>
<ul class="simple">
<li><p>LLMs are sensitive to input formatting and structure, requiring careful data preparation to achieve optimal results <span id="id2">[<a class="reference internal" href="#id282" title="Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, and Sadid Hasan. Does prompt formatting have any impact on llm performance? 2024. URL: https://arxiv.org/abs/2411.10541, arXiv:2411.10541.">He <em>et al.</em>, 2024</a>, <a class="reference internal" href="#id274" title="Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu, and Jieping Ye. Enhancing llm's cognition via structurization. 2024. URL: https://arxiv.org/abs/2407.16434, arXiv:2407.16434.">Liu <em>et al.</em>, 2024</a>, <a class="reference internal" href="#id194" title="Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag: html is better than plain text for modeling retrieved knowledge in rag systems. 2024. URL: https://arxiv.org/abs/2411.02959, arXiv:2411.02959.">Tan <em>et al.</em>, 2024</a>]</span>.</p></li>
<li><p>They operate with knowledge cutoffs, providing potentially stale or outdated information that may not reflect current reality and demonstrate problems with temporal knowledge accuracy <span id="id3">[<a class="reference internal" href="#id197" title="Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang. Knowledge of knowledge: exploring known-unknowns uncertainty with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, 6416–6432. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.findings-acl.383, doi:10.18653/v1/2024.findings-acl.383.">Amayuelas <em>et al.</em>, 2024</a>]</span>.</p></li>
<li><p>LLMs also face “lost-in-the-middle” problems <span id="id4">[<a class="reference internal" href="#id281" title="Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. Less is more for long document summary evaluation by llms. 2024. URL: https://arxiv.org/abs/2309.07382, arXiv:2309.07382.">Wu <em>et al.</em>, 2024</a>]</span> and struggle with less common but important information showing a systematic loss of long-tail knowledge <span id="id5">[<a class="reference internal" href="#id198" title="Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic forgetting in language models via implicit inference. In The Twelfth International Conference on Learning Representations. 2024. URL: https://openreview.net/forum?id=VrHiF2hsrm.">Kotha <em>et al.</em>, 2024</a>]</span>.</p></li>
</ul>
<p>Motivated by these challenges, this chapter explores two key input data components:</p>
<ol class="arabic simple">
<li><p>Data Parsing and Chunking: Parsing and chunking documents into a unified format that is suitable and more manageable for LLMs to process.</p></li>
<li><p>Retrieval Augmentation: Augmenting LLMs with the ability to retrieve relevant, recent, and specialized information.</p></li>
</ol>
<p>In data parsing, we will explore some useful open source tools that help transform data into LLM-compatible formats, demonstrating their impact through a case study of structured information extraction from complex PDFs. In a second case study, we will introduce some chunking strategies to help LLMs process long inputs and implement a particular technique called Chunking with Contextual Linking the enables contextually relevant chunk processing.</p>
<p>In retrieval augmentation, we will explore how to enhance LLMs with semantic search capabilities for incorporating external context using RAGs (Retrieval Augmented Generation) while discussing whether RAGs will be really needed in the future given the rise of long-context language models.</p>
<p>While RAGs are useful for incorporating external context, they are not a silver bullet nor a mandatory component for all LLM applications. In our last case study, we leverage long-context windows to build a quiz generator from a large knowledge base. We will also explore some additional relevant techniques such as prompt caching and response verification through citations.</p>
<p>By the chapter’s conclusion, readers will possess relevant knowledge of input data management strategies for LLMs and practical expertise in selecting and implementing appropriate approaches and tools for specific use cases.</p>
</section>
<section id="parsing-documents">
<h2><a class="toc-backref" href="#id294" role="doc-backlink"><span class="section-number">5.2. </span>Parsing Documents</a><a class="headerlink" href="#parsing-documents" title="Permalink to this heading">¶</a></h2>
<p>Data parsing and formatting play a critical role in LLMs performance <span id="id6">[<a class="reference internal" href="#id282" title="Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, and Sadid Hasan. Does prompt formatting have any impact on llm performance? 2024. URL: https://arxiv.org/abs/2411.10541, arXiv:2411.10541.">He <em>et al.</em>, 2024</a>, <a class="reference internal" href="#id274" title="Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu, and Jieping Ye. Enhancing llm's cognition via structurization. 2024. URL: https://arxiv.org/abs/2407.16434, arXiv:2407.16434.">Liu <em>et al.</em>, 2024</a>, <a class="reference internal" href="#id194" title="Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag: html is better than plain text for modeling retrieved knowledge in rag systems. 2024. URL: https://arxiv.org/abs/2411.02959, arXiv:2411.02959.">Tan <em>et al.</em>, 2024</a>]</span>. Hence, building robust data ingestion and preprocessing pipelines is essential for any LLM application.</p>
<p>This section explores open source tools that streamline input data processing, in particular for parsing purposes, providing a unified interface for converting diverse data formats into standardized representations that LLMs can effectively process. By abstracting away format-specific complexities, they allow developers to focus on core application logic rather than parsing implementation details while maximizing the LLM performance.</p>
<p>We will cover open source tools that provide parsing capabilities for a wide range of data formats. And we will demonstrate how some of these tools can be used to extract structured information from complex PDFs demonstrating how the quality of the parser can impact LLM’s performance.</p>
<section id="markitdown">
<h3><a class="toc-backref" href="#id295" role="doc-backlink"><span class="section-number">5.2.1. </span>MarkItDown</a><a class="headerlink" href="#markitdown" title="Permalink to this heading">¶</a></h3>
<p>MarkItDown <span id="id7">[<a class="reference internal" href="#id148" title="Microsoft. Markitdown: structured generation with large language models. GitHub Repository, 2024. Framework for structured text generation using LLMs. URL: https://github.com/microsoft/markitdown.">Microsoft, 2024</a>]</span> is a Python package and CLI tool developed by the Microsoft AutoGen team for converting various file formats to Markdown. It supports a wide range of formats including PDF, PowerPoint, Word, Excel, images (with OCR and EXIF metadata), audio (with transcription), HTML, and other text-based formats making it a useful tool for document indexing and LLM-based applications.</p>
<p>Key features:</p>
<ul class="simple">
<li><p>Simple command-line and Python API interfaces</p></li>
<li><p>Support for multiple file formats</p></li>
<li><p>Optional LLM integration for enhanced image descriptions</p></li>
<li><p>Batch processing capabilities</p></li>
<li><p>Docker support for containerized usage</p></li>
</ul>
<p>Sample usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">markitdown</span> <span class="kn">import</span> <span class="n">MarkItDown</span>

<span class="n">md</span> <span class="o">=</span> <span class="n">MarkItDown</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">md</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;test.xlsx&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">text_content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="docling">
<h3><a class="toc-backref" href="#id296" role="doc-backlink"><span class="section-number">5.2.2. </span>Docling</a><a class="headerlink" href="#docling" title="Permalink to this heading">¶</a></h3>
<p>Docling <span id="id8">[<a class="reference internal" href="#id145" title="IBM Research. Docling: a document-level linguistic annotation framework. GitHub Repository, 2024. Framework for document-level linguistic annotation and analysis. URL: https://github.com/DS4SD/docling.">IBM Research, 2024</a>]</span> is a Python package developed by IBM Research for parsing and converting documents into various formats. It provides advanced document understanding capabilities with a focus on maintaining document structure and formatting.</p>
<p>Key features:</p>
<ul class="simple">
<li><p>Support for multiple document formats (PDF, DOCX, PPTX, XLSX, Images, HTML, etc.)</p></li>
<li><p>Advanced PDF parsing including layout analysis and table extraction</p></li>
<li><p>Unified document representation format</p></li>
<li><p>Integration with LlamaIndex and LangChain</p></li>
<li><p>OCR support for scanned documents</p></li>
<li><p>Simple CLI interface</p></li>
</ul>
<p>Sample usage:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">docling.document_converter</span> <span class="kn">import</span> <span class="n">DocumentConverter</span>

<span class="n">converter</span> <span class="o">=</span> <span class="n">DocumentConverter</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;document.pdf&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">document</span><span class="o">.</span><span class="n">export_to_markdown</span><span class="p">())</span>
</pre></div>
</div>
</section>
<section id="structured-data-extraction">
<h3><a class="toc-backref" href="#id297" role="doc-backlink"><span class="section-number">5.2.3. </span>Structured Data Extraction</a><a class="headerlink" href="#structured-data-extraction" title="Permalink to this heading">¶</a></h3>
<p>A common use case where document parsing matters is structured data extraction, particularly in the presence of complex formatting and layout. In this case study, we will extract the economic forecasts from Merrill Lynch’s CIO Capital Market Outlook released on December 16, 2024 <span id="id9">[<a class="reference internal" href="#id149" title="Merrill Lynch. Chief investment officer capital market outlook. CIO Weekly Letter, 2024. URL: https://olui2.fs.ml.com/publish/content/application/pdf/gwmol/me-cio-weekly-letter.pdf.">Merrill Lynch, 2024</a>]</span>. We will focus on page 7 of this document, which contains several economic variables organized in a mix of tables, text and images (see <a class="reference internal" href="#forecast"><span class="std std-numref">Fig. 5.1</span></a>).</p>
<figure class="align-center" id="forecast">
<a class="reference internal image-reference" href="../_images/forecast.png"><img alt="Forecast" src="../_images/forecast.png" style="width: 807.75px; height: 858.15px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.1 </span><span class="caption-text">Merrill Lynch’s CIO Capital Market Outlook released on December 16, 2024 <span id="id10">[<a class="reference internal" href="#id149" title="Merrill Lynch. Chief investment officer capital market outlook. CIO Weekly Letter, 2024. URL: https://olui2.fs.ml.com/publish/content/application/pdf/gwmol/me-cio-weekly-letter.pdf.">Merrill Lynch, 2024</a>]</span></span><a class="headerlink" href="#forecast" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FORECAST_FILE_PATH</span> <span class="o">=</span> <span class="s2">&quot;../data/input/forecast.pdf&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>First, we will use MarkItDown to extract the text content from the document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">markitdown</span> <span class="kn">import</span> <span class="n">MarkItDown</span>

<span class="n">md</span> <span class="o">=</span> <span class="n">MarkItDown</span><span class="p">()</span>
<span class="n">result_md</span> <span class="o">=</span> <span class="n">md</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">FORECAST_FILE_PATH</span><span class="p">)</span><span class="o">.</span><span class="n">text_content</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we will do the same with Docling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">docling.document_converter</span> <span class="kn">import</span> <span class="n">DocumentConverter</span>

<span class="n">converter</span> <span class="o">=</span> <span class="n">DocumentConverter</span><span class="p">()</span>
<span class="n">forecast_result_docling</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">source</span><span class="p">)</span><span class="o">.</span><span class="n">document</span><span class="o">.</span><span class="n">export_to_markdown</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>How similar are the two results? We can use use Levenshtein distance to measure the similarity between the two results. We will also calculate a naive score using the <code class="docutils literal notranslate"><span class="pre">SequenceMatcher</span></code> from the <code class="docutils literal notranslate"><span class="pre">difflib</span></code> package, which is a simple measure of similarity between two strings based on the number of matches in the longest common subsequence.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">Levenshtein</span>
<span class="k">def</span> <span class="nf">levenshtein_similarity</span><span class="p">(</span><span class="n">text1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">text2</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate normalized Levenshtein distance</span>
<span class="sd">    Returns value between 0 (completely different) and 1 (identical)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">distance</span> <span class="o">=</span> <span class="n">Levenshtein</span><span class="o">.</span><span class="n">distance</span><span class="p">(</span><span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">text1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">text2</span><span class="p">))</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">distance</span> <span class="o">/</span> <span class="n">max_len</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">difflib</span> <span class="kn">import</span> <span class="n">SequenceMatcher</span>
<span class="k">def</span> <span class="nf">simple_similarity</span><span class="p">(</span><span class="n">text1</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">text2</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate similarity ratio using SequenceMatcher</span>
<span class="sd">    Returns value between 0 (completely different) and 1 (identical)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">SequenceMatcher</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">text1</span><span class="p">,</span> <span class="n">text2</span><span class="p">)</span><span class="o">.</span><span class="n">ratio</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">levenshtein_similarity</span><span class="p">(</span><span class="n">forecast_result_md</span><span class="p">,</span> <span class="n">forecast_result_docling</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.13985705461925346
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simple_similarity</span><span class="p">(</span><span class="n">forecast_result_md</span><span class="p">,</span> <span class="n">forecast_result_docling</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.17779960707269155
</pre></div>
</div>
</div>
</div>
<p>It turns out that the two results are quite different, with a similarity score of about 13.98% and 17.77% for Levenshtein and <code class="docutils literal notranslate"><span class="pre">SequenceMatcher</span></code>, respectively.</p>
<p>Docling’s result is a quite readable markdown displaying key economic variables and their forecasts. Conversely, MarkItDown’s result is a bit messy and hard to read but the information is there just not in a structured format. Does it matter? That’s what we will explore next.</p>
<p><strong>Docling’s result</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">forecast_result_docling</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference internal" href="#id11"><span class="std std-numref">Fig. 5.2</span></a> shows part of the parsed result from Docling.</p>
<figure class="align-center" id="id11">
<a class="reference internal image-reference" href="../_images/docling.png"><img alt="Docling's result" src="../_images/docling.png" style="width: 1106.3999999999999px; height: 661.8px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.2 </span><span class="caption-text">Docling’s parsed result</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>MarkItDown’s result</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span><span class="p">,</span> <span class="n">Markdown</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">forecast_result_md</span><span class="p">[:</span><span class="mi">500</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference internal" href="#id12"><span class="std std-numref">Fig. 5.3</span></a> shows part of the parsed result from MarkItDown.</p>
<figure class="align-center" id="id12">
<a class="reference internal image-reference" href="../_images/markitdown.png"><img alt="MarkItDown's parsed result" src="../_images/markitdown.png" style="width: 1287.0px; height: 567.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.3 </span><span class="caption-text">MarkItDown’s parsed result</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Now, let’s focus on the economic forecasts. In particular, we are interested in extracting the CIO’s 2025E forecasts.</p>
<figure class="align-center" id="forecast2025">
<a class="reference internal image-reference" href="../_images/2025.png"><img alt="Forecast 2025" src="../_images/2025.png" style="width: 1010.25px; height: 293.85px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.4 </span><span class="caption-text">Merrill Lynch’s CIO Economic Forecasts.</span><a class="headerlink" href="#forecast2025" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We will define a <code class="docutils literal notranslate"><span class="pre">Forecast</span></code> pydantic model to represent an economic forecast composed of a <code class="docutils literal notranslate"><span class="pre">financial_variable</span></code> and a <code class="docutils literal notranslate"><span class="pre">financial_forecast</span></code>. We will also define a <code class="docutils literal notranslate"><span class="pre">EconForecast</span></code> pydantic model to represent the list of economic forecasts we want to extract from the document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">BaseModel</span>
<span class="k">class</span> <span class="nc">Forecast</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">financial_variable</span><span class="p">:</span> <span class="nb">str</span>
    <span class="n">financial_forecast</span><span class="p">:</span> <span class="nb">float</span>
<span class="k">class</span> <span class="nc">EconForecast</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">forecasts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Forecast</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We write a simple function to extract the economic forecasts from the document using an LLM model (with structured output) with the following prompt template, where <code class="docutils literal notranslate"><span class="pre">extract_prompt</span></code> represents the kind of data the user would like to extract and <code class="docutils literal notranslate"><span class="pre">doc</span></code> is the input document to analyze.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">BASE_PROMPT</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: You are an expert at structured data extraction. </span>
<span class="s2">    TASK: Extract the following data </span><span class="si">{</span><span class="n">extract_prompt</span><span class="si">}</span><span class="s2"> from input DOCUMENT</span>
<span class="s2">    FORMAT: The output should be a JSON object with &#39;financial_variable&#39; as key and &#39;financial_forecast&#39; as value.</span>
<span class="s2">    &quot;&quot;&quot;</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BASE_PROMPT</span><span class="si">}</span><span class="s2"> </span><span class="se">\n\n</span><span class="s2"> DOCUMENT: </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>  <span class="n">doc</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EconForecast</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extract data of a financial document using an LLM model.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        doc: The financial document text to analyze</span>
<span class="sd">        client: The LLM model to use for analysis</span>
<span class="sd">        extract_prompt: The prompt to use for extraction</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        EconForecasts object containing sentiment analysis results</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">BASE_PROMPT</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: You are an expert at structured data extraction. </span>
<span class="s2">    TASK: Extract the following data </span><span class="si">{</span><span class="n">extract_prompt</span><span class="si">}</span><span class="s2"> from input DOCUMENT</span>
<span class="s2">    FORMAT: The output should be a JSON object with &#39;financial_variable&#39; as key and &#39;financial_forecast&#39; as value.</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BASE_PROMPT</span><span class="si">}</span><span class="s2"> </span><span class="se">\n\n</span><span class="s2"> DOCUMENT: </span><span class="si">{</span><span class="n">doc</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="n">completion</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
                <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span>
            <span class="p">},</span>
            <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">doc</span><span class="p">}</span>
        <span class="p">],</span>
        <span class="n">response_format</span><span class="o">=</span><span class="n">EconForecast</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">parsed</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Load environment variables from .env file</span>
<span class="n">load_dotenv</span><span class="p">(</span><span class="n">override</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The user then calls the <code class="docutils literal notranslate"><span class="pre">extract_from_doc</span></code> function simply defining that “Economic Forecasts for 2025E” is the data they would like to extract from the document. We perform the extraction twice, once with MarkItDown and once with Docling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">extract_prompt</span> <span class="o">=</span> <span class="s2">&quot;Economic Forecasts for 2025E&quot;</span>
<span class="n">md_financials</span> <span class="o">=</span> <span class="n">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">,</span> <span class="n">forecast_result_md</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
<span class="n">docling_financials</span> <span class="o">=</span> <span class="n">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">,</span> <span class="n">forecast_result_docling</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The response is an <code class="docutils literal notranslate"><span class="pre">EconForecast</span></code> object containing a list of <code class="docutils literal notranslate"><span class="pre">Forecast</span></code> objects, as defined in the pydantic model. We can then convert the response to a pandas DataFrame for easier comparison.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">md_financials</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>EconForecast(forecasts=[Forecast(financial_variable=&#39;Real global GDP (% y/y annualized)&#39;, financial_forecast=3.2), Forecast(financial_variable=&#39;Real U.S. GDP (% q/q annualized)&#39;, financial_forecast=2.4), Forecast(financial_variable=&#39;CPI inflation (% y/y)&#39;, financial_forecast=2.5), Forecast(financial_variable=&#39;Core CPI inflation (% y/y)&#39;, financial_forecast=3.0), Forecast(financial_variable=&#39;Unemployment rate (%)&#39;, financial_forecast=4.3), Forecast(financial_variable=&#39;Fed funds rate, end period (%)&#39;, financial_forecast=3.88)])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_md_forecasts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">f</span><span class="o">.</span><span class="n">financial_variable</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">financial_forecast</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">md_financials</span><span class="o">.</span><span class="n">forecasts</span><span class="p">],</span> 
                      <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;Forecast&#39;</span><span class="p">])</span>
<span class="n">df_docling_forecasts</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">f</span><span class="o">.</span><span class="n">financial_variable</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">financial_forecast</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">docling_financials</span><span class="o">.</span><span class="n">forecasts</span><span class="p">],</span> 
                      <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;Forecast&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_md_forecasts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Variable</th>
      <th>Forecast</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Real global GDP (% y/y annualized)</td>
      <td>3.20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Real U.S. GDP (% q/q annualized)</td>
      <td>2.40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CPI inflation (% y/y)</td>
      <td>2.50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Core CPI inflation (% y/y)</td>
      <td>3.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Unemployment rate (%)</td>
      <td>4.30</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Fed funds rate, end period (%)</td>
      <td>3.88</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_docling_forecasts</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Variable</th>
      <th>Forecast</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Real global GDP (% y/y annualized)</td>
      <td>3.20</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Real U.S. GDP (% q/q annualized)</td>
      <td>2.40</td>
    </tr>
    <tr>
      <th>2</th>
      <td>CPI inflation (% y/y)</td>
      <td>2.50</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Core CPI inflation (% y/y)</td>
      <td>3.00</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Unemployment rate (%)</td>
      <td>4.30</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Fed funds rate, end period (%)</td>
      <td>3.88</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The results from MarkItDown and Docling are identical and accurately match the true values from the document. This demonstrates that despite MarkItDown’s output appearing less readable from a human perspective, both approaches enabled the LLM to successfully extract the economic forecast data with equal accuracy, in this particular case.</p>
<p>Now, let’s focus on the asset class weightings. We will extract the asset class weightings from the document and compare the results from MarkItDown and Docling. The information now is presented in a quite different structure as we can see in <a class="reference internal" href="#asset-class"><span class="std std-ref">Asset Class Weightings</span></a>. The CIO view information is represented in a spectrum starting with “Underweight”, passing through “Neutral” and reaching “Overweight”. The actual view is marked by some colored dots in the chart. Let’s see if we can extract this relatively more complex information from the document.</p>
<figure class="align-center" id="asset-class">
<a class="reference internal image-reference" href="../_images/asset_class.png"><img alt="Asset Class Weightings" src="../_images/asset_class.png" style="width: 575.0px; height: 739.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.5 </span><span class="caption-text">Asset Class Weightings</span><a class="headerlink" href="#asset-class" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The user will simply define the following data to extract: “Asset Class Weightings (as of 12/3/2024) in a scale from -2 to 2”. In that way, we expect that “Underweight” will be mapped to -2, “Neutral” to 0 and “Overweight” to 2 with some values in between.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">extract_prompt</span> <span class="o">=</span> <span class="s2">&quot;Asset Class Weightings (as of 12/3/2024) in a scale from -2 to 2&quot;</span>
<span class="n">asset_class_docling</span> <span class="o">=</span> <span class="n">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">,</span> <span class="n">forecast_result_docling</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
<span class="n">asset_class_md</span> <span class="o">=</span> <span class="n">extract_from_doc</span><span class="p">(</span><span class="n">extract_prompt</span><span class="p">,</span> <span class="n">forecast_result_md</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_md</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">f</span><span class="o">.</span><span class="n">financial_variable</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">financial_forecast</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">asset_class_md</span><span class="o">.</span><span class="n">forecasts</span><span class="p">],</span> 
                 <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;Forecast&#39;</span><span class="p">])</span>
<span class="n">df_docling</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([(</span><span class="n">f</span><span class="o">.</span><span class="n">financial_variable</span><span class="p">,</span> <span class="n">f</span><span class="o">.</span><span class="n">financial_forecast</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">asset_class_docling</span><span class="o">.</span><span class="n">forecasts</span><span class="p">],</span> 
                 <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">,</span> <span class="s1">&#39;Forecast&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We construct a DataFrame to compare the results from MarkItDown and Docling with an added “true_value” column containing the true values from the document, which we extracted manually from the chart. This enables us to calculate accuracy of the structured data extraction task in case.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create DataFrame with specified columns</span>
<span class="n">df_comparison</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;variable&#39;</span><span class="p">:</span> <span class="n">df_docling</span><span class="p">[</span><span class="s1">&#39;Variable&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="s1">&#39;markitdown&#39;</span><span class="p">:</span> <span class="n">df_md</span><span class="p">[</span><span class="s1">&#39;Forecast&#39;</span><span class="p">],</span>
    <span class="s1">&#39;docling&#39;</span><span class="p">:</span> <span class="n">df_docling</span><span class="p">[</span><span class="s1">&#39;Forecast&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>  <span class="c1"># Drop last row</span>
    <span class="s1">&#39;true_value&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">]</span>
<span class="p">})</span>

<span class="n">display</span><span class="p">(</span><span class="n">df_comparison</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variable</th>
      <th>markitdown</th>
      <th>docling</th>
      <th>true_value</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Global Equities</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>U.S. Large Cap Growth</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>U.S. Large Cap Value</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>U.S. Small Cap Growth</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>U.S. Small Cap Value</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>International Developed</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Emerging Markets</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Global Fixed Income</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>U.S. Governments</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>U.S. Mortgages</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>U.S. Corporates</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>International Fixed Income</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>12</th>
      <td>High Yield</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>U.S. Investment-grade</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Tax Exempt U.S. High Yield Tax Exempt</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate accuracy for markitdown and docling</span>
<span class="n">markitdown_accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_comparison</span><span class="p">[</span><span class="s1">&#39;markitdown&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">df_comparison</span><span class="p">[</span><span class="s1">&#39;true_value&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">docling_accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_comparison</span><span class="p">[</span><span class="s1">&#39;docling&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">df_comparison</span><span class="p">[</span><span class="s1">&#39;true_value&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Markitdown accuracy: </span><span class="si">{</span><span class="n">markitdown_accuracy</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Docling accuracy: </span><span class="si">{</span><span class="n">docling_accuracy</span><span class="si">:</span><span class="s2">.2%</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Markitdown accuracy: 53.33%
Docling accuracy: 93.33%
</pre></div>
</div>
</div>
</div>
<p>We observe that Docling performs significantly better at 93.33% accuracy missing only one value. MarkItDown achieves 53.33% accuracy struggling with nuanced asset class weightings. In this case, Docling’s structured parsed output did help the LLM to extract the information more accurately compared to MarkItDown’s unstructured output. Hence, in this case, the strategy used to parse the data did impact the LLM’s ability to extract structured information. Having said that, it is important to mention that a more robust analysis would run data extraction on a large sample data a number of repeated runs to estimate error rates since results are non-deterministic.</p>
<p>What if we want to systematically extract all tables from the document? We can use Docling to do that by simply accessing the <code class="docutils literal notranslate"><span class="pre">tables</span></code> attribute of the <code class="docutils literal notranslate"><span class="pre">DocumentConverter</span></code> object.</p>
<p>By doing that, we observe that Docling extracted 7 tables from the document exporting tables from top down and left to right in order of appearance in the document.
Below, we display the first two and the last tables. We can see the first table successfully extracted for Equities forecasts, the second one for Fixed Income forecasts as well as the last table, which contains CIO Equity Sector Views.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">docling.document_converter</span> <span class="kn">import</span> <span class="n">DocumentConverter</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">convert_and_export_tables</span><span class="p">(</span><span class="n">file_path</span><span class="p">:</span> <span class="n">Path</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Convert document and export tables to DataFrames.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        file_path: Path to input document</span>
<span class="sd">        </span>
<span class="sd">    Returns:</span>
<span class="sd">        List of pandas DataFrames containing the tables</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">doc_converter</span> <span class="o">=</span> <span class="n">DocumentConverter</span><span class="p">()</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    
    <span class="n">conv_res</span> <span class="o">=</span> <span class="n">doc_converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>
    
    <span class="n">tables</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Export tables</span>
    <span class="k">for</span> <span class="n">table</span> <span class="ow">in</span> <span class="n">conv_res</span><span class="o">.</span><span class="n">document</span><span class="o">.</span><span class="n">tables</span><span class="p">:</span>
        <span class="n">table_df</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">export_to_dataframe</span><span class="p">()</span>
        <span class="n">tables</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">table_df</span><span class="p">)</span>

    <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Document converted in </span><span class="si">{</span><span class="n">end_time</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> seconds.&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">tables</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert and export tables</span>
<span class="n">tables</span> <span class="o">=</span> <span class="n">convert_and_export_tables</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">FORECAST_FILE_PATH</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">tables</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>7
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">tables</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Total Return in USD (%).Current</th>
      <th>Total Return in USD (%).WTD</th>
      <th>Total Return in USD (%).MTD</th>
      <th>Total Return in USD (%).YTD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>DJIA</td>
      <td>43,828.06</td>
      <td>-1.8</td>
      <td>-2.3</td>
      <td>18.4</td>
    </tr>
    <tr>
      <th>1</th>
      <td>NASDAQ</td>
      <td>19,926.72</td>
      <td>0.4</td>
      <td>3.7</td>
      <td>33.7</td>
    </tr>
    <tr>
      <th>2</th>
      <td>S&amp;P 500</td>
      <td>6,051.09</td>
      <td>-0.6</td>
      <td>0.4</td>
      <td>28.6</td>
    </tr>
    <tr>
      <th>3</th>
      <td>S&amp;P 400 Mid Cap</td>
      <td>3,277.20</td>
      <td>-1.6</td>
      <td>-2.6</td>
      <td>19.5</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Russell 2000</td>
      <td>2,346.90</td>
      <td>-2.5</td>
      <td>-3.5</td>
      <td>17.3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>MSCI World</td>
      <td>3,817.24</td>
      <td>-1.0</td>
      <td>0.2</td>
      <td>22.1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>MSCI EAFE</td>
      <td>2,319.05</td>
      <td>-1.5</td>
      <td>0.2</td>
      <td>6.4</td>
    </tr>
    <tr>
      <th>7</th>
      <td>MSCI Emerging Markets</td>
      <td>1,107.01</td>
      <td>0.3</td>
      <td>2.7</td>
      <td>10.6</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">tables</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Total Return in USD (%).Current</th>
      <th>Total Return in USD (%).WTD</th>
      <th>Total Return in USD (%).MTD</th>
      <th>Total Return in USD (%).YTD</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Corporate &amp; Government</td>
      <td>4.66</td>
      <td>-1.34</td>
      <td>-0.92</td>
      <td>1.94</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Agencies</td>
      <td>4.54</td>
      <td>-0.58</td>
      <td>-0.31</td>
      <td>3.35</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Municipals</td>
      <td>3.55</td>
      <td>-0.87</td>
      <td>-0.54</td>
      <td>1.99</td>
    </tr>
    <tr>
      <th>3</th>
      <td>U.S. Investment Grade Credit</td>
      <td>4.79</td>
      <td>-1.38</td>
      <td>-0.93</td>
      <td>1.97</td>
    </tr>
    <tr>
      <th>4</th>
      <td>International</td>
      <td>5.17</td>
      <td>-1.40</td>
      <td>-0.90</td>
      <td>3.20</td>
    </tr>
    <tr>
      <th>5</th>
      <td>High Yield</td>
      <td>7.19</td>
      <td>-0.22</td>
      <td>0.20</td>
      <td>8.87</td>
    </tr>
    <tr>
      <th>6</th>
      <td>90 Day Yield</td>
      <td>4.32</td>
      <td>4.39</td>
      <td>4.49</td>
      <td>5.33</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2 Year Yield</td>
      <td>4.24</td>
      <td>4.10</td>
      <td>4.15</td>
      <td>4.25</td>
    </tr>
    <tr>
      <th>8</th>
      <td>10 Year Yield</td>
      <td>4.40</td>
      <td>4.15</td>
      <td>4.17</td>
      <td>3.88</td>
    </tr>
    <tr>
      <th>9</th>
      <td>30 Year Yield</td>
      <td>4.60</td>
      <td>4.34</td>
      <td>4.36</td>
      <td>4.03</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">tables</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Sector</th>
      <th>CIO View.</th>
      <th>CIO View.Underweight</th>
      <th>CIO View.Neutral</th>
      <th>CIO View.</th>
      <th>CIO View.Overweight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Utilities</td>
      <td>slight over weight green   </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>1</th>
      <td>Financials</td>
      <td>slight over weight green   </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>2</th>
      <td>Healthcare</td>
      <td>slight over weight green   </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>3</th>
      <td>Consumer  Discretionary</td>
      <td>Slight over weight green  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>4</th>
      <td>Information  Technology</td>
      <td>Neutral yellow  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>5</th>
      <td>Communication  Services</td>
      <td>Neutral yellow  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>6</th>
      <td>Industrials</td>
      <td>Neutral yellow  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>7</th>
      <td>Real Estate</td>
      <td>Neutral yellow  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>8</th>
      <td>Energy</td>
      <td>slight underweight orange  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>9</th>
      <td>Materials</td>
      <td>slight underweight orange  </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <th>10</th>
      <td>Consumer  Staples</td>
      <td>underweight red</td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Coming back to MarkItDown, one interesting feature to explore is the ability to extract information from images by passing an image capable LLM model to its constructor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">md_llm</span> <span class="o">=</span> <span class="n">MarkItDown</span><span class="p">(</span><span class="n">llm_client</span><span class="o">=</span><span class="n">client</span><span class="p">,</span> <span class="n">llm_model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">md_llm</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s2">&quot;../data/input/forecast.png&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here’s the description we obtain from the image of our input document.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">text_content</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<h1 class="rubric" id="description">Description:</h1>
<p><strong>Markets in Review: Economic Forecasts and Asset Class Weightings (as of 12/13/2024)</strong></p>
<p>This detailed market overview presents key performance metrics and economic forecasts as of December 13, 2024.</p>
<p><strong>Equities Overview:</strong></p>
<ul class="simple">
<li><p><strong>Total Returns:</strong> Highlights returns for major indices such as the DJIA (18.4% YTD), NASDAQ (33.7% YTD), and S&amp;P 500 (28.6% YTD), showcasing strong performance across the board.</p></li>
<li><p><strong>Forecasts:</strong> Economic indicators reveal a projected real global GDP growth of 3.1%, with inflation rates expected to stabilize around 2.2% in 2025. Unemployment rates are anticipated to remain low at 4.4%.</p></li>
</ul>
<p><strong>Fixed Income:</strong></p>
<ul class="simple">
<li><p>Focuses on various segments, including Corporate &amp; Government bonds, which offer an annualized return of 4.66% and indicate shifting trends in interest rates over 2-Year (4.25%) and 10-Year (4.03%) bonds.</p></li>
</ul>
<p><strong>Commodities &amp; Currencies:</strong></p>
<ul class="simple">
<li><p>Commodities such as crude oil and gold show varied performance, with oil increasing by 4.8% and gold prices sitting at $2,648.23 per ounce.</p></li>
<li><p>Currency metrics highlight the Euro and USD trends over the past year.</p></li>
</ul>
<p><strong>S&amp;P Sector Returns:</strong></p>
<ul class="simple">
<li><p>A quick reference for sector performance indicates a significant 2.5% return in Communication Services, while other sectors like Consumer Staples and Materials display minor fluctuations.</p></li>
</ul>
<p><strong>CIO Asset Class Weightings:</strong></p>
<ul class="simple">
<li><p>Emphasizes strategic asset allocation recommendations which are crucial for an investor’s portfolio. Underweight positions in U.S. Small Cap Growth and International Developed contrast with overweight positions in certain sectors such as Utilities and Financials, signaling tactical shifts based on ongoing economic assessments.</p></li>
</ul>
<p><strong>Note:</strong> This summary is sourced from BofA Global Research and aims to provide a comprehensive view of current market conditions and forecasts to assist investors in making informed decisions.</p>
</div>
</div>
<hr class="docutils" />
<p>Overall, the description is somewhat accurate but contains a few inaccuracies including:</p>
<ul class="simple">
<li><p>For the sector weightings, the description states there are “underweight positions in U.S. Small Cap Growth” but looking at the Asset Class Weightings chart, U.S. Small Cap Growth actually shows an overweight position (green circle).</p></li>
<li><p>The description mentions “overweight positions in certain sectors such as Utilities and Financials” but looking at the CIO Equity Sector Views, both these sectors show neutral positions, not overweight positions.</p></li>
<li><p>For fixed income, the description cites a “10-Year (4.03%)” yield, but the image shows the 30-Year Yield at 4.03%, while the 10-Year Yield is actually 4.40%.</p></li>
</ul>
<p>Arguably, the description’s inaccuracies could be a consequence of the underlying LLM model’s inability to process the image.</p>
<p>We have covered MarkitDown and Docling as examples of open source tools that can help developers parse input data into a suitable format to LLMs. Other relevant open source tools worth mentioning include:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://Unstructured.io">Unstructured.io</a> <span id="id13">[<a class="reference internal" href="#id146" title="Unstructured.io. Unstructured: open source libraries for pre-processing documents. GitHub Repository, 2024. URL: https://github.com/Unstructured-IO/unstructured.">Unstructured.io, 2024</a>]</span>: A Python library for unstructured data extraction.</p></li>
<li><p>FireCrawl <span id="id14">[<a class="reference internal" href="#id147" title="Mendable AI. Firecrawl: a fast and efficient web crawler for llm training data. GitHub Repository, 2024. High-performance web crawler optimized for collecting LLM training data. URL: https://github.com/mendableai/firecrawl.">Mendable AI, 2024</a>]</span>: A Fast and Efficient Web Crawler for LLM Training Data.</p></li>
<li><p>LlamaParse <span id="id15">[<a class="reference internal" href="#id44" title="LlamaIndex. Llamaparse: extract structured data from text and pdfs using llms. 2024. LlamaParse. URL: https://github.com/run-llama/llama_parse.">LlamaIndex, 2024</a>]</span>: Llamaindex’s data parsing solution.</p></li>
</ul>
<p>The choice of tool depends on the specific requirements of the application and the nature of the input data. This choice should be taken as a critical decision of any data intensive LLM-based application and deserves dedicated research and evidence-based experimentation.</p>
</section>
</section>
<section id="retrieval-augmented-generation">
<h2><a class="toc-backref" href="#id298" role="doc-backlink"><span class="section-number">5.3. </span>Retrieval-Augmented Generation</a><a class="headerlink" href="#retrieval-augmented-generation" title="Permalink to this heading">¶</a></h2>
<p>What happens if we asked ChatGPT who’s the author of the book “Taming LLMs”?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Load environment variables from .env file</span>
<span class="n">load_dotenv</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o-mini&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;Who&#39;s the Author of the Book Taming LLMs?&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">parse</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;gpt-4o-mini&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">question</span><span class="p">}</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The book &quot;Taming LLMs&quot; is authored by *G. Arulkumaran, H. M. B. P. D. Karthikeyan, and I. A. M. Almasri.* If you need more information about the book or its contents, feel free to ask!
</pre></div>
</div>
</div>
</div>
<p>Turns out ChatGPT hallucinates. A quick web search on the before mentioned authors yields no results. In fact, those authors names are made up. And of course the correct answer would have been “Tharsis Souza”.</p>
<p>LLMs only have access to the information they have been trained on, which of course has been fixed at a point in time. Hence, LLMs operate with stale data. The problem gets exacerbated by the fact that LLMs are trained to provide an answer even if the answer is unknown by them, hence leading to hallucinations.</p>
<p>One solution to this problem is to use a retrieval system to fetch information from a knowledge base to provide recent and relevant context to user queries using so-called Retrieval Augmented Generation (RAG) system.</p>
<p>RAG utilizes a retrieval system to fetch external knowledge and augment LLM’s context. It is a useful technique for building LLM applications that require domain-specific information or knowledge-intensive tasks <span id="id16">[<a class="reference internal" href="#id143" title="Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. 2021. URL: https://arxiv.org/abs/2005.11401, arXiv:2005.11401.">Lewis <em>et al.</em>, 2021</a>]</span>. It has also proved effective in mitigating LLMs hallucinations <span id="id17">[<a class="reference internal" href="#id199" title="Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. When do LLMs need retrieval augmentation? mitigating LLMs' overconfidence helps retrieval augmentation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, 11375–11388. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: https://aclanthology.org/2024.findings-acl.675, doi:10.18653/v1/2024.findings-acl.675.">Ni <em>et al.</em>, 2024</a>, <a class="reference internal" href="#id193" title="Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large language models. In Proceedings of the ACM Web Conference 2024, WWW '24, 1453-1463. New York, NY, USA, 2024. Association for Computing Machinery. URL: https://doi.org/10.1145/3589334.3645481, doi:10.1145/3589334.3645481.">Zhou <em>et al.</em>, 2024</a>]</span>.</p>
<p>In the above example, a RAG would help with hallucinations by grounding the LLM’s response to information provided in the knowledge base. Additional common use cases of RAG systems include:</p>
<ol class="arabic simple">
<li><p><strong>Enterprise Knowledge Management</strong>: RAG enables organizations to synthesize answers from diverse internal data sources like documents, databases, and communication channels. This creates a unified knowledge interface that can accurately answer questions using the organization’s own data.</p></li>
<li><p><strong>Document Processing and Analysis</strong>: RAG excels at extracting and analyzing information from complex documents like financial reports, presentations, and spreadsheets. The system can enable LLMs to understand context and relationships across different document types and formats.</p></li>
<li><p><strong>Intelligent Customer Support</strong>: By combining knowledge bases with conversational abilities, RAG powers chatbots and support systems that can maintain context across chat history, provide accurate responses, and handle complex customer queries while reducing hallucinations.</p></li>
<li><p><strong>Domain-Specific Applications</strong>: RAG allows LLMs to be equipped with specialized knowledge in fields like medicine, law, or engineering by retrieving information from domain-specific literature, regulations, and technical documentation. This enables accurate responses aligned with professional standards and current best practices.</p></li>
<li><p><strong>Code Documentation and Technical Support</strong>: RAG can help developers by retrieving relevant code examples, API documentation, and best practices from repositories and documentation, which often suffer updates frequently, enabling more accurate and contextual coding assistance.</p></li>
</ol>
<p>If LLMs alone work on stale, general-purpose data with the added challenge of being prone to hallucinations, RAG systems serve as an added capability enabling LLMs to work on recent, domain-specific knowledge increasing the likelihood of LLMs to provide responses that are factual and relevant to user queries.</p>
<section id="rag-pipeline">
<h3><a class="toc-backref" href="#id299" role="doc-backlink"><span class="section-number">5.3.1. </span>RAG Pipeline</a><a class="headerlink" href="#rag-pipeline" title="Permalink to this heading">¶</a></h3>
<p>RAG architectures vary but they all share the same goal: to retrieve relevant information from a knowledge base to maximize the LLM’s ability to effectively and accurately respond to prompts, particularly when the answer requires out-of-training data information.</p>
<p>We will introduce key components of a RAG system one by one leading to a full canonical RAG pipeline at the end that ultimately will be used to answer our original question “Who’s the author of the book Taming LLMs?”, accurately.</p>
<p>The following basic components will be introduced (see <a class="reference internal" href="#id18"><span class="std std-numref">Fig. 5.6</span></a> for a visual representation):</p>
<ul class="simple">
<li><p>Vector Database</p>
<ul>
<li><p>Embeddings</p></li>
<li><p>Indexing</p></li>
</ul>
</li>
<li><p>Retrieval System including re-ranking</p></li>
<li><p>LLM Augmented Generation via in-context learning</p></li>
</ul>
<p>Data extraction, parsing and chunking are also part of a canonical pipeline as we prepare the knowledge base. Those are concepts that we have already explored in the previous sections, hence we will be succinct here. We will start by preparing the knowledge base.</p>
<figure class="align-center" id="id18">
<a class="reference internal image-reference" href="../_images/rag.svg"><img alt="RAG Pipeline" height="605" src="../_images/rag.svg" width="447" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.6 </span><span class="caption-text">Simplified RAG Pipeline</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="preparing-the-knowledge-base">
<h4><a class="toc-backref" href="#id300" role="doc-backlink"><span class="section-number">5.3.1.1. </span>Preparing the Knowledge Base</a><a class="headerlink" href="#preparing-the-knowledge-base" title="Permalink to this heading">¶</a></h4>
<p>Every RAG system requires a knowledge base. In our case, the knowledge base is a set of documents that we equip the LLM to answer our authorship question.</p>
<p>Hence, we will compose our knowledge base by adding the web version of (some of the chapters of) the book “Taming LLMs”, namely:</p>
<ul class="simple">
<li><p>Introduction</p></li>
<li><p>Structured Output</p></li>
<li><p>Input (this very chapter)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">book_url</span> <span class="o">=</span> <span class="s2">&quot;https://www.tamingllms.com/&quot;</span>
<span class="n">chapters</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;markdown/intro.html&quot;</span><span class="p">,</span>
            <span class="s2">&quot;notebooks/structured_output.html&quot;</span><span class="p">,</span>
            <span class="s2">&quot;notebooks/input.html&quot;</span><span class="p">]</span>

<span class="n">chapter_urls</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">book_url</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">chapter</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">chapter</span> <span class="ow">in</span> <span class="n">chapters</span><span class="p">]</span>
<span class="n">chapter_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">chapter</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.html&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">chapter</span> <span class="ow">in</span> <span class="n">chapters</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>We use <code class="docutils literal notranslate"><span class="pre">Docling</span></code> to download the chapters from the web and parse them as markdown files.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">chapters</span> <span class="o">=</span> <span class="p">[</span><span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">chapter_url</span><span class="p">)</span><span class="o">.</span><span class="n">document</span><span class="o">.</span><span class="n">export_to_markdown</span><span class="p">()</span> <span class="k">for</span> <span class="n">chapter_url</span> <span class="ow">in</span> <span class="n">chapter_urls</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Now we are ready to store the chapters in a vector database to enable the construction of a retrieval system.</p>
</section>
<section id="vector-database">
<h4><a class="toc-backref" href="#id301" role="doc-backlink"><span class="section-number">5.3.1.2. </span>Vector Database</a><a class="headerlink" href="#vector-database" title="Permalink to this heading">¶</a></h4>
<p>Vector databases are specialized databases designed to store and retrieve high-dimensional vectors, which are mathematical representations of data like text, images, or audio. These databases are optimized for similarity search operations, making them ideal for embeddings-based retrieval systems.</p>
<p>A typical pipeline involving a vector database includes the following:</p>
<ol class="arabic simple">
<li><p>Input data is converted into “documents” forming a collection representing our knowledge base</p></li>
<li><p>Each document is converted into an embedding which are stored in the vector database</p></li>
<li><p>Embeddings are indexed in the vector database for efficient similarity search</p></li>
<li><p>The vector database is queried to retrieve the most relevant documents</p></li>
<li><p>The retrieved documents are used to answer questions</p></li>
</ol>
<p>Vector databases are not a mandatory component of RAG systems. In fact, we can use a simple list of strings to store the chapters (or their chunks) and then use the LLM to answer questions about the document. However, vector databases are useful for RAG applications as they enable:</p>
<ul class="simple">
<li><p>Fast similarity search for finding relevant context</p></li>
<li><p>Efficient storage of document embeddings</p></li>
<li><p>Scalable retrieval for large document collections</p></li>
<li><p>Flexible querying with metadata filters</p></li>
</ul>
<p>In that way, RAG applications can be seen as a retrieval system that uses a vector database to store and retrieve embeddings of documents, which in turn are used to augment LLMs with contextually relevant information as we will see in the next sections.</p>
<p>Here, we will use ChromaDB <span id="id19">[<a class="reference internal" href="#id287" title="ChromaDB. Chromadb documentation. Website, 2024b. URL: https://docs.trychroma.com/.">ChromaDB, 2024b</a>]</span> as an example of an open source vector database but key features and concepts we cover are applicable to other vector databases, in general.</p>
<p>ChromaDB is a popular open-source vector database that offers:</p>
<ul class="simple">
<li><p>Efficient storage and retrieval of embeddings</p></li>
<li><p>Support for metadata and filtering</p></li>
<li><p>Easy integration with Python applications</p></li>
<li><p>In-memory and persistent storage options</p></li>
<li><p>Support for multiple distance metrics</p></li>
</ul>
<p>Other notable vector databases include Weaviate, FAISS, and Milvus.</p>
<p>In ChromaDB, we can create a vector database client as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">chromadb</span>
<span class="n">chroma_client</span> <span class="o">=</span> <span class="n">chromadb</span><span class="o">.</span><span class="n">Client</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>This will create a vector database in memory. We can also create a persistent vector database by specifying a path to a directory or alternatively by using a cloud-based vector database service like AWS, Azure or GCP. We will use a vector database in memory for this example.</p>
<p>Next, we create a collection to store the embeddings of the chapters. And add our chapters as documents to the collection as follows.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">collection</span> <span class="o">=</span> <span class="n">chroma_client</span><span class="o">.</span><span class="n">create_collection</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;taming_llms&quot;</span><span class="p">)</span>

<span class="n">collection</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">chapters</span><span class="p">,</span>
    <span class="n">ids</span><span class="o">=</span><span class="n">chapter_ids</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We are ready to query the collection. We write a simple function that takes the collection, input query and number of retrieved results as argument and returns the retrieved documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">query_collection</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">query_text</span><span class="p">,</span> <span class="n">n_results</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">collection</span><span class="o">.</span><span class="n">query</span><span class="p">(</span>
        <span class="n">query_texts</span><span class="o">=</span><span class="p">[</span><span class="n">query_text</span><span class="p">],</span>
        <span class="n">n_results</span><span class="o">=</span><span class="n">n_results</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
</div>
</div>
<p>We write a simple query, enquiring the purpose of the book.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="s2">&quot;What is the purpose of this book?&quot;</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">query_collection</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ids&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">([[</span><span class="s1">&#39;intro&#39;</span><span class="p">,</span> <span class="s1">&#39;input&#39;</span><span class="p">,</span> <span class="s1">&#39;structured_output&#39;</span><span class="p">]])</span>
</pre></div>
</div>
</div>
</div>
<p>As response, we obtain an object that contains several attributes including:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">documents</span></code>: The actual documents retrieved from the collection, i.e. the chapters</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ids</span></code>: The ids of the documents retrieved from the collection</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">distances</span></code>: The distances of the documents to the query vector</p></li>
</ul>
<p>We can see that the chapters “Introduction”, “Input” and “Structured Output” are retrieved from the collection ordered by their distance to the query vector.</p>
<p>We observe that the Introduction chapter is the most relevant one as it ranks first, followed by the Input and Structured Output chapters. Indeed, the purpose of the book is included in the Introduction chapter demonstrating the retrieval system successfully retrieved the most relevant document to the input query, in this simple example.</p>
<p>In order to understand how the retrieval system works and how the “distance to the query vector” is computed, we need to understand how the embeddings are created and how the documents are indexed.</p>
<p><strong>Embeddings</strong></p>
<p>Embeddings are numerical representations of data (including text, images, audio, etc.) that capture meaning, allowing machines to process data quantitatively. Each embedding can be represented as a vector of floating-point numbers such that embedded data with similar meanings produce similar, i.e. close, vectors <a class="footnote-reference brackets" href="#embeddings-definition" id="id20" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p>
<p>For text data, small distances among embeddings suggest high semantic relatedness and large distances suggest low semantic relatedness among the embedded texts. HuggingFace provides a leaderboard of embeddings models <span id="id21">[<a class="reference internal" href="#id283" title="HuggingFace. Massive text embedding benchmark (mteb) leaderboard. Website, 2024i. URL: https://huggingface.co/spaces/mteb/leaderboard.">HuggingFace, 2024i</a>]</span>, which are ranked by in dimensions such as classification, clustering and reranking performance.</p>
<p>Behind the scenes, ChromaDB is using the model <code class="docutils literal notranslate"><span class="pre">all-MiniLM-L6-v2</span></code> by default <a class="footnote-reference brackets" href="#chroma-embeddings" id="id22" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a> to create embeddings for the input documents and the query (see <a class="reference internal" href="#embedding"><span class="std std-numref">Fig. 5.7</span></a>). This model is available in <code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code> <span id="id23">[<a class="reference internal" href="#id286" title="HuggingFace. Sentence transformers. Website, 2024f. URL: https://huggingface.co/sentence-transformers.">HuggingFace, 2024f</a>]</span>. Let’s see how it works.</p>
<figure class="align-center" id="embedding">
<a class="reference internal image-reference" href="../_images/embedding.svg"><img alt="Embedding" height="314" src="../_images/embedding.svg" width="891" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.7 </span><span class="caption-text">Embedding</span><a class="headerlink" href="#embedding" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>

<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">SentenceTransformer</span><span class="p">(</span><span class="s1">&#39;all-MiniLM-L6-v2&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We replicate what ChromaDB did by embedding our chapters as well as input query using sentence transformers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="s2">&quot;What is the purpose of this book?&quot;</span>
<span class="n">docs_to_embed</span> <span class="o">=</span> <span class="p">[</span><span class="n">q</span><span class="p">]</span> <span class="o">+</span> <span class="n">chapters</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">docs_to_embed</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(4, 384)
</pre></div>
</div>
</div>
</div>
<p>As a result, we obtain four 384-dimensional vectors representing our embeddings (one for each of the three chapters and one for the input query).</p>
<p>Now we can calculate similarity among the embeddings. By default, sentence transformers uses cosine similarity to calculate the similarity between embeddings.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">similarities</span> <span class="o">=</span> <span class="n">embedding_model</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
<span class="n">similarities</span>
</pre></div>
</div>
</div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.4402</span><span class="p">,</span> <span class="mf">0.3022</span><span class="p">,</span> <span class="mf">0.4028</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.4402</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.6606</span><span class="p">,</span> <span class="mf">0.5807</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.3022</span><span class="p">,</span> <span class="mf">0.6606</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.6313</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.4028</span><span class="p">,</span> <span class="mf">0.5807</span><span class="p">,</span> <span class="mf">0.6313</span><span class="p">,</span> <span class="mf">1.0000</span><span class="p">]])</span>
</pre></div>
</div>
<p>Let’s visualize the similarity matrix to better understand the relationships between our documents in <a class="reference internal" href="#similarities"><span class="std std-numref">Fig. 5.8</span></a>. The top row of the matrix represents the similarity of the input query against all chapters. That’s exactly what we previously obtained by querying ChromaDB which returned a response with documents ranked by similarity to input query.</p>
<figure class="align-center" id="similarities">
<a class="reference internal image-reference" href="../_images/similarity.png"><img alt="Similarity matrix heatmap" src="../_images/similarity.png" style="width: 604.8000000000001px; height: 531.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.8 </span><span class="caption-text">Similarity matrix heatmap showing relationships among query and chapters.</span><a class="headerlink" href="#similarities" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Calculating similarity among embeddings can become computationally intensive if brute force is used, i.e. pair-wise computation, as the number of documents grows in the knowledge base. Indexing is a technique to help address this challenge.</p>
<p><strong>Indexing</strong></p>
<p>Indexing is a crucial optimization technique that makes similarity searches faster and more efficient.</p>
<p>Without indexing, finding similar vectors would require an exhaustive search - comparing a query vector against every single vector in the database. For large datasets, this becomes prohibitively slow.</p>
<p>Common indexing strategies include:</p>
<ol class="arabic simple">
<li><p><strong>Tree-based Indexes</strong></p>
<ul class="simple">
<li><p>Examples include KD-trees and Ball trees</p></li>
<li><p>Work by partitioning the vector space into hierarchical regions</p></li>
<li><p>Effective for low-dimensional data but suffer from the “curse of dimensionality”</p></li>
</ul>
</li>
<li><p><strong>Graph-based Indexes</strong></p>
<ul class="simple">
<li><p>HNSW (Hierarchical Navigable Small World) is a prominent example</p></li>
<li><p>Creates a multi-layered graph structure for navigation</p></li>
<li><p>Offers excellent search speed but requires more memory</p></li>
</ul>
</li>
<li><p><strong>LSH (Locality-Sensitive Hashing)</strong></p>
<ul class="simple">
<li><p>Uses hash functions that map similar vectors to the same buckets</p></li>
<li><p>More memory-efficient than graph-based methods</p></li>
<li><p>May sacrifice some accuracy for performance</p></li>
</ul>
</li>
<li><p><strong>Quantization-based Indexes</strong></p>
<ul class="simple">
<li><p>Product Quantization compresses vectors by encoding them into discrete values</p></li>
<li><p>Reduces memory footprint significantly</p></li>
<li><p>Good balance between accuracy and resource usage</p></li>
</ul>
</li>
</ol>
<p>HNSW is the underlying library for Chroma vector indexing and search <span id="id24">[<a class="reference internal" href="#id289" title="ChromaDB. Chromadb cookbook: hnsw configuration. Website, 2024a. URL: https://cookbook.chromadb.dev/core/configuration/#hnsw-configuration.">ChromaDB, 2024a</a>]</span>. HNSW provides fast searches with high accuracy but uses more memory. LSH and quantization methods offer better memory efficiency but may sacrifice some precision.</p>
<p>But are indexing + basic embeddings based similarity sufficient? Often not, as we will see next as we cover reranking technique.</p>
</section>
<section id="reranking">
<h4><a class="toc-backref" href="#id302" role="doc-backlink"><span class="section-number">5.3.1.3. </span>Reranking</a><a class="headerlink" href="#reranking" title="Permalink to this heading">¶</a></h4>
<p>Let’s go back to querying our vector database. Here are additional examples.</p>
<p>First, we write a query about how to get structured output from LLMs. Successfully retrieving the “Structured Output” chapter from the book as top result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="s2">&quot;How to get structured output from LLMs?&quot;</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">query_collection</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ids&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[&#39;structured_output&#39;, &#39;input&#39;, &#39;intro&#39;]]
</pre></div>
</div>
</div>
</div>
<p>Next, we would like to obtain a tutorial on <code class="docutils literal notranslate"><span class="pre">Docling</span></code>, a tool we covered in this very chapter. However, we fail to obtain the correct chapter and instead obtain the “Introduction” chapter as a result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="s2">&quot;Docling tutorial&quot;</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">query_collection</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
<span class="n">res</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;ids&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[&#39;intro&#39;, &#39;input&#39;, &#39;structured_output&#39;]]
</pre></div>
</div>
</div>
</div>
<p>Retrieval systems solely based on vector similarity search might miss semantic relevance. That brings the need for techniques that can improve accuracy of the retrieval system. One such technique is re-ranking.</p>
<p>Re-ranking is a method that can improve accuracy of the retrieval system by re-ranking the retrieved documents based on their relevance to the input query.</p>
<p>In the following, we will use the <code class="docutils literal notranslate"><span class="pre">sentence_transformers</span></code> library to re-rank the retrieved documents based on their relevance to the input query. We utilize the <code class="docutils literal notranslate"><span class="pre">CrossEncoder</span></code> model to re-rank the documents. Cross-Encoder models are more accurate at judging relevance at the cost of speed compared to basic vector-based similarity.</p>
<p>We can implement a reranking step in a RAG system using a Cross-Encoder model in the following steps:</p>
<ol class="arabic simple">
<li><p>First, we initialize the Cross-Encoder model:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">CrossEncoder</span><span class="p">(</span><span class="s1">&#39;cross-encoder/ms-marco-MiniLM-L-6-v2&#39;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Uses the <code class="docutils literal notranslate"><span class="pre">ms-marco-MiniLM-L-6-v2</span></code> model, which is specifically trained for passage reranking</p></li>
<li><p>Sets a maximum sequence length of 512 tokens</p></li>
<li><p>This model is designed to score the relevance between query-document pairs</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Then we perform the reranking:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([(</span><span class="n">q</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Creates pairs of (query, document) for each retrieved document</p></li>
<li><p>The model predicts relevance scores for each pair</p></li>
<li><p>Higher scores indicate better semantic match between query and document</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Finally, we select the best match:</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)])</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">np.argmax(scores)</span></code> finds the index of the highest scoring document</p></li>
<li><p>Uses that index to retrieve the most relevant document</p></li>
</ul>
<p>We obtain the following scores for the retrieved documents (“intro”, “input”, “structured_output”), the higher the score, the more relevant the document is in relation to the input query.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">8.52623</span> <span class="p">,</span> <span class="o">-</span><span class="mf">6.328738</span><span class="p">,</span> <span class="o">-</span><span class="mf">8.750055</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<p>As a result, we obtain the index of the highest scoring document, which corresponds to the “input” chapter. Hence, the re-ranking step successfully retrieved the correct chapter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">[</span><span class="s2">&quot;ids&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>input
</pre></div>
</div>
</div>
</div>
<p>The ideia is to first run semantic similarity on embeddings, which should be fast but potentially inaccurate, and then run re-raking on the top-k results, which is more accurate but slower. By doing so, we can balance the speed and accuracy of the retrieval system.</p>
<p>Hence, instead of going over all retrieved documents:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([(</span><span class="n">q</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
<p>We would run reranking on the TOPK results, where TOPK &lt;&lt;&lt; number of documents:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([(</span><span class="n">q</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="n">TOPK</span><span class="p">]])</span>
</pre></div>
</div>
</section>
<section id="llms-with-rag">
<h4><a class="toc-backref" href="#id303" role="doc-backlink"><span class="section-number">5.3.1.4. </span>LLMs with RAG</a><a class="headerlink" href="#llms-with-rag" title="Permalink to this heading">¶</a></h4>
<p>We are finally ready to use the retrieval system to help the LLM answer our authorship question. A common way to integrate RAGs with LLMs is via in-context learning. With in-context learning the LLM learns from the retrieved documents by providing them in the context window as represented in <a class="reference internal" href="#incontext"><span class="std std-numref">Fig. 5.9</span></a>. This is accomplished via a prompt template structure as follows.</p>
<figure class="align-center" id="incontext">
<a class="reference internal image-reference" href="../_images/incontext.svg"><img alt="In-Context Learning" height="266" src="../_images/incontext.svg" width="419" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.9 </span><span class="caption-text">RAG LLM with In-Context Learning</span><a class="headerlink" href="#incontext" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">rag_system_prompt_template</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2"> You are a helpful assistant that answers questions based on the provided CONTEXT.</span>

<span class="s2"> CONTEXT: </span><span class="si">{</span><span class="n">context</span><span class="si">}</span>
<span class="s2"> &quot;&quot;&quot;</span>

 <span class="n">user_prompt_template</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2"> QUESTION: </span><span class="si">{</span><span class="nb">input</span><span class="si">}</span>
<span class="s2"> &quot;&quot;&quot;</span>
</pre></div>
</div>
<p>This prompt strategy demonstrates a common in-context learning pattern where retrieved documents are incorporated into the LLM’s context to enhance response accuracy and relevance. The prompt structure typically consists of a system prompt that:</p>
<ul class="simple">
<li><p>Sets clear boundaries for the LLM to use information from the provided context</p></li>
<li><p>Includes the retrieved documents as context</p></li>
</ul>
<p>This approach:</p>
<ul class="simple">
<li><p>Reduces hallucination by grounding responses in source documents</p></li>
<li><p>Improves answer relevance by providing contextually relevant information to the LLM</p></li>
</ul>
<p>The context variable is typically populated with the highest-scoring document(s) from the retrieval step, while the input variable contains the user’s original query.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">RAG_qa</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">context</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a summary of input using a given model</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rag_system_prompt_template</span> <span class="o">=</span>  <span class="sa">f</span><span class="s2">&quot;&quot;&quot;You are a helpful assistant that answers questions based on the provided CONTEXT.</span>

<span class="s2">    CONTEXT: </span><span class="si">{</span><span class="n">context</span><span class="si">}</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">rag_system_prompt_template</span><span class="p">},</span>
                 <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;QUESTION: </span><span class="si">{</span><span class="nb">input</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">}]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span>
</pre></div>
</div>
</div>
</div>
<p>First, we set the LLM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Load environment variables from .env file</span>
<span class="n">load_dotenv</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;gpt-4o-mini&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Then, we run the retrieve step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">query_collection</span><span class="p">(</span><span class="n">collection</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Next, we run the re-ranking step setting it to consider the <code class="docutils literal notranslate"><span class="pre">TOPK</span></code> retrieved documents.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">TOPK</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([(</span><span class="n">q</span><span class="p">,</span> <span class="n">doc</span><span class="p">)</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][:</span><span class="n">TOPK</span><span class="p">]])</span>
<span class="n">res_reranked</span> <span class="o">=</span> <span class="n">res</span><span class="p">[</span><span class="s2">&quot;documents&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>We then pass the top document as context and invoke the LLM with our RAG-based template leading to a successful response.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">answer</span> <span class="o">=</span> <span class="n">RAG_qa</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">res_reranked</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">question</span><span class="p">)</span>
<span class="n">answer</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The author of the book &quot;Taming LLMs&quot; is Tharsis Souza.
</pre></div>
</div>
</div>
</div>
<p>In this section, we motivated the use of RAGs as a tool to equip LLMs with relevant context and provided a canonical implementation of its core components. RAGs, however, can be implemented in many shapes and forms and entire books have been written about them. We point the user to additional resources if more specialized techniques and architectures are needed <span id="id25">[<a class="reference internal" href="#id160" title="Jay Alammar and Maarten Grootendorst. Hands-On Large Language Models. O'Reilly, 2024. ISBN 978-1098150969. URL: https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/.">Alammar and Grootendorst, 2024</a>, <a class="reference internal" href="#id161" title="Nir Diamant. Rag techniques. GitHub Repository, 2024. Collection of advanced RAG techniques and implementation patterns. URL: https://github.com/NirDiamant/RAG_Techniques.">Diamant, 2024</a>, <a class="reference internal" href="#id159" title="Abhinav Kimothi. A Simple Guide to Retrieval Augmented Generation. Manning Publications, 2024. ISBN 9781633435858. Manning Early Access Program (MEAP). URL: https://www.manning.com/books/a-simple-guide-to-retrieval-augmented-generation.">Kimothi, 2024</a>, <a class="reference internal" href="#id162" title="AthinaAI. Rag cookbooks. GitHub Repository, 2024. Collection of recipes and best practices for building RAG applications. URL: https://github.com/athina-ai/rag-cookbooks.">AthinaAI, 2024</a>]</span>.</p>
<p>Next, we discuss RAGs challenges and limitations and conclude our RAGs section envisioning the future of RAGs challenged by the rise of long-context language models.</p>
</section>
</section>
<section id="challenges-and-limitations">
<h3><a class="toc-backref" href="#id304" role="doc-backlink"><span class="section-number">5.3.2. </span>Challenges and Limitations</a><a class="headerlink" href="#challenges-and-limitations" title="Permalink to this heading">¶</a></h3>
<p>While RAG systems offer powerful capabilities for enhancing LLM responses with external knowledge, they face several significant challenges and limitations that require careful consideration:</p>
<ul class="simple">
<li><p><strong>Data Quality and Accuracy</strong>: The effectiveness of RAG systems fundamentally depends on the quality and reliability of their knowledge sources. When these sources contain inaccurate, outdated, biased, or incomplete information, the system’s responses become unreliable. This challenge is particularly acute when dealing with rapidly evolving topics or when sourcing information from unverified channels.</p></li>
<li><p><strong>Computational Cost and Latency</strong>: Implementing RAG systems at scale presents computational and operational challenges. The process of embedding documents, maintaining vector databases, and performing similarity searches across large knowledge bases demands computational, budget and operational resources. In real-time applications, these requirements can introduce noticeable latency, potentially degrading the user experience and limiting practical applications.</p></li>
<li><p><strong>Explainability and Evaluation</strong>: The complexity of RAG systems, arising from the intricate interaction between retrieval mechanisms and generative models, makes it difficult to trace and explain their reasoning processes. Traditional evaluation metrics often fail to capture the nuanced aspects of RAG performance, such as contextual relevance and factual consistency. This limitation hampers both system improvement and stakeholder trust. Readers are encouraged to read Chapter <a class="reference internal" href="evals.html#evals"><span class="std std-ref">The Evals Gap</span></a> for general LLM evaluation issues as well as consider tools such as Ragas <span id="id26">[<a class="reference internal" href="#id280" title="Ragas. Rag evaluation - ragas documentation. Website, 2024. URL: https://docs.ragas.io/en/stable/getstarted/rag_evaluation/.">Ragas, 2024</a>]</span> for RAG evaluation.</p></li>
<li><p><strong>Hallucination Management</strong>: Though RAG systems help ground LLM responses in source documents, they do not completely eliminate hallucinations. The generative component may still produce content that extrapolates beyond or misinterprets the retrieved context. This risk becomes particularly concerning when the system confidently presents incorrect information with apparent source attribution.</p></li>
</ul>
<p>Moreover, recent research has shed light on critical limitations of key techniques used in RAGs systems. A relevant finding pertains to reranking, which has shown <span id="id27">[<a class="reference internal" href="#id275" title="Mathew Jacob, Erik Lindgren, Matei Zaharia, Michael Carbin, Omar Khattab, and Andrew Drozdov. Drowning in documents: consequences of scaling reranker inference. 2024. URL: https://arxiv.org/abs/2411.11767, arXiv:2411.11767.">Jacob <em>et al.</em>, 2024</a>]</span>:</p>
<ul class="simple">
<li><p><strong>Diminishing Returns</strong>: Performance degrades as the number of documents (K) increases, sometimes performing worse than basic retrievers when dealing with large datasets.</p></li>
<li><p><strong>Poor Document Discrimination</strong>: Rerankers can be misled by irrelevant documents, sometimes assigning high scores to content with minimal relevance to the query.</p></li>
<li><p><strong>Consistency Issues</strong>: Performance and relative rankings between different rerankers can vary significantly depending on the number of documents being processed.</p></li>
</ul>
</section>
<section id="will-rags-exist-in-the-future">
<h3><a class="toc-backref" href="#id305" role="doc-backlink"><span class="section-number">5.3.3. </span>Will RAGs exist in the future?</a><a class="headerlink" href="#will-rags-exist-in-the-future" title="Permalink to this heading">¶</a></h3>
<p>This question is posed as we contrast RAGs with LLMs with long-context windows (LC).</p>
<p>Recent research has shed light on this specific point <span id="id28">[<a class="reference internal" href="#id279" title="Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach. 2024. URL: https://arxiv.org/abs/2407.16833, arXiv:2407.16833.">Li <em>et al.</em>, 2024</a>]</span>, suggesting that, on the one hand, RAGs can be seen as a cost-effective alternative to LC models:</p>
<ul class="simple">
<li><p>RAGs offer lower computational cost compared to LC due to the significantly shorter input length required for processing.</p></li>
<li><p>This cost-efficiency arises because RAG reduces the number of input tokens to LLMs, which of course reduces usage cost as pricing is based on the number of input (and output) tokens.</p></li>
</ul>
<p>On the other hand, this RAG benefit is achieved at the cost of performance:</p>
<ul class="simple">
<li><p>Recent advancements in LLMs, in particular with Gemini-1.5 and GPT-4o models, demonstrate capabilities in understanding long contexts directly, which enables them to outperform RAG in terms of average performance</p></li>
<li><p>LC models can process extremely long contexts, such as Gemini 1.5 which can handle up to 1 million tokens, and these models benefit from large-scale pretraining to develop strong long-context capabilities.</p></li>
</ul>
<p>This cost-performance trade-off is illustrated in <a class="reference internal" href="#lc"><span class="std std-numref">Fig. 5.10</span></a>, where LC models outperform RAGs in terms of average performance while RAGs are more cost-effective.</p>
<figure class="align-center" id="lc">
<a class="reference internal image-reference" href="../_images/LC.png"><img alt="Long-Context LLMs for Superior Performance" src="../_images/LC.png" style="width: 511.5px; height: 374.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.10 </span><span class="caption-text">Long-Context LLMs demonstrate superior performance while RAGs are more cost-effective <span id="id29">[<a class="reference internal" href="#id279" title="Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach. 2024. URL: https://arxiv.org/abs/2407.16833, arXiv:2407.16833.">Li <em>et al.</em>, 2024</a>]</span>.</span><a class="headerlink" href="#lc" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#lc"><span class="std std-numref">Fig. 5.10</span></a> also shows a model called “SELF-ROUTE” which combines RAG and LC by routing queries based on model self-reflection. This is a hybrid approach that reduces computational costs while maintaining performance comparable to LC. The advantage of SELF-ROUTE is most significant for smaller values of <em>k</em>, where <em>k</em> is the number of retrieved text chunks, and SELF-ROUTE shows a marked improvement in performance over RAG, while as k increases the performance of RAG and SELF-ROUTE approaches that of LC.</p>
<p>Another example of a hybrid approach that combines the benefits of both LC and RAGs is RetroLLM <span id="id30">[<a class="reference internal" href="#id276" title="Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou. Retrollm: empowering large language models to retrieve fine-grained evidence within generation. 2024. URL: https://arxiv.org/abs/2412.11919, arXiv:2412.11919.">Li <em>et al.</em>, 2024</a>]</span>, which is a unified framework that integrates retrieval and generation into a single process, enabling language models to generate fine-grained evidence directly from a corpus. The key contribution is that this approach delivers those benefits while eliminating the need for a separate retriever, addressing limitations of traditional RAG methods. Experimental results demonstrate RetroLLM’s superior performance compared to traditional RAG methods, across both in-domain and out-of-domain tasks. It also achieves a significant reduction in token consumption due to its fine-grained evidence retrieval.</p>
<p>A relevant development in this area is the introduction of LOFT <span id="id31">[<a class="reference internal" href="#id278" title="Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more? 2024. URL: https://arxiv.org/abs/2406.13121, arXiv:2406.13121.">Lee <em>et al.</em>, 2024</a>]</span>, a benchmark to assess this paradigm shift from RAGs to LCs, using real-world tasks requiring context up to millions of tokens. Evidence suggests LCs can deliver performance with simplified pipelines compared to RAGs, particularly for tasking requiring multi-hop reasoning over long contexts when using Chain-of-Thought <span id="id32">[<a class="reference internal" href="#id277" title="Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. 2023. URL: https://arxiv.org/abs/2201.11903, arXiv:2201.11903.">Wei <em>et al.</em>, 2023</a>]</span>. However, LCs can still be outperformed by specialized retrievers, in particular Gecko, a specialized model fine-tuned on extensive text retrieval and similarity tasks.</p>
<p>Bottom-line: Do we really need RAGs? The answer is conditional:</p>
<ul class="simple">
<li><p><strong>RAG may be relevant when cost-effectiveness is a key requirement</strong> and where the model needs to access vast amounts of external knowledge without incurring high computational expenses. However, as LLMs context window sizes increase and LLMs cost per input token is decreases, RAG may not be as relevant as it was before.</p></li>
<li><p><strong>Long-context LLMs are superior when performance is the primary concern</strong>, and the model needs to handle extensive texts that require deep contextual understanding and reasoning.</p></li>
<li><p><strong>Hybrid approaches like SELF-ROUTE are valuable as they combine the strengths of RAG and LC</strong> offering a practical balance between cost and performance, especially for applications where both factors are critical.</p></li>
</ul>
<p>Ultimately, the choice between RAG, LC, or a hybrid method depends on the specific requirements of the task, available resources, and the acceptable trade-off between cost and performance.</p>
<p>In a later case study, we demonstrate the power of LCs as we construct a Quiz generator with citations over a large knowledge base without the use of chunking nor RAGs.</p>
</section>
</section>
<section id="a-note-on-frameworks">
<h2><a class="toc-backref" href="#id306" role="doc-backlink"><span class="section-number">5.4. </span>A Note on Frameworks</a><a class="headerlink" href="#a-note-on-frameworks" title="Permalink to this heading">¶</a></h2>
<p>We have covered a few open source tools for parsing data and provided a canonical RAG pipeline directly using an open source VectorDB together with an LLM. There is a growing number of frameworks that offer similar functionality wrapping the same core concepts at a higher level of abstraction. The two most popular ones are <code class="docutils literal notranslate"><span class="pre">Langchain</span></code> and <code class="docutils literal notranslate"><span class="pre">LlamaIndex</span></code>.</p>
<p>For instance, the code below shows how to use <code class="docutils literal notranslate"><span class="pre">LlamaIndex</span></code>’s <code class="docutils literal notranslate"><span class="pre">LlamaParse</span></code> for parsing input documents, which offers support for a wide range of file formats (e.g. .pdf, .pptx, .docx, .xlsx, .html). We we can see that the code is very similar to the one we used for <code class="docutils literal notranslate"><span class="pre">MarkitDown</span></code> and <code class="docutils literal notranslate"><span class="pre">Docling</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_parse</span> <span class="kn">import</span> <span class="n">LlamaParse</span>

<span class="c1"># Initialize the parser</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">LlamaParse</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;llx-your-api-key-here&quot;</span><span class="p">,</span>
    <span class="n">result_type</span><span class="o">=</span><span class="s2">&quot;markdown&quot;</span><span class="p">,</span>  <span class="c1"># Can be &quot;markdown&quot; or &quot;text&quot;</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="n">documents</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">load_data</span><span class="p">([</span><span class="s2">&quot;./doc1.pdf&quot;</span><span class="p">,</span> <span class="s2">&quot;./doc2.pdf&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>As another example, the code below replicates our ChromaDB-based retrieval system using <code class="docutils literal notranslate"><span class="pre">LlamaIndex</span></code> <span id="id33">[<a class="reference internal" href="#id284" title="LlamaIndex. Storing - llamaindex documentation. Website, 2024. URL: https://docs.llamaindex.ai/en/stable/understanding/storing/storing/.">LlamaIndex, 2024</a>]</span>.</p>
<p>As we can see, similar concepts are used in both frameworks:</p>
<ul class="simple">
<li><p>Documents to represent elements of the knowledge base</p></li>
<li><p>Collections to store the documents</p></li>
<li><p>Indexing of embeddings in the VectorDB and finally</p></li>
<li><p>Querying the VectorDB to retrieve the documents</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">chromadb</span>
<span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">VectorStoreIndex</span><span class="p">,</span> <span class="n">SimpleDirectoryReader</span>
<span class="kn">from</span> <span class="nn">llama_index.vector_stores.chroma</span> <span class="kn">import</span> <span class="n">ChromaVectorStore</span>
<span class="kn">from</span> <span class="nn">llama_index.core</span> <span class="kn">import</span> <span class="n">StorageContext</span>

<span class="c1"># load some documents</span>
<span class="n">documents</span> <span class="o">=</span> <span class="n">SimpleDirectoryReader</span><span class="p">(</span><span class="s2">&quot;./data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>

<span class="c1"># initialize client, setting path to save data</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">chromadb</span><span class="o">.</span><span class="n">PersistentClient</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s2">&quot;./chroma_db&quot;</span><span class="p">)</span>

<span class="c1"># create collection</span>
<span class="n">chroma_collection</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">get_or_create_collection</span><span class="p">(</span><span class="s2">&quot;tamingllms&quot;</span><span class="p">)</span>

<span class="c1"># assign chroma as the vector_store to the context</span>
<span class="n">vector_store</span> <span class="o">=</span> <span class="n">ChromaVectorStore</span><span class="p">(</span><span class="n">chroma_collection</span><span class="o">=</span><span class="n">chroma_collection</span><span class="p">)</span>
<span class="n">storage_context</span> <span class="o">=</span> <span class="n">StorageContext</span><span class="o">.</span><span class="n">from_defaults</span><span class="p">(</span><span class="n">vector_store</span><span class="o">=</span><span class="n">vector_store</span><span class="p">)</span>

<span class="c1"># create your index</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">VectorStoreIndex</span><span class="o">.</span><span class="n">from_documents</span><span class="p">(</span>
    <span class="n">documents</span><span class="p">,</span> <span class="n">storage_context</span><span class="o">=</span><span class="n">storage_context</span>
<span class="p">)</span>

<span class="c1"># create a query engine and query</span>
<span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="o">.</span><span class="n">as_query_engine</span><span class="p">()</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">query_engine</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;Who is the author of Taming LLMs?&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>Frameworks are useful for quickly prototyping RAG systems and for building applications on top of them as they provide a higher level of abstraction and integration with third-party libraries. However, the underlying concepts are the same as the ones we have covered in this chapter. More often than not, problems arise when developers either do not understand the underlying concepts or fail to understand the details of the implement behind the abstractions provided by the framework. Therefore, it is recommended to try and start your implementation using lower level tools as much as possible and only when (i) the underlying problem as well as (ii) the desired solution are well understood, then consider moving to higher level frameworks if really needed.</p>
</section>
<section id="case-studies">
<h2><a class="toc-backref" href="#id307" role="doc-backlink"><span class="section-number">5.5. </span>Case Studies</a><a class="headerlink" href="#case-studies" title="Permalink to this heading">¶</a></h2>
<p>This section presents two case studies to complement topics we have covered in this chapter in the context of managing input data for LLMs.</p>
<p>First, we cover content chunking, in particular Content Chunking with Contextual Linking which showcases how intelligent chunking strategies can overcome both context window and output token limitations. This case study illustrates techniques for breaking down and reassembling content while maintaining coherence, enabling the generation of high-quality long-form outputs despite model constraints.</p>
<p>Second, we build a Quiz generator with citations using long context window. Not all knowledge intense applications require RAGs. In this case study, we show how to use long context window as well as some additional input management techniques such as prompt caching for efficiency and reference management to enhance response accuracy and verifiability. These approaches show how to maximize the benefits of larger context models while maintaining response quality.</p>
<section id="case-study-i-content-chunking-with-contextual-linking">
<h3><a class="toc-backref" href="#id308" role="doc-backlink"><span class="section-number">5.5.1. </span>Case Study I: Content Chunking with Contextual Linking</a><a class="headerlink" href="#case-study-i-content-chunking-with-contextual-linking" title="Permalink to this heading">¶</a></h3>
<p>Content chunking is commonly used to breakdown long-form content into smaller, manageable chunks. In the context of RAGs, this can be helpful not only to help the retrieval system find more contextually relevant documents but also lead to a more cost efficient LLM solution since fewer tokens are processed in the context window. Furthermore, semantic chunking can increase accuracy of RAG systems <span id="id34">[<a class="reference internal" href="#id285" title="ZenML. Scaling rag accuracy from 49% to 86% in finance q&amp;a assistant. Website, 2024. URL: https://www.zenml.io/llmops-database/scaling-rag-accuracy-from-49-to-86-in-finance-q-a-assistant.">ZenML, 2024</a>]</span>.</p>
<p>Content chunking with contextual linking is a chunking technique that seeks to split input content while keeping chunk-specific context, hence allowing the LLM to maintain coherence and context when generating responses per chunks. In that way, this technique tackles two key problems:</p>
<ol class="arabic simple">
<li><p>The LLM’s inability to process long inputs to do context-size limits</p></li>
<li><p>The LLM’s inability to maintain coherence and context when generating responses per chunks</p></li>
</ol>
<p>As a consequence, a third problem is also tackled: LLM’s inability to generate long-form content due to the <code class="docutils literal notranslate"><span class="pre">max_output_tokens</span></code> limitation. Since we generate responses per chunk, as we will see later, we end up with a solution that is capable of generating long-form content while maintaining coherence.</p>
<p>We exemplify this technique by following these steps:</p>
<ol class="arabic simple">
<li><p><strong>Chunking the Content</strong>: The input content is split into smaller chunks. This allows the LLM to process each chunk individually, focusing on generating a complete and detailed response for that specific section of the input.</p></li>
<li><p><strong>Maintaining Context</strong>: Each chunk is linked with contextual information from the previous chunks. This helps in maintaining the flow and coherence of the content across multiple chunks.</p></li>
<li><p><strong>Generating Linked Prompts</strong>: For each chunk, a prompt is generated that includes the chunk’s content and its context. This prompt is then used to generate the output for that chunk.</p></li>
<li><p><strong>Combining the Outputs</strong>: The outputs of all chunks are combined to form the final long-form content.</p></li>
</ol>
<p>Let’s examine an example implementation of this technique.</p>
<section id="generating-long-form-content">
<h4><a class="toc-backref" href="#id309" role="doc-backlink"><span class="section-number">5.5.1.1. </span>Generating long-form content</a><a class="headerlink" href="#generating-long-form-content" title="Permalink to this heading">¶</a></h4>
<ul class="simple">
<li><p>Goal: Generate a long-form report analyzing a company’s financial statement.</p></li>
<li><p>Input: A company’s 10K SEC filing.</p></li>
</ul>
<figure class="align-center" id="content-chunking-with-contextual-linking">
<a class="reference internal image-reference" href="../_images/diagram1.png"><img alt="Content Chunking with Contextual Linking" src="../_images/diagram1.png" style="width: 819.0px; height: 1725.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.11 </span><span class="caption-text">Content Chunking with Contextual Linking Schematic Representation.</span><a class="headerlink" href="#content-chunking-with-contextual-linking" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The diagram in <a class="reference internal" href="#content-chunking-with-contextual-linking"><span class="std std-numref">Fig. 5.11</span></a> illustrates the process we will follow for handling long-form content generation with Large Language Models through “Content Chunking with Contextual Linking.” It shows how input content is first split into manageable chunks using a chunking function (e.g. <code class="docutils literal notranslate"><span class="pre">CharacterTextSplitter</span></code> with <code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> tokenizer), then each chunk is processed sequentially while maintaining context from previous chunks. For each chunk, the system updates the context, generates a dynamic prompt with specific parameters, makes a call to the LLM chain, and stores the response. After all chunks are processed, the individual responses are combined with newlines to create the final report, effectively working around the token limit constraints of LLMs while maintaining coherence across the generated content.</p>
<p><strong>Step 1: Chunking the Content</strong></p>
<p>There are different methods for chunking, and each of them might be appropriate for different situations. However, we can broadly group chunking strategies in two types:</p>
<ul>
<li><p><strong>Fixed-size Chunking</strong>: This is the most common and straightforward approach to chunking. We simply decide the number of tokens in our chunk and, optionally, whether there should be any overlap between them. In general, we will want to keep some overlap between chunks to make sure that the semantic context doesn’t get lost between chunks. Fixed-sized chunking may be a reasonable path in many common cases. Compared to other forms of chunking, fixed-sized chunking is computationally cheap and simple to use since it doesn’t require the use of any specialied techniques or libraries.</p></li>
<li><p><strong>Content-aware Chunking</strong>: These are a set of methods for taking advantage of the nature of the content we’re chunking and applying more sophisticated chunking to it. Examples include:</p>
<ul class="simple">
<li><p><strong>Sentence Splitting</strong>: Many models are optimized for embedding sentence-level content. Naturally, we would use sentence chunking, and there are several approaches and tools available to do this, including naive splitting (e.g. splitting on periods), NLTK, and spaCy.</p></li>
<li><p><strong>Recursive Chunking</strong>: Recursive chunking divides the input text into smaller chunks in a hierarchical and iterative manner using a set of separators.</p></li>
<li><p><strong>Semantic Chunking</strong>: This is a class of methods that leverages embeddings to extract the semantic meaning present in your data, creating chunks that are made up of sentences that talk about the same theme or topic.</p></li>
</ul>
<p>Here, we will utilize <code class="docutils literal notranslate"><span class="pre">langchain</span></code> for a content-aware sentence-splitting strategy for chunking. Langchain offers several text splitters <span id="id35">[<a class="reference internal" href="#id92" title="LangChain. Text splitters - langchain documentation. https://python.langchain.com/docs/how_to/#text-splitters, 2024. Accessed: 12/07/2024.">LangChain, 2024</a>]</span> such as JSON-, Markdown- and HTML-based or split by token. We will use the <code class="docutils literal notranslate"><span class="pre">CharacterTextSplitter</span></code> with <code class="docutils literal notranslate"><span class="pre">tiktoken</span></code> as our tokenizer to count the number of tokens per chunk which we can use to ensure that we do not surpass the input token limit of our model.</p>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_chunks</span><span class="p">(</span><span class="n">text</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Split input text into chunks of specified size with specified overlap.</span>

<span class="sd">    Args:</span>
<span class="sd">        text (str): The input text to be chunked.</span>
<span class="sd">        chunk_size (int): The maximum size of each chunk in tokens.</span>
<span class="sd">        chunk_overlap (int): The number of tokens to overlap between chunks.</span>

<span class="sd">    Returns:</span>
<span class="sd">        list: A list of text chunks.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">langchain_text_splitters</span> <span class="kn">import</span> <span class="n">CharacterTextSplitter</span>

    <span class="n">text_splitter</span> <span class="o">=</span> <span class="n">CharacterTextSplitter</span><span class="o">.</span><span class="n">from_tiktoken_encoder</span><span class="p">(</span><span class="n">chunk_size</span><span class="o">=</span><span class="n">chunk_size</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">chunk_overlap</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text_splitter</span><span class="o">.</span><span class="n">split_text</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Step 2: Writing the Base Prompt Template</strong></p>
<p>We will write a base prompt template which will serve as a foundational structure for all chunks, ensuring consistency in the instructions and context provided to the language model. The template includes the following parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">role</span></code>: Defines the role or persona the model should assume.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">context</span></code>: Provides the background information or context for the task.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">instruction</span></code>: Specifies the task or action the model needs to perform.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_text</span></code>: Contains the actual text input that the model will process.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">requirements</span></code>: Lists any specific requirements or constraints for the output.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.prompts</span> <span class="kn">import</span> <span class="n">PromptTemplate</span>
<span class="k">def</span> <span class="nf">get_base_prompt_template</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    
    <span class="n">base_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    ROLE: </span><span class="si">{role}</span>
<span class="s2">    CONTEXT: </span><span class="si">{context}</span>
<span class="s2">    INSTRUCTION: </span><span class="si">{instruction}</span>
<span class="s2">    INPUT: </span><span class="si">{input}</span>
<span class="s2">    REQUIREMENTS: </span><span class="si">{requirements}</span>
<span class="s2">    &quot;&quot;&quot;</span>
    
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">PromptTemplate</span><span class="o">.</span><span class="n">from_template</span><span class="p">(</span><span class="n">base_prompt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">prompt</span>
</pre></div>
</div>
</div>
</div>
<p>We will write a simple function that returns an <code class="docutils literal notranslate"><span class="pre">LLMChain</span></code> which is a simple <code class="docutils literal notranslate"><span class="pre">langchain</span></code> construct that allows you to chain together a combination of prompt templates, language models and output parsers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_core.output_parsers</span> <span class="kn">import</span> <span class="n">StrOutputParser</span>
<span class="kn">from</span> <span class="nn">langchain_community.chat_models</span> <span class="kn">import</span> <span class="n">ChatLiteLLM</span>

<span class="k">def</span> <span class="nf">get_llm_chain</span><span class="p">(</span><span class="n">prompt_template</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">temperature</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns an LLMChain instance using langchain.</span>

<span class="sd">    Args:</span>
<span class="sd">        prompt_template (str): The prompt template to use.</span>
<span class="sd">        model_name (str): The name of the model to use.</span>
<span class="sd">        temperature (float): The temperature setting for the model.</span>

<span class="sd">    Returns:</span>
<span class="sd">        llm_chain: An instance of the LLMChain.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span>
    <span class="kn">import</span> <span class="nn">os</span>

    <span class="c1"># Load environment variables from .env file</span>
    <span class="n">load_dotenv</span><span class="p">()</span>
    
    <span class="n">api_key_label</span> <span class="o">=</span> <span class="n">model_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="o">+</span> <span class="s2">&quot;_API_KEY&quot;</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="n">ChatLiteLLM</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">,</span>
        <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="n">api_key_label</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">prompt_template</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">StrOutputParser</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">llm_chain</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Step 3: Constructing Dynamic Prompt Parameters</strong></p>
<p>Now, we will write a function (<code class="docutils literal notranslate"><span class="pre">get_dynamic_prompt_template</span></code>) that constructs prompt parameters dynamically for each chunk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span>
<span class="k">def</span> <span class="nf">get_dynamic_prompt_params</span><span class="p">(</span><span class="n">prompt_params</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> 
                            <span class="n">part_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                            <span class="n">total_parts</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                            <span class="n">chat_context</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                            <span class="n">chunk</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Construct prompt template dynamically per chunk while maintaining the chat context of the response generation.</span>
<span class="sd">    </span>
<span class="sd">    Args:</span>
<span class="sd">        prompt_params (Dict): Original prompt parameters</span>
<span class="sd">        part_idx (int): Index of current conversation part</span>
<span class="sd">        total_parts (int): Total number of conversation parts</span>
<span class="sd">        chat_context (str): Chat context from previous parts</span>
<span class="sd">        chunk (str): Current chunk of text to be processed</span>
<span class="sd">    Returns:</span>
<span class="sd">        str: Dynamically constructed prompt template with part-specific params</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dynamic_prompt_params</span> <span class="o">=</span> <span class="n">prompt_params</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="c1"># saves the chat context from previous parts</span>
    <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;context&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chat_context</span>
    <span class="c1"># saves the current chunk of text to be processed as input</span>
    <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;input&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">chunk</span>
    
    <span class="c1"># Add part-specific instructions</span>
    <span class="k">if</span> <span class="n">part_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># Introduction part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating the Introduction part of a long report.</span>
<span class="s2">        Don&#39;t cover any topics yet, just define the scope of the report.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    <span class="k">elif</span> <span class="n">part_idx</span> <span class="o">==</span> <span class="n">total_parts</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># Conclusion part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating the last part of a long report. </span>
<span class="s2">        For this part, first discuss the below INPUT. Second, write a &quot;Conclusion&quot; section summarizing the main points discussed given in CONTEXT.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># Main analysis part</span>
        <span class="n">dynamic_prompt_params</span><span class="p">[</span><span class="s2">&quot;instruction&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        You are generating part </span><span class="si">{</span><span class="n">part_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2"> of </span><span class="si">{</span><span class="n">total_parts</span><span class="si">}</span><span class="s2"> parts of a long report.</span>
<span class="s2">        For this part, analyze the below INPUT.</span>
<span class="s2">        Organize your response in a way that is easy to read and understand either by creating new or merging with previously created structured sections given in CONTEXT.</span>
<span class="s2">        &quot;&quot;&quot;</span>
    
    <span class="k">return</span> <span class="n">dynamic_prompt_params</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Step 4: Generating the Report</strong></p>
<p>Finally, we will write a function that generates the actual report by calling the <code class="docutils literal notranslate"><span class="pre">LLMChain</span></code> with the dynamically updated prompt parameters for each chunk and concatenating the results at the end.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_report</span><span class="p">(</span><span class="n">input_content</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">llm_model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                    <span class="n">role</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">requirements</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                    <span class="n">chunk_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="c1"># stores the parts of the report, each generated by an individual LLM call</span>
    <span class="n">report_parts</span> <span class="o">=</span> <span class="p">[]</span> 
    <span class="c1"># split the input content into chunks</span>
    <span class="n">chunks</span> <span class="o">=</span> <span class="n">get_chunks</span><span class="p">(</span><span class="n">input_content</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="p">)</span>
    <span class="c1"># initialize the chat context with the input content</span>
    <span class="n">chat_context</span> <span class="o">=</span> <span class="n">input_content</span>
    <span class="c1"># number of parts to be generated</span>
    <span class="n">num_parts</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">)</span>

    <span class="n">prompt_params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="n">role</span><span class="p">,</span> <span class="c1"># user-provided</span>
        <span class="s2">&quot;context&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dinamically updated per part</span>
        <span class="s2">&quot;instruction&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dynamically updated per part</span>
        <span class="s2">&quot;input&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="c1"># dynamically updated per part</span>
        <span class="s2">&quot;requirements&quot;</span><span class="p">:</span> <span class="n">requirements</span> <span class="c1">#user-priovided</span>
    <span class="p">}</span>

    <span class="c1"># get the LLMChain with the base prompt template</span>
    <span class="n">llm_chain</span> <span class="o">=</span> <span class="n">get_llm_chain</span><span class="p">(</span><span class="n">get_base_prompt_template</span><span class="p">(),</span> 
                                 <span class="n">llm_model_name</span><span class="p">)</span>

    <span class="c1"># dynamically update prompt_params per part</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generating </span><span class="si">{</span><span class="n">num_parts</span><span class="si">}</span><span class="s2"> report parts&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chunks</span><span class="p">):</span>
        <span class="n">dynamic_prompt_params</span> <span class="o">=</span> <span class="n">get_dynamic_prompt_params</span><span class="p">(</span>
            <span class="n">prompt_params</span><span class="p">,</span>
            <span class="n">part_idx</span><span class="o">=</span><span class="n">i</span><span class="p">,</span>
            <span class="n">total_parts</span><span class="o">=</span><span class="n">num_parts</span><span class="p">,</span>
            <span class="n">chat_context</span><span class="o">=</span><span class="n">chat_context</span><span class="p">,</span>
            <span class="n">chunk</span><span class="o">=</span><span class="n">chunk</span>
        <span class="p">)</span>
        
        <span class="c1"># invoke the LLMChain with the dynamically updated prompt parameters</span>
        <span class="n">response</span> <span class="o">=</span> <span class="n">llm_chain</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">dynamic_prompt_params</span><span class="p">)</span>

        <span class="c1"># update the chat context with the cummulative response</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">chat_context</span> <span class="o">=</span> <span class="n">response</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">chat_context</span> <span class="o">=</span> <span class="n">chat_context</span> <span class="o">+</span> <span class="n">response</span>
            
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Generated part </span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">num_parts</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
        <span class="n">report_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>

    <span class="n">report</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">report_parts</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">report</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Example Usage</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the text from sample 10K SEC filing</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/apple.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the chunk and chunk overlap size</span>
<span class="n">MAX_CHUNK_SIZE</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">MAX_CHUNK_OVERLAP</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">report</span> <span class="o">=</span> <span class="n">generate_report</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">llm_model_name</span><span class="o">=</span><span class="s2">&quot;gemini/gemini-1.5-flash-latest&quot;</span><span class="p">,</span> 
                           <span class="n">role</span><span class="o">=</span><span class="s2">&quot;Financial Analyst&quot;</span><span class="p">,</span> 
                           <span class="n">requirements</span><span class="o">=</span><span class="s2">&quot;The report should be in a readable, structured format, easy to understand and follow. Focus on finding risk factors and market moving insights.&quot;</span><span class="p">,</span>
                           <span class="n">chunk_size</span><span class="o">=</span><span class="n">MAX_CHUNK_SIZE</span><span class="p">,</span> 
                           <span class="n">chunk_overlap</span><span class="o">=</span><span class="n">MAX_CHUNK_OVERLAP</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the generated report to a local file</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;data/apple_report.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">report</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Read and display the generated report</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/apple_report.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">report_content</span> <span class="o">=</span> <span class="n">file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span>

<span class="c1"># Display first and last 10% of the report content</span>
<span class="n">report_lines</span> <span class="o">=</span> <span class="n">report_content</span><span class="o">.</span><span class="n">splitlines</span><span class="p">()</span>
<span class="n">total_lines</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">report_lines</span><span class="p">)</span>
<span class="n">quarter_lines</span> <span class="o">=</span> <span class="n">total_lines</span> <span class="o">//</span> <span class="mi">10</span>

<span class="n">top_portion</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">report_lines</span><span class="p">[:</span><span class="n">quarter_lines</span><span class="p">])</span>
<span class="n">bottom_portion</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">report_lines</span><span class="p">[</span><span class="o">-</span><span class="n">quarter_lines</span><span class="p">:])</span>

<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">top_portion</span><span class="si">}</span><span class="se">\n\n</span><span class="s2"> (...) </span><span class="se">\n\n</span><span class="s2"> </span><span class="si">{</span><span class="n">bottom_portion</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<p><strong>Introduction</strong></p>
<p>This report provides a comprehensive analysis of Apple Inc.’s financial performance and position for the fiscal year ended September 28, 2024, as disclosed in its Form 10-K filing with the United States Securities and Exchange Commission.  The analysis will focus on identifying key risk factors impacting Apple’s business, evaluating its financial health, and uncovering market-moving insights derived from the provided data.  The report will delve into Apple’s various segments, product lines, and services, examining their performance and contributions to overall financial results.  Specific attention will be paid to identifying trends, potential challenges, and opportunities for future growth.  The analysis will also consider the broader macroeconomic environment and its influence on Apple’s operations and financial outlook.  Finally, the report will incorporate relevant information from Apple’s definitive proxy statement for its 2025 annual meeting of shareholders, as incorporated by reference in the Form 10-K.</p>
<p><strong>PART 2: Key Risk Factors and Market-Moving Insights</strong></p>
<p>This section analyzes key risk factors disclosed in Apple Inc.’s 2024 Form 10-K, focusing on their potential impact on financial performance and identifying potential market-moving insights.  The analysis is structured around the major risk categories identified in the filing.</p>
<p><strong>2.1 Dependence on Third-Party Developers:</strong></p>
<p>Apple’s success is heavily reliant on the continued support and innovation of third-party software developers.  The Form 10-K highlights several critical aspects of this dependence:</p>
<ul class="simple">
<li><p><strong>Market Share Vulnerability:</strong> Apple’s relatively smaller market share in smartphones, personal computers, and tablets compared to competitors (Android, Windows, gaming consoles) could discourage developers from prioritizing Apple’s platform, leading to fewer high-quality apps and potentially impacting customer purchasing decisions.  This is a significant risk, especially given the rapid pace of technological change.  A decline in app availability or quality could negatively impact sales and market share.  <strong>Market-moving insight:</strong>  Monitoring developer activity and app quality across competing platforms is crucial for assessing this risk.  Any significant shift in developer focus away from iOS could be a negative market signal.</p></li>
<li><p><strong>App Store Dynamics:</strong> While Apple allows developers to retain most App Store revenue, its commission structure and recent changes (e.g., complying with the Digital Markets Act (DMA) in the EU) introduce uncertainty.  Changes to the App Store’s policies or fee structures could materially affect Apple’s revenue and profitability.  <strong>Market-moving insight:</strong>  Closely monitoring regulatory developments (especially concerning the DMA) and their impact on App Store revenue is essential.  Any significant changes to Apple’s App Store policies or revenue streams could trigger market reactions.</p></li>
<li><p><strong>Content Acquisition and Creation:</strong> Apple’s reliance on third-party digital content providers for its services introduces risks related to licensing agreements, competition, and pricing.  The cost of producing its own digital content is also increasing due to competition for talent and subscribers.  Failure to secure or create appealing content could negatively impact user engagement and revenue.  <strong>Market-moving insight:</strong>  Analyzing the success of Apple’s original content initiatives and the renewal rates of third-party content agreements will provide insights into this risk.</p></li>
</ul>
<p><strong>2.2 Operational Risks:</strong></p>
<p>(…)</p>
<p>The reconciliation of segment operating income to consolidated operating income reveals that research and development (R&amp;D) and other corporate expenses significantly impact overall profitability.  While increased R&amp;D is generally positive, it reduces short-term profits.  The geographical breakdown of net sales and long-lived assets further emphasizes the concentration of Apple’s business in the U.S. and China.  <strong>Market-moving insight:</strong>  Continued weakness in the Greater China market, sustained flat iPhone sales, or any significant changes in R&amp;D spending should be closely monitored for their potential impact on Apple’s financial performance and investor sentiment.</p>
<p><strong>5.4 Auditor’s Report and Internal Controls:</strong></p>
<p>The auditor’s report expresses an unqualified opinion on Apple’s financial statements and internal control over financial reporting.  However, it identifies uncertain tax positions as a critical audit matter.  The significant amount of unrecognized tax benefits ($22.0 billion) and the complexity involved in evaluating these positions highlight a substantial risk.  Management’s assessment of these positions involves significant judgment and relies on interpretations of complex tax laws.  Apple’s management also asserts that its disclosure controls and procedures are effective.  <strong>Market-moving insight:</strong>  Any changes in tax laws, unfavorable rulings on uncertain tax positions, or weaknesses in internal controls could materially affect Apple’s financial results and investor confidence.</p>
<p><strong>Conclusion</strong></p>
<p>This report provides a comprehensive analysis of Apple Inc.’s financial performance and position for fiscal year 2024.  While Apple maintains a strong financial position with substantial cash reserves and a robust capital return program, several key risk factors could significantly impact its future performance.  These risks include:</p>
<ul class="simple">
<li><p><strong>Dependence on third-party developers:</strong>  A shift in developer focus away from iOS or changes to the App Store’s policies could negatively impact Apple’s revenue and profitability.</p></li>
<li><p><strong>Operational risks:</strong>  Employee retention challenges, reseller dependence, and cybersecurity threats pose significant operational risks.</p></li>
<li><p><strong>Legal and regulatory risks:</strong>  Ongoing antitrust litigation, the Digital Markets Act (DMA) compliance, and data privacy regulations introduce substantial legal and regulatory uncertainties.</p></li>
<li><p><strong>Financial risks:</strong>  Volatility in sales and profit margins, foreign exchange rate fluctuations, credit risk, and tax risks could impact Apple’s financial performance.</p></li>
<li><p><strong>Supply chain concentration:</strong>  Apple’s reliance on a concentrated network of outsourcing partners, primarily located in a few Asian countries, and dependence on single or limited sources for certain custom components, exposes the company to significant supply chain risks.</p></li>
<li><p><strong>Uncertain tax positions:</strong>  The significant amount of unrecognized tax benefits represents a substantial uncertainty that could materially affect Apple’s financial results.</p></li>
</ul>
<p>Despite these risks, Apple’s strong liquidity position, continued growth in its Services segment, and robust capital return program provide a degree of resilience.  However, investors and analysts should closely monitor the market-moving insights identified throughout this report, including developer activity, regulatory developments, regional economic conditions, supply chain stability, and the resolution of uncertain tax positions, to assess their potential impact on Apple’s future performance and valuation.  The significant short-term obligations, while manageable given Apple’s cash position, highlight the need for continued financial discipline and effective risk management.  A deeper, more granular analysis of the financial statements and notes is recommended for a more complete assessment.</p>
</div>
</div>
</section>
<hr class="docutils" />
<section id="discussion">
<h4><a class="toc-backref" href="#id310" role="doc-backlink"><span class="section-number">5.5.1.2. </span>Discussion</a><a class="headerlink" href="#discussion" title="Permalink to this heading">¶</a></h4>
<p>Results from the generated report present a few interesting aspects:</p>
<ul class="simple">
<li><p><strong>Coherence</strong>: The generated report demonstrates an apparent level of coherence. The sections are logically structured, and the flow of information is smooth. Each part of the report builds upon the previous sections, providing a comprehensive analysis of Apple Inc.’s financial performance and key risk factors. The use of headings and subheadings helps in maintaining clarity and organization throughout the document.</p></li>
<li><p><strong>Adherence to Instructions</strong>: The LLM followed the provided instructions effectively. The report is in a readable, structured format, and it focuses on identifying risk factors and market-moving insights as requested. The analysis is detailed and covers various aspects of Apple’s financial performance, including revenue segmentation, profitability, liquidity, and capital resources. The inclusion of market-moving insights adds value to the report, aligning with the specified requirements.</p></li>
</ul>
<p>Despite the seemingly good quality of the results, there are some limitations to consider:</p>
<ul class="simple">
<li><p><strong>Depth of Analysis</strong>: While the report covers a wide range of topics, the depth of analysis in certain sections may not be as comprehensive as a human expert’s evaluation. Some nuances and contextual factors might be overlooked by the LLM. Splitting the report into multiple parts helps in mitigating this issue.</p></li>
<li><p><strong>Chunking Strategy</strong>: The current approach splits the text into chunks based on size, which ensures that each chunk fits within the model’s token limit. However, this method may disrupt the logical flow of the document, as sections of interest might be split across multiple chunks. An alternative approach could be “structured” chunking, where the text is divided based on meaningful sections or topics. This would preserve the coherence of each section, making it easier to follow and understand. Implementing structured chunking requires additional preprocessing to identify and segment the text appropriately, but it can significantly enhance the readability and logical flow of the generated report.</p></li>
</ul>
<p>Here, we implemented a simple strategy to improve the coherence in output generation given a multi-part chunked input. Many other strategies are possible. One related technique worth mentioning is Anthropic’s Contextual Retrieval <span id="id36">[<a class="reference internal" href="#id195" title="Anthropic. Introducing contextual retrieval. 09 2024a. URL: https://www.anthropic.com/news/contextual-retrieval.">Anthropic, 2024a</a>]</span>. The approach, as shown in <a class="reference internal" href="#anth-contextual"><span class="std std-numref">Fig. 5.12</span></a>, employs an LLM itself to generate relevant context per chunk before passing these two pieces of information together to the LLM. This process was proposed in the context of RAGs to enhance its retrieval capabilities but can be applied more generally to improve output generation.</p>
<figure class="align-center" id="anth-contextual">
<a class="reference internal image-reference" href="../_images/anth_contextual.png"><img alt="Anthropic Contextual Linking" src="../_images/anth_contextual.png" style="width: 545.5px; height: 359.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.12 </span><span class="caption-text">Anthropic Contextual Linking <span id="id37">[<a class="reference internal" href="#id195" title="Anthropic. Introducing contextual retrieval. 09 2024a. URL: https://www.anthropic.com/news/contextual-retrieval.">Anthropic, 2024a</a>]</span>.</span><a class="headerlink" href="#anth-contextual" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="case-study-ii-quiz-generation-with-citations">
<h3><a class="toc-backref" href="#id311" role="doc-backlink"><span class="section-number">5.5.2. </span>Case Study II: Quiz Generation with Citations</a><a class="headerlink" href="#case-study-ii-quiz-generation-with-citations" title="Permalink to this heading">¶</a></h3>
<p>In this case study, we will build a Quiz generator with citations that explores additional input management techniques particularly useful with long context windows. The implementation includes prompt caching for efficiency and citation tracking to enhance accuracy and verifiability. We will use Gemini 1.5 Pro as our LLM model, which has a context window of 2M tokens.</p>
<section id="use-case">
<h4><a class="toc-backref" href="#id312" role="doc-backlink"><span class="section-number">5.5.2.1. </span>Use Case</a><a class="headerlink" href="#use-case" title="Permalink to this heading">¶</a></h4>
<p>Let’s assume you are a Harvard student enrolled in GOV 1039 “The Birth of Modern Democracy” (see <a class="reference internal" href="#harvard-class"><span class="std std-numref">Fig. 5.13</span></a>), you face a daunting reading list for next Tuesday’s class on Rights. The readings include foundational documents like the Magna Carta, Declaration of Independence, and US Bill of Rights, each with specific sections to analyze.</p>
<figure class="align-center" id="harvard-class">
<a class="reference internal image-reference" href="../_images/harvard.png"><img alt="Harvard Class" src="../_images/harvard.png" style="width: 691.0px; height: 435.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.13 </span><span class="caption-text">Harvard’s Democratic Theory Class</span><a class="headerlink" href="#harvard-class" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Instead of trudging through these dense historical texts sequentially, we would like to:</p>
<ul class="simple">
<li><p>Extract key insights and connections between these documents, conversationally.</p></li>
<li><p>Engage with the material through a quiz format.</p></li>
<li><p>Add citations to help with verifying answers.</p></li>
</ul>
</section>
<section id="implementation">
<h4><a class="toc-backref" href="#id313" role="doc-backlink"><span class="section-number">5.5.2.2. </span>Implementation</a><a class="headerlink" href="#implementation" title="Permalink to this heading">¶</a></h4>
<p>The full implementation is available at Book’s <a class="reference external" href="https://github.com/souzatharsis/tamingLLMs/tamingllms/notebooks/src/gemini_duo.py">Github repository</a>. Here, we will cover the most relevant parts of the implementation.</p>
<p><strong>Client Class</strong></p>
<p>First, we will define the <code class="docutils literal notranslate"><span class="pre">Client</span></code> class which will provide the key interface users will interact with. It has the following summarized interface:</p>
<ul class="simple">
<li><p>Initialization:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">__init__(knowledge_base:</span> <span class="pre">List[str]</span> <span class="pre">=</span> <span class="pre">[])</span></code>: Initialize with optional list of URLs as knowledge base</p></li>
</ul>
</li>
<li><p>Core Methods:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">add_knowledge_base(urls:</span> <span class="pre">List[str])</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code>: Add URLs to the knowledge base</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add(urls:</span> <span class="pre">List[str])</span> <span class="pre">-&gt;</span> <span class="pre">None</span></code>: Extract content from URLs and add to conversation input</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">msg(msg:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">&quot;&quot;,</span> <span class="pre">add_citations:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">False)</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code>: Enables users to send messages to the client</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">quiz(add_citations:</span> <span class="pre">bool</span> <span class="pre">=</span> <span class="pre">True,</span> <span class="pre">num_questions:</span> <span class="pre">int</span> <span class="pre">=</span> <span class="pre">10)</span> <span class="pre">-&gt;</span> <span class="pre">str</span></code>: Generate a quiz based on full input memory</p></li>
</ul>
</li>
<li><p>Key Attributes:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">knowledge_base</span></code>: List of URLs providing foundation knowledge</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input</span></code>: Current input being studied (short-term memory)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_memory</span></code>: Cumulative input + knowledge base (long-term memory)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response</span></code>: Latest response from LLM</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response_memory</span></code>: Cumulative responses (long-term memory)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">urls_memory</span></code>: Cumulative list of processed URLs</p></li>
</ul>
</li>
</ul>
<p><strong>Corpus-in-Context Prompting</strong></p>
<p>The <code class="docutils literal notranslate"><span class="pre">add()</span></code> method is key since it is used to add content to the client. It takes a list of URLs and extracts the content from each URL using a content extractor (using MarkitDown). The content is then added to the conversation input memory in a way that enables citations using the “Corpus-in-Context” (CIC) Prompting <span id="id38">[<a class="reference internal" href="#id278" title="Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more? 2024. URL: https://arxiv.org/abs/2406.13121, arXiv:2406.13121.">Lee <em>et al.</em>, 2024</a>]</span>.</p>
<p><a class="reference internal" href="#cic"><span class="std std-numref">Fig. 5.14</span></a> shows how CIC format is used to enable citations. It inserts a corpus into the prompt. Each candidate citable part (e.g., passage, chapter) in a corpus is assigned a unique identifier (ID) that can be referenced as needed for that task.</p>
<figure class="align-center" id="cic">
<a class="reference internal image-reference" href="../_images/cic.png"><img alt="CIC Format" src="../_images/cic.png" style="width: 830.5px; height: 361.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.14 </span><span class="caption-text">Example of Corpus-in-Context Prompting for retrieval.</span><a class="headerlink" href="#cic" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>CiC prompting leverages LLM’s capacity to follow instructions by carefully annotating the corpus with document IDs. It benefits from a strong, capable models to retrieve over large corpora provided in context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">urls</span> <span class="o">=</span> <span class="n">urls</span>

        <span class="c1"># Add new content to input following CIC format to enable citations</span>
        <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">urls_memory</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
            <span class="n">content</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extractor</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">url</span><span class="p">)</span><span class="o">.</span><span class="n">text_content</span>
            <span class="n">formatted_content</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;ID: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_id</span><span class="si">}</span><span class="s2"> | </span><span class="si">{</span><span class="n">content</span><span class="si">}</span><span class="s2"> | END ID: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">reference_id</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input</span> <span class="o">+=</span> <span class="n">formatted_content</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">reference_id</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="c1"># Update memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_memory</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">input</span>
</pre></div>
</div>
<p>The method <code class="docutils literal notranslate"><span class="pre">add_knowledge_base()</span></code> is a simple wrapper around the <code class="docutils literal notranslate"><span class="pre">add()</span></code> method. It is used to add URLs to the knowledge base, which are later cached by the LLM model as we will see later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">add_knowledge_base</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">urls</span><span class="p">)</span>
</pre></div>
</div>
<p>Later, when the user sends a message to the client, the <code class="docutils literal notranslate"><span class="pre">msg()</span></code> method is used to generate a response  while enabling citations. <code class="docutils literal notranslate"><span class="pre">self.content_generator</span></code> is an instance of our LLM model, which we will go through next.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">msg</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">add_citations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">add_citations</span><span class="p">:</span>
            <span class="n">msg</span> <span class="o">=</span> <span class="n">msg</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n\n</span><span class="s2"> For key statements, add Input ID to the response.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">content_generator</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
            <span class="n">input_content</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input</span><span class="p">,</span>
            <span class="n">user_instructions</span><span class="o">=</span><span class="n">msg</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">response_memory</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">response_memory</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">text</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">text</span>
</pre></div>
</div>
<p><strong>Prompt Caching</strong></p>
<p>LLM-based applications often involve repeatedly passing the same input tokens to a model, which can be inefficient and costly. Context caching addresses this by allowing you to cache input tokens after their first use and reference them in subsequent requests. This approach significantly reduces costs compared to repeatedly sending the same token corpus, especially at scale.</p>
<p>In our application, the user might passes a large knowledge base to the client that can be referenced multiple times by smaller user requests. Our <code class="docutils literal notranslate"><span class="pre">Client</span></code> class is composed of a <code class="docutils literal notranslate"><span class="pre">LLMBackend</span></code> class that takes the <code class="docutils literal notranslate"><span class="pre">input_memory</span></code> containing the entire knowledge base and any additional user added content.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">llm</span> <span class="o">=</span> <span class="n">LLMBackend</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_memory</span><span class="p">)</span>
</pre></div>
</div>
<p>In our <code class="docutils literal notranslate"><span class="pre">LLMBackend</span></code> Class, we leverage prompt caching on input tokens and uses them for subsequent requests.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LLMBackend</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">cache_ttl</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">60</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache</span> <span class="o">=</span> <span class="n">caching</span><span class="o">.</span><span class="n">CachedContent</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
            <span class="n">model</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span>
            <span class="n">display_name</span><span class="o">=</span><span class="s1">&#39;due_knowledge_base&#39;</span><span class="p">,</span> <span class="c1"># used to identify the cache</span>
            <span class="n">system_instruction</span><span class="o">=</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">compose_prompt</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">conversation_config</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">ttl</span><span class="o">=</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="n">cache_ttl</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">genai</span><span class="o">.</span><span class="n">GenerativeModel</span><span class="o">.</span><span class="n">from_cached_content</span><span class="p">(</span><span class="n">cached_content</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cache</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Quiz Generation</strong></p>
<p>Coming back to our <code class="docutils literal notranslate"><span class="pre">Client</span></code> class, we implement the <code class="docutils literal notranslate"><span class="pre">quiz()</span></code> method to generate a quiz based on the full input memory, i.e. the initial knowledge base and any additional user added content.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">quiz()</span></code> method returns a <code class="docutils literal notranslate"><span class="pre">Quiz</span></code> instance which behind the scenes caches input tokens. The user later can invoke its <code class="docutils literal notranslate"><span class="pre">generate()</span></code> method to generate a quiz passing the user instructions in <code class="docutils literal notranslate"><span class="pre">msg</span></code> parameter, as we will see later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">quiz</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">add_citations</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">num_questions</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a quiz instance based on full input memory.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quiz_instance</span> <span class="o">=</span> <span class="n">Quiz</span><span class="p">(</span>
                         <span class="nb">input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">input_memory</span><span class="p">,</span>
                         <span class="n">add_citations</span><span class="o">=</span><span class="n">add_citations</span><span class="p">,</span>
                         <span class="n">num_questions</span><span class="o">=</span><span class="n">num_questions</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">quiz_instance</span>
</pre></div>
</div>
<p>We write a simple prompt template for quiz generation:</p>
<blockquote>
<div><p>ROLE:</p>
<ul class="simple">
<li><p>You are a Harvard Professor providing a quiz.
INSTRUCTIONS:</p></li>
<li><p>Generate a quiz with {num_questions} questions based on the input.</p></li>
<li><p>The quiz should be multi-choice.</p></li>
<li><p>Answers should be provided at the end of the quiz.</p></li>
<li><p>Questions should have broad coverage of the input including multiple Input IDs.</p></li>
<li><p>Level of difficulty is advanced/hard.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">{citations}</span></code></p></li>
</ul>
<p>STRUCTURE:</p>
<ul class="simple">
<li><p>Sequence of questions and alternatives.</p></li>
<li><p>At the end provide the correct answers.</p></li>
</ul>
</div></blockquote>
<p>where, <code class="docutils literal notranslate"><span class="pre">{citations}</span></code> instructs the model to add CiC citations to the response if user requests it.</p>
</section>
<section id="example-usage">
<h4><a class="toc-backref" href="#id314" role="doc-backlink"><span class="section-number">5.5.2.3. </span>Example Usage</a><a class="headerlink" href="#example-usage" title="Permalink to this heading">¶</a></h4>
<p><strong>Dataset</strong></p>
<p>First, we will define our knowledge base.</p>
<ul class="simple">
<li><p>Harvard Class: <a class="reference external" href="https://scholar.harvard.edu/files/dlcammack/files/gov_1039_syllabus.pdf">GOV 1039 Syllabus</a></p></li>
<li><p>Class / Topic: “Rights”</p></li>
<li><p>Reading List:</p>
<ul>
<li><p>ID 1. The Declaration of Independence of the United States of America</p></li>
<li><p>ID 2. The United States Bill of Rights</p></li>
<li><p>ID 3. John F. Kennedy’s Inaugural Address</p></li>
<li><p>ID 4. Lincoln’s Gettysburg Address</p></li>
<li><p>ID 5. The United States Constitution</p></li>
<li><p>ID 6. Give Me Liberty or Give Me Death</p></li>
<li><p>ID 7. The Mayflower Compact</p></li>
<li><p>ID 8. Abraham Lincoln’s Second Inaugural Address</p></li>
<li><p>ID 9. Abraham Lincoln’s First Inaugural Address</p></li>
</ul>
</li>
</ul>
<p>We will take advantage of Project Gutenberg’s to create our knowledge base.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kb</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;https://www.gutenberg.org/cache/epub/</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">/pg</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.txt&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">9</span><span class="p">)]</span>
</pre></div>
</div>
</div>
</div>
<p>We will import our module <code class="docutils literal notranslate"><span class="pre">gemini_duo</span></code> as <code class="docutils literal notranslate"><span class="pre">genai_duo</span></code> and initialize the <code class="docutils literal notranslate"><span class="pre">Client</span></code> class with our knowledge base.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gemini_duo</span> <span class="k">as</span> <span class="nn">genai_duo</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Markdown</span><span class="p">,</span> <span class="n">display</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">duo</span> <span class="o">=</span> <span class="n">genai_duo</span><span class="o">.</span><span class="n">Client</span><span class="p">(</span><span class="n">knowledge_base</span><span class="o">=</span><span class="n">kb</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>At this point, we converted each book into markdown using MarkitDown and cached the content in our LLM model. We can access how many tokens we have cached in our LLM model by looking at the <code class="docutils literal notranslate"><span class="pre">usage_metadata</span></code> attribute of the Gemini’s model response. At this point, we have cached at total of 38470 tokens.</p>
<p>Now, we can add references to our knowledge base at anytime by calling the <code class="docutils literal notranslate"><span class="pre">add()</span></code> method. We add the following references:</p>
<ol class="arabic simple">
<li><p>The Magna Carta</p></li>
<li><p>William Shap McKechnie on Magna Carta book</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">study_references</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;https://www.gutenberg.org/cache/epub/10000/pg10000.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;https://www.gutenberg.org/cache/epub/65363/pg65363.txt&quot;</span><span class="p">]</span>

<span class="n">duo</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">study_references</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can instantiate a <code class="docutils literal notranslate"><span class="pre">Quiz</span></code> object and generate a quiz based on the full input memory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quiz</span> <span class="o">=</span> <span class="n">duo</span><span class="o">.</span><span class="n">quiz</span><span class="p">(</span><span class="n">add_citations</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">Markdown</span><span class="p">(</span><span class="n">quiz</span><span class="o">.</span><span class="n">generate</span><span class="p">()))</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference internal" href="#quiz"><span class="std std-numref">Fig. 5.15</span></a> shows a sample quiz with citations. Marked in yellow are the citations which refer to the input IDs of the resources we added to the model.</p>
<figure class="align-center" id="quiz">
<a class="reference internal image-reference" href="../_images/quiz.png"><img alt="Quiz with Citations" src="../_images/quiz.png" style="width: 891.0px; height: 772.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5.15 </span><span class="caption-text">Sample Quiz with Citations.</span><a class="headerlink" href="#quiz" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="id39">
<h4><a class="toc-backref" href="#id315" role="doc-backlink"><span class="section-number">5.5.2.4. </span>Discussion</a><a class="headerlink" href="#id39" title="Permalink to this heading">¶</a></h4>
<p>The experiment demonstrated the ability to build a knowledge base from multiple sources while leveraging prompt caching for efficiency and generate quizzes with citations for verifiability. The system successfully ingested content from Project Gutenberg texts, including historical documents like the Magna Carta, and used them to create interactive educational content.</p>
<p>However, several limitations emerged during this process:</p>
<ol class="arabic simple">
<li><p>Memory Management: The system currently loads all content into memory, which could become problematic with larger knowledge bases. A more scalable approach might involve chunking or streaming the content.</p></li>
<li><p>Citation Quality: While the system provides citations, they lack specificity - pointing to entire documents rather than specific passages or page numbers. This limits the ability to fact-check or verify specific claims.</p></li>
<li><p>Content Verification: While citations are provided, the system is not guaranteed to provide factual information. This could lead to potential hallucinations or misinterpretations.</p></li>
</ol>
<p>While limitations are present in this simple example, the case study highlights that not always complex systems are needed. Alternative simple strategies should be preferred when possible, particularly if capable, long-context window models are available and fit within the application requirements.</p>
</section>
</section>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id316" role="doc-backlink"><span class="section-number">5.6. </span>Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>This chapter has explored critical strategies and techniques for managing input data in LLM applications, focusing on three key areas: data parsing, retrieval augmentation, and practical implementation patterns. We examined how parsing tools like MarkItDown and Docling can transform diverse data formats into LLM-compatible representations, demonstrating through case studies how parser quality can impact LLM performance. The chapter also investigated retrieval augmentation techniques, particularly RAG systems, showing how they can enhance LLM capabilities by providing access to external knowledge while discussing their future relevance in the context of emerging long-context language models.</p>
<p>Through our case studies, we demonstrated practical approaches to handling common challenges in LLM applications. The Content Chunking with Contextual Linking case study illustrated techniques for managing long-form content generation while maintaining coherence across chunks. The Quiz Generation with Citations case study showcased how long-context windows can be effectively utilized without the need for complex retrieval systems, highlighting the importance of choosing the right approach based on specific application requirements rather than defaulting to more complex solutions.</p>
<p>As the field continues to evolve, the choice between traditional RAG systems and emerging long-context models will likely become increasingly nuanced. While RAGs offer cost-effective solutions for incorporating external knowledge, the rise of long-context models suggests a future where simpler architectures might suffice for many applications. The key insight is that effective input data management requires careful consideration of trade-offs among complexity, cost, and performance, always guided by specific application requirements rather than following a one-size-fits-all approach. Success in building robust LLM applications will depend on understanding these trade-offs and selecting appropriate strategies for each use case.</p>
<p><a class="reference external" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="CC BY-NC-SA 4.0" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" /></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@misc</span><span class="p">{</span><span class="n">tharsistpsouza2024tamingllms</span><span class="p">,</span>
  <span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">Tharsis</span> <span class="n">T</span><span class="o">.</span> <span class="n">P</span><span class="o">.</span> <span class="n">Souza</span><span class="p">},</span>
  <span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">Taming</span> <span class="n">LLMs</span><span class="p">:</span> <span class="n">A</span> <span class="n">Practical</span> <span class="n">Guide</span> <span class="n">to</span> <span class="n">LLM</span> <span class="n">Pitfalls</span> <span class="k">with</span> <span class="n">Open</span> <span class="n">Source</span> <span class="n">Software</span><span class="p">},</span>
  <span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2024</span><span class="p">},</span>
  <span class="n">chapter</span> <span class="o">=</span> <span class="p">{</span><span class="n">Managing</span> <span class="n">Input</span> <span class="n">Data</span><span class="p">},</span>
  <span class="n">journal</span> <span class="o">=</span> <span class="p">{</span><span class="n">GitHub</span> <span class="n">repository</span><span class="p">},</span>
  <span class="n">url</span> <span class="o">=</span> <span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">souzatharsis</span><span class="o">/</span><span class="n">tamingLLMs</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id317" role="doc-backlink"><span class="section-number">5.7. </span>References</a><a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id40">
<div class="citation" id="id160" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">AG24</a><span class="fn-bracket">]</span></span>
<p>Jay Alammar and Maarten Grootendorst. <em>Hands-On Large Language Models</em>. O'Reilly, 2024. ISBN 978-1098150969. URL: <a class="reference external" href="https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/">https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/</a>.</p>
</div>
<div class="citation" id="id197" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">AWP+24</a><span class="fn-bracket">]</span></span>
<p>Alfonso Amayuelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang. Knowledge of knowledge: exploring known-unknowns uncertainty with large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, <em>Findings of the Association for Computational Linguistics: ACL 2024</em>, 6416–6432. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2024.findings-acl.383">https://aclanthology.org/2024.findings-acl.383</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2024.findings-acl.383">doi:10.18653/v1/2024.findings-acl.383</a>.</p>
</div>
<div class="citation" id="id273" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id290">BCV14</a><span class="fn-bracket">]</span></span>
<p>Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: a review and new perspectives. 2014. URL: <a class="reference external" href="https://arxiv.org/abs/1206.5538">https://arxiv.org/abs/1206.5538</a>, <a class="reference external" href="https://arxiv.org/abs/1206.5538">arXiv:1206.5538</a>.</p>
</div>
<div class="citation" id="id161" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">Dia24</a><span class="fn-bracket">]</span></span>
<p>Nir Diamant. Rag techniques. GitHub Repository, 2024. Collection of advanced RAG techniques and implementation patterns. URL: <a class="reference external" href="https://github.com/NirDiamant/RAG_Techniques">https://github.com/NirDiamant/RAG_Techniques</a>.</p>
</div>
<div class="citation" id="id282" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>HRK+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin X Wang, and Sadid Hasan. Does prompt formatting have any impact on llm performance? 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2411.10541">https://arxiv.org/abs/2411.10541</a>, <a class="reference external" href="https://arxiv.org/abs/2411.10541">arXiv:2411.10541</a>.</p>
</div>
<div class="citation" id="id275" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">JLZ+24</a><span class="fn-bracket">]</span></span>
<p>Mathew Jacob, Erik Lindgren, Matei Zaharia, Michael Carbin, Omar Khattab, and Andrew Drozdov. Drowning in documents: consequences of scaling reranker inference. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2411.11767">https://arxiv.org/abs/2411.11767</a>, <a class="reference external" href="https://arxiv.org/abs/2411.11767">arXiv:2411.11767</a>.</p>
</div>
<div class="citation" id="id159" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">Kim24</a><span class="fn-bracket">]</span></span>
<p>Abhinav Kimothi. <em>A Simple Guide to Retrieval Augmented Generation</em>. Manning Publications, 2024. ISBN 9781633435858. Manning Early Access Program (MEAP). URL: <a class="reference external" href="https://www.manning.com/books/a-simple-guide-to-retrieval-augmented-generation">https://www.manning.com/books/a-simple-guide-to-retrieval-augmented-generation</a>.</p>
</div>
<div class="citation" id="id198" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">KSR24</a><span class="fn-bracket">]</span></span>
<p>Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic forgetting in language models via implicit inference. In <em>The Twelfth International Conference on Learning Representations</em>. 2024. URL: <a class="reference external" href="https://openreview.net/forum?id=VrHiF2hsrm">https://openreview.net/forum?id=VrHiF2hsrm</a>.</p>
</div>
<div class="citation" id="id278" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LCD+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id31">2</a>,<a role="doc-backlink" href="#id38">3</a>)</span>
<p>Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, Devendra Singh Sachan, Michael Boratko, Yi Luan, Sébastien M. R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, Jeremy R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu. Can long-context language models subsume retrieval, rag, sql, and more? 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2406.13121">https://arxiv.org/abs/2406.13121</a>, <a class="reference external" href="https://arxiv.org/abs/2406.13121">arXiv:2406.13121</a>.</p>
</div>
<div class="citation" id="id143" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">LPP+21</a><span class="fn-bracket">]</span></span>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. 2021. URL: <a class="reference external" href="https://arxiv.org/abs/2005.11401">https://arxiv.org/abs/2005.11401</a>, <a class="reference external" href="https://arxiv.org/abs/2005.11401">arXiv:2005.11401</a>.</p>
</div>
<div class="citation" id="id276" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id30">LJZ+24</a><span class="fn-bracket">]</span></span>
<p>Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, and Zhicheng Dou. Retrollm: empowering large language models to retrieve fine-grained evidence within generation. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2412.11919">https://arxiv.org/abs/2412.11919</a>, <a class="reference external" href="https://arxiv.org/abs/2412.11919">arXiv:2412.11919</a>.</p>
</div>
<div class="citation" id="id279" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LLZ+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id28">1</a>,<a role="doc-backlink" href="#id29">2</a>)</span>
<p>Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Bendersky. Retrieval augmented generation or long-context llms? a comprehensive study and hybrid approach. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2407.16833">https://arxiv.org/abs/2407.16833</a>, <a class="reference external" href="https://arxiv.org/abs/2407.16833">arXiv:2407.16833</a>.</p>
</div>
<div class="citation" id="id274" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>LFC+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Kai Liu, Zhihang Fu, Chao Chen, Wei Zhang, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu, and Jieping Ye. Enhancing llm's cognition via structurization. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2407.16434">https://arxiv.org/abs/2407.16434</a>, <a class="reference external" href="https://arxiv.org/abs/2407.16434">arXiv:2407.16434</a>.</p>
</div>
<div class="citation" id="id44" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">Lla24</a><span class="fn-bracket">]</span></span>
<p>LlamaIndex. Llamaparse: extract structured data from text and pdfs using llms. 2024. LlamaParse. URL: <a class="reference external" href="https://github.com/run-llama/llama_parse">https://github.com/run-llama/llama_parse</a>.</p>
</div>
<div class="citation" id="id199" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">NBGC24</a><span class="fn-bracket">]</span></span>
<p>Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. When do LLMs need retrieval augmentation? mitigating LLMs' overconfidence helps retrieval augmentation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, <em>Findings of the Association for Computational Linguistics: ACL 2024</em>, 11375–11388. Bangkok, Thailand, August 2024. Association for Computational Linguistics. URL: <a class="reference external" href="https://aclanthology.org/2024.findings-acl.675">https://aclanthology.org/2024.findings-acl.675</a>, <a class="reference external" href="https://doi.org/10.18653/v1/2024.findings-acl.675">doi:10.18653/v1/2024.findings-acl.675</a>.</p>
</div>
<div class="citation" id="id194" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>TDW+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Jiejun Tan, Zhicheng Dou, Wen Wang, Mang Wang, Weipeng Chen, and Ji-Rong Wen. Htmlrag: html is better than plain text for modeling retrieved knowledge in rag systems. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2411.02959">https://arxiv.org/abs/2411.02959</a>, <a class="reference external" href="https://arxiv.org/abs/2411.02959">arXiv:2411.02959</a>.</p>
</div>
<div class="citation" id="id277" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id32">WWS+23</a><span class="fn-bracket">]</span></span>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. 2023. URL: <a class="reference external" href="https://arxiv.org/abs/2201.11903">https://arxiv.org/abs/2201.11903</a>, <a class="reference external" href="https://arxiv.org/abs/2201.11903">arXiv:2201.11903</a>.</p>
</div>
<div class="citation" id="id281" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">WIP+24</a><span class="fn-bracket">]</span></span>
<p>Yunshu Wu, Hayate Iso, Pouya Pezeshkpour, Nikita Bhutani, and Estevam Hruschka. Less is more for long document summary evaluation by llms. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2309.07382">https://arxiv.org/abs/2309.07382</a>, <a class="reference external" href="https://arxiv.org/abs/2309.07382">arXiv:2309.07382</a>.</p>
</div>
<div class="citation" id="id193" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id17">ZLJ+24</a><span class="fn-bracket">]</span></span>
<p>Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. Metacognitive retrieval-augmented large language models. In <em>Proceedings of the ACM Web Conference 2024</em>, WWW '24, 1453–1463. New York, NY, USA, 2024. Association for Computing Machinery. URL: <a class="reference external" href="https://doi.org/10.1145/3589334.3645481">https://doi.org/10.1145/3589334.3645481</a>, <a class="reference external" href="https://doi.org/10.1145/3589334.3645481">doi:10.1145/3589334.3645481</a>.</p>
</div>
<div class="citation" id="id195" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Anthropic4a<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id36">1</a>,<a role="doc-backlink" href="#id37">2</a>)</span>
<p>Anthropic. Introducing contextual retrieval. 09 2024a. URL: <a class="reference external" href="https://www.anthropic.com/news/contextual-retrieval">https://www.anthropic.com/news/contextual-retrieval</a>.</p>
</div>
<div class="citation" id="id162" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">AthinaAI24</a><span class="fn-bracket">]</span></span>
<p>AthinaAI. Rag cookbooks. GitHub Repository, 2024. Collection of recipes and best practices for building RAG applications. URL: <a class="reference external" href="https://github.com/athina-ai/rag-cookbooks">https://github.com/athina-ai/rag-cookbooks</a>.</p>
</div>
<div class="citation" id="id289" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">ChromaDB4a</a><span class="fn-bracket">]</span></span>
<p>ChromaDB. Chromadb cookbook: hnsw configuration. Website, 2024a. URL: <a class="reference external" href="https://cookbook.chromadb.dev/core/configuration/#hnsw-configuration">https://cookbook.chromadb.dev/core/configuration/#hnsw-configuration</a>.</p>
</div>
<div class="citation" id="id287" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">ChromaDB4b</a><span class="fn-bracket">]</span></span>
<p>ChromaDB. Chromadb documentation. Website, 2024b. URL: <a class="reference external" href="https://docs.trychroma.com/">https://docs.trychroma.com/</a>.</p>
</div>
<div class="citation" id="id286" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">HuggingFace4f</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Sentence transformers. Website, 2024f. URL: <a class="reference external" href="https://huggingface.co/sentence-transformers">https://huggingface.co/sentence-transformers</a>.</p>
</div>
<div class="citation" id="id283" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">HuggingFace4i</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Massive text embedding benchmark (mteb) leaderboard. Website, 2024i. URL: <a class="reference external" href="https://huggingface.co/spaces/mteb/leaderboard">https://huggingface.co/spaces/mteb/leaderboard</a>.</p>
</div>
<div class="citation" id="id145" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">IBMResearch24</a><span class="fn-bracket">]</span></span>
<p>IBM Research. Docling: a document-level linguistic annotation framework. GitHub Repository, 2024. Framework for document-level linguistic annotation and analysis. URL: <a class="reference external" href="https://github.com/DS4SD/docling">https://github.com/DS4SD/docling</a>.</p>
</div>
<div class="citation" id="id92" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id35">LangChain24</a><span class="fn-bracket">]</span></span>
<p>LangChain. Text splitters - langchain documentation. <a class="reference external" href="https://python.langchain.com/docs/how_to/#text-splitters">https://python.langchain.com/docs/how_to/#text-splitters</a>, 2024. Accessed: 12/07/2024.</p>
</div>
<div class="citation" id="id284" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id33">LlamaIndex24</a><span class="fn-bracket">]</span></span>
<p>LlamaIndex. Storing - llamaindex documentation. Website, 2024. URL: <a class="reference external" href="https://docs.llamaindex.ai/en/stable/understanding/storing/storing/">https://docs.llamaindex.ai/en/stable/understanding/storing/storing/</a>.</p>
</div>
<div class="citation" id="id147" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">MendableAI24</a><span class="fn-bracket">]</span></span>
<p>Mendable AI. Firecrawl: a fast and efficient web crawler for llm training data. GitHub Repository, 2024. High-performance web crawler optimized for collecting LLM training data. URL: <a class="reference external" href="https://github.com/mendableai/firecrawl">https://github.com/mendableai/firecrawl</a>.</p>
</div>
<div class="citation" id="id149" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>MerrillLynch24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id10">2</a>)</span>
<p>Merrill Lynch. Chief investment officer capital market outlook. CIO Weekly Letter, 2024. URL: <a class="reference external" href="https://olui2.fs.ml.com/publish/content/application/pdf/gwmol/me-cio-weekly-letter.pdf">https://olui2.fs.ml.com/publish/content/application/pdf/gwmol/me-cio-weekly-letter.pdf</a>.</p>
</div>
<div class="citation" id="id148" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">Microsoft24</a><span class="fn-bracket">]</span></span>
<p>Microsoft. Markitdown: structured generation with large language models. GitHub Repository, 2024. Framework for structured text generation using LLMs. URL: <a class="reference external" href="https://github.com/microsoft/markitdown">https://github.com/microsoft/markitdown</a>.</p>
</div>
<div class="citation" id="id288" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id291">OpenAI24</a><span class="fn-bracket">]</span></span>
<p>OpenAI. What are embeddings? Website, 2024. URL: <a class="reference external" href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings">https://platform.openai.com/docs/guides/embeddings/what-are-embeddings</a>.</p>
</div>
<div class="citation" id="id280" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">Ragas24</a><span class="fn-bracket">]</span></span>
<p>Ragas. Rag evaluation - ragas documentation. Website, 2024. URL: <a class="reference external" href="https://docs.ragas.io/en/stable/getstarted/rag_evaluation/">https://docs.ragas.io/en/stable/getstarted/rag_evaluation/</a>.</p>
</div>
<div class="citation" id="id146" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">Unstructuredio24</a><span class="fn-bracket">]</span></span>
<p>Unstructured.io. Unstructured: open source libraries for pre-processing documents. GitHub Repository, 2024. URL: <a class="reference external" href="https://github.com/Unstructured-IO/unstructured">https://github.com/Unstructured-IO/unstructured</a>.</p>
</div>
<div class="citation" id="id285" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id34">ZenML24</a><span class="fn-bracket">]</span></span>
<p>ZenML. Scaling rag accuracy from 49% to 86% in finance q&amp;a assistant. Website, 2024. URL: <a class="reference external" href="https://www.zenml.io/llmops-database/scaling-rag-accuracy-from-49-to-86-in-finance-q-a-assistant">https://www.zenml.io/llmops-database/scaling-rag-accuracy-from-49-to-86-in-finance-q-a-assistant</a>.</p>
</div>
</div>
</div>
<hr class="footnotes docutils" />
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="embeddings-definition" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">1</a><span class="fn-bracket">]</span></span>
<p>Bengio et al. <span id="id290">[<a class="reference internal" href="#id273" title="Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: a review and new perspectives. 2014. URL: https://arxiv.org/abs/1206.5538, arXiv:1206.5538.">Bengio <em>et al.</em>, 2014</a>]</span> provide serves as an excellent reference for representation learning in general including embeddings. OpenAI provides a good intro to Embeddings for developers <span id="id291">[<a class="reference internal" href="#id288" title="OpenAI. What are embeddings? Website, 2024. URL: https://platform.openai.com/docs/guides/embeddings/what-are-embeddings.">OpenAI, 2024</a>]</span></p>
</aside>
<aside class="footnote brackets" id="chroma-embeddings" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">2</a><span class="fn-bracket">]</span></span>
<p>ChromaDB enables custom embedding functions and provides a list of wrappers around commonly used embedding models and APIs <a class="reference external" href="https://docs.trychroma.com/docs/embeddings/embedding-functions">https://docs.trychroma.com/docs/embeddings/embedding-functions</a></p>
</aside>
</aside>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="structured_output.html"
       title="previous chapter">← <span class="section-number">4. </span>Structured Output</a>
  </li>
  <li class="next">
    <a href="safety.html"
       title="next chapter"><span class="section-number">6. </span>Safety →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; Copyright Tharsis T. P. Souza, 2024.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.9.1.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>