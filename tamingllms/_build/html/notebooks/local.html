<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

      <title>8. Breaking Free from Cloud Providers</title>
    
          <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../_static/theme.css " type="text/css" />
          <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-thebe.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/scripts/sphinx-book-theme.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../_static/design-tabs.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
        <script async="async" src="../_static/sphinx-thebe.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../_static/theme-vendors.js"></script> -->
      <script src="../_static/theme.js" defer></script>
      <link rel="canonical" href="https://souzatharsis.github.io/tamingllms/notebooks/local.html" />
    
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="prev" title="7. Preference-Based Alignment" href="alignment.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../markdown/toc.html" class="home-link">
    
      <span class="site-name">tamingLLMs</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../markdown/toc.html#taming-llms">taming llms</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="../markdown/preface.html" class="reference internal ">Preface</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../markdown/intro.html" class="reference internal ">Introduction</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="output_size_limit.html" class="reference internal ">Output Size Limit</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="structured_output.html" class="reference internal ">Wrestling with Structured Output</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="evals.html" class="reference internal ">The Evals Gap</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="safety.html" class="reference internal ">Safety</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="alignment.html" class="reference internal ">Preference-Based Alignment</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">Breaking Free from Cloud Providers</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#introduction" class="reference internal">Introduction</a></li>
                
                  <li class="toctree-l2"><a href="#local-models-considerations" class="reference internal">Local Models Considerations</a></li>
                
                  <li class="toctree-l2"><a href="#tools-for-local-llm-deployment" class="reference internal">Tools for Local LLM Deployment</a></li>
                
                  <li class="toctree-l2"><a href="#case-study-the-effect-of-quantization-on-llm-performance" class="reference internal">Case Study: The Effect of Quantization on LLM Performance</a></li>
                
                  <li class="toctree-l2"><a href="#conclusion" class="reference internal">Conclusion</a></li>
                
                  <li class="toctree-l2"><a href="#citation" class="reference internal">Citation</a></li>
                
                  <li class="toctree-l2"><a href="#references" class="reference internal">References</a></li>
                
              </ul>
            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../markdown/toc.html">Docs</a> &raquo;</li>
    
    <li><span class="section-number">8. </span>Breaking Free from Cloud Providers</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="alignment.html"
       title="previous chapter">← <span class="section-number">7. </span>Preference-Based Alignment</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section class="tex2jax_ignore mathjax_ignore" id="breaking-free-from-cloud-providers">
<h1><a class="toc-backref" href="#id171" role="doc-backlink"><span class="section-number">8. </span>Breaking Free from Cloud Providers</a><a class="headerlink" href="#breaking-free-from-cloud-providers" title="Permalink to this heading">¶</a></h1>
<blockquote class="epigraph">
<div><p>Freedom is something that dies unless it’s used.</p>
<p class="attribution">—Hunter S. Thompson</p>
</div></blockquote>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#breaking-free-from-cloud-providers" id="id171">Breaking Free from Cloud Providers</a></p>
<ul>
<li><p><a class="reference internal" href="#introduction" id="id172">Introduction</a></p></li>
<li><p><a class="reference internal" href="#local-models-considerations" id="id173">Local Models Considerations</a></p></li>
<li><p><a class="reference internal" href="#tools-for-local-llm-deployment" id="id174">Tools for Local LLM Deployment</a></p>
<ul>
<li><p><a class="reference internal" href="#serving-models" id="id175">Serving Models</a></p>
<ul>
<li><p><a class="reference internal" href="#llama-cpp" id="id176">LLama.cpp</a></p></li>
<li><p><a class="reference internal" href="#llamafile" id="id177">Llamafile</a></p></li>
<li><p><a class="reference internal" href="#ollama" id="id178">Ollama</a></p></li>
<li><p><a class="reference internal" href="#comparison" id="id179">Comparison</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#ui" id="id180">UI</a></p>
<ul>
<li><p><a class="reference internal" href="#lm-studio" id="id181">LM Studio</a></p></li>
<li><p><a class="reference internal" href="#jan" id="id182">Jan</a></p></li>
<li><p><a class="reference internal" href="#open-webui" id="id183">Open WebUI</a></p></li>
<li><p><a class="reference internal" href="#id14" id="id184">Comparison</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#case-study-the-effect-of-quantization-on-llm-performance" id="id185">Case Study: The Effect of Quantization on LLM Performance</a></p>
<ul>
<li><p><a class="reference internal" href="#prompts-dataset" id="id186">Prompts Dataset</a></p></li>
<li><p><a class="reference internal" href="#quantization" id="id187">Quantization</a></p></li>
<li><p><a class="reference internal" href="#benchmarking" id="id188">Benchmarking</a></p></li>
<li><p><a class="reference internal" href="#results" id="id189">Results</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion" id="id190">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#citation" id="id191">Citation</a></p></li>
<li><p><a class="reference internal" href="#references" id="id192">References</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="introduction">
<h2><a class="toc-backref" href="#id172" role="doc-backlink"><span class="section-number">8.1. </span>Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>Running LLMs locally versus using cloud APIs offers several important advantages.</p>
<p>Privacy-sensitive data processing is one of the primary reasons for running LLMs locally. Organizations handling medical records must comply with HIPAA regulations that require data to remain on-premise. Similarly, businesses processing confidential documents and intellectual property, as well as organizations subject to GDPR and other privacy regulations, need to maintain strict control over their data processing pipeline.</p>
<p>Cost considerations become significant when operating at scale. Organizations running high-volume applications can face prohibitive API costs with cloud-based solutions. Development and testing environments that require frequent model interactions, educational institutions supporting multiple student projects, and research initiatives involving extensive model experimentation can all achieve substantial cost savings through local deployment.</p>
<p>Applications with stringent latency requirements form another important category. Real-time systems where network delays would be unacceptable, edge computing scenarios demanding quick responses, and interactive applications requiring sub-second performance all benefit from local deployment. This extends to embedded systems in IoT devices where cloud connectivity might be unreliable or impractical.</p>
<p>Finally, local deployment enables deeper customization and fine-tuning capabilities. Organizations can perform specialized domain adaptation through model modifications, experiment with different architectures and parameters, and integrate models with proprietary systems and workflows. This flexibility is particularly valuable for developing novel applications that require direct model access and manipulation.</p>
</section>
<section id="local-models-considerations">
<h2><a class="toc-backref" href="#id173" role="doc-backlink"><span class="section-number">8.2. </span>Local Models Considerations</a><a class="headerlink" href="#local-models-considerations" title="Permalink to this heading">¶</a></h2>
</section>
<section id="tools-for-local-llm-deployment">
<h2><a class="toc-backref" href="#id174" role="doc-backlink"><span class="section-number">8.3. </span>Tools for Local LLM Deployment</a><a class="headerlink" href="#tools-for-local-llm-deployment" title="Permalink to this heading">¶</a></h2>
<p>Local LLM deployment tools generally fall into two categories: inference-focused tools that prioritize performance and programmability for technical users requiring production-grade deployments, and user interface (UI) tools that emphasize accessibility through graphical interfaces for non-technical users, trading some performance for ease of use and broader adoption. In the following sections we will explore some of these tools discussing their features, capabilities, and trade-offs.</p>
<section id="serving-models">
<h3><a class="toc-backref" href="#id175" role="doc-backlink"><span class="section-number">8.3.1. </span>Serving Models</a><a class="headerlink" href="#serving-models" title="Permalink to this heading">¶</a></h3>
<p>Before exploring specific tools, it’s important to understand what “serving” an LLM model means in practice. Serving refers to the process of making a trained language model available for inference. At a high level, this involves setting up the infrastructure needed to accept and process input text and generate responses while efficiently managing system resources. The serving process involves several key responsibilities:</p>
<ol class="arabic simple">
<li><p><strong>Model Loading and Initialization</strong></p></li>
</ol>
<ul class="simple">
<li><p>Loading the trained model weights and parameters into memory</p></li>
<li><p>Initializing any required runtime configurations and optimizations</p></li>
<li><p>Setting up inference pipelines and processing workflows</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Resource Management</strong></p></li>
</ol>
<ul class="simple">
<li><p>Allocating and managing system memory (RAM/VRAM) for model weights</p></li>
<li><p>Handling computational resources like CPU/GPU efficiently</p></li>
<li><p>Implementing caching and batching strategies where appropriate</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Request Processing and Inference</strong></p></li>
</ol>
<ul class="simple">
<li><p>Accepting input requests through defined interfaces</p></li>
<li><p>Converting input text into token vectors <span class="math notranslate nohighlight">\(\mathbf{x} = [x_1, x_2, ..., x_n]\)</span> through tokenization</p></li>
<li><p>Computing probability distributions <span class="math notranslate nohighlight">\(P(x_{n+1}|x_1, x_2, ..., x_n; θ)\)</span> for next tokens</p></li>
<li><p>Performing matrix multiplications and attention computations</p></li>
<li><p>Sampling each new token from the calculated probability distribution</p></li>
<li><p>Post-processing and returning responses</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>Performance Optimization</strong></p></li>
</ol>
<ul class="simple">
<li><p>Implementing techniques like quantization to reduce memory usage</p></li>
<li><p>Optimizing inference speed through batching and caching</p></li>
<li><p>Managing concurrent requests and load balancing</p></li>
<li><p>Monitoring system resource utilization</p></li>
</ul>
<p>The serving layer acts as the bridge between the trained model and applications while working on top of a hardware stack as shown in <a class="reference internal" href="#local-inference"><span class="std std-numref">Fig. 8.1</span></a>. Getting this layer right is crucial for building locally-served reliable AI-powered applications, as it directly impacts the end-user experience in terms of response times, reliability, and resource efficiency.</p>
<figure class="align-center" id="local-inference">
<a class="reference internal image-reference" href="../_images/local_inference.svg"><img alt="Local Inference Server" height="523" src="../_images/local_inference.svg" width="626" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">Local Inference Server.</span><a class="headerlink" href="#local-inference" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>There are several key tools for serving local LLMs. We will cover the following:</p>
<ul class="simple">
<li><p><strong>LLama.cpp</strong>: A highly optimized C++ implementation for running LLMs on consumer hardware</p></li>
<li><p><strong>Llamafile</strong>: A self-contained executable format by Mozilla for easy model distribution and deployment</p></li>
<li><p><strong>Ollama</strong>: A tool that simplifies running and managing local LLMs with Docker-like commands</p></li>
</ul>
<p>Let’s explore each of these options in detail.</p>
<section id="llama-cpp">
<h4><a class="toc-backref" href="#id176" role="doc-backlink"><span class="section-number">8.3.1.1. </span>LLama.cpp</a><a class="headerlink" href="#llama-cpp" title="Permalink to this heading">¶</a></h4>
<p>LLama.cpp <span id="id1">[<a class="reference internal" href="#id86" title="Georgi Gerganov and contributors. Llama.cpp. GitHub Repository, 2024a. High-performance inference of LLaMA models in pure C/C++. URL: https://github.com/ggerganov/llama.cpp.">Gerganov and contributors, 2024a</a>]</span> is an MIT-licensed open source optimized implementation of the <strong>LLama</strong> model architecture designed to run efficiently on machines with limited memory.</p>
<p>Originally developed by Georgi Gerganov and today counting with hundreds of contributors, this C/C++ version provides a simplified interface and advanced features that allow language models to run without overwhelming systems. With the ability to run in resource-constrained environments, LLama.cpp makes powerful language models more accessible and practical for a variety of applications.</p>
<p>In its “Manifesto” <span id="id2">[<a class="reference internal" href="#id76" title="Georgi Gerganov and others. Quantization of llama models - discussion. GitHub Discussion, 2023. Discussion thread about quantization techniques and tradeoffs in llama.cpp. URL: https://github.com/ggerganov/llama.cpp/discussions/205.">Gerganov and others, 2023</a>]</span>, the author sees significant potential in bringing AI from cloud to edge devices, emphasizing the importance of keeping development lightweight, experimental, and enjoyable rather than getting bogged down in complex engineering challenges. The author states a vision that emphasizes maintaining an exploratory, hacker-minded approach while building practical edge computing solutions highlighting the following core principles:</p>
<ul class="simple">
<li><p>“Will remain open-source”</p></li>
<li><p>Focuses on simplicity and efficiency in codebase</p></li>
<li><p>Emphasizes quick prototyping over premature optimization</p></li>
<li><p>Aims to stay adaptable given rapid AI model improvements</p></li>
<li><p>Values practical experimentation over complex engineering</p></li>
</ul>
<p>LLama.cpp implementation characteristics include:</p>
<ol class="arabic simple">
<li><p><strong>Memory Efficiency</strong>: The main advantage of LLama.cpp is its ability to reduce memory requirements, allowing users to run large language models on at the edge.</p></li>
<li><p><strong>Computational Efficiency</strong>: Besides reducing memory usage, LLama.cpp also focuses on improving execution efficiency, using specific C++ code optimizations to accelerate the process.</p></li>
<li><p><strong>Ease of Implementation</strong>: Although it’s a lighter solution, LLama.cpp doesn’t sacrifice result quality. It maintains the ability to generate texts and perform NLP tasks with high precision.</p></li>
</ol>
<p><strong>GGUF</strong></p>
<p>GGUF (GPT-Generated Unified Format) <span id="id3">[<a class="reference internal" href="#id89" title="Georgi Gerganov and contributors. Gguf file format specification. GitHub Repository, 2024b. Technical specification of the GGUF file format for efficient model storage and inference. URL: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md.">Gerganov and contributors, 2024b</a>]</span> is the latest model format used by LLama.cpp, replacing the older GGML format. It was designed specifically for efficient inference of large language models on consumer hardware. The key features that make GGUF particularly valuable include <span id="id4">[<a class="reference internal" href="#id87" title="IBM Think. Gguf vs ggml: what's the difference? 2024. Comparison of GGUF and GGML model formats. URL: https://www.ibm.com/think/topics/gguf-versus-ggml.">IBM Think, 2024</a>]</span>:</p>
<ul class="simple">
<li><p>Improved quantization: GGUF supports multiple quantization levels to reduce model size while preserving performance. Common quantization schemes that are supported by GGUF include:</p>
<ul>
<li><p>2-bit quantization: Offers the highest compression, significantly reducing model size and inference speed, though with a potential impact on accuracy.</p></li>
<li><p>4-bit quantization: Balances compression and accuracy, making it suitable for many practical applications.</p></li>
<li><p>8-bit quantization: Provides good accuracy with moderate compression, widely used in various applications.</p></li>
</ul>
</li>
<li><p>Metadata support: The format includes standardized metadata about model architecture, tokenization, and other properties</p></li>
<li><p>Memory mapping: Enables efficient loading of large models by mapping them directly from disk rather than loading entirely into RAM</p></li>
<li><p>Architecture-specific optimizations: Takes advantage of CPU/GPU specific instructions for faster inference</p></li>
<li><p>Versioning support: Includes proper versioning to handle format evolution and backwards compatibility</p></li>
</ul>
<p>These capabilities make GGUF models significantly more practical for running LLMs locally compared to full-precision formats, often dramatically reducing memory requirements. Hugging Face hosts a growing collection of pre-converted GGUF models <span id="id5">[<a class="reference internal" href="#id88" title="Hugging Face. Gguf models on hugging face. Online Repository, 2024x. Collection of models in GGUF format for efficient local inference. URL: https://huggingface.co/models?search=gguf.">Hugging Face, 2024x</a>]</span> and provides a tool (ggml-org/gguf-my-repo) to convert existing models to GGUF format, making it easier for developers to access and deploy optimized versions of popular language models.</p>
<p><strong>Setup</strong></p>
<p>Please follow the instructions on the <a class="reference external" href="https://github.com/ggerganov/llama.cpp">llama.cpp GitHub repository</a> <span id="id6">[<a class="reference internal" href="#id86" title="Georgi Gerganov and contributors. Llama.cpp. GitHub Repository, 2024a. High-performance inference of LLaMA models in pure C/C++. URL: https://github.com/ggerganov/llama.cpp.">Gerganov and contributors, 2024a</a>]</span> to install and compile the library.</p>
<p>Here, we will compile the library from source on a Linux machine with 8 jobs in parallel for enhanced performance (add the -j argument to run multiple jobs in parallel).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>cmake

cmake<span class="w"> </span>-B<span class="w"> </span>build
cmake<span class="w"> </span>--build<span class="w"> </span>build<span class="w"> </span>--config<span class="w"> </span>Release<span class="w"> </span>-j<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
<p>Python bindings are available through <code class="docutils literal notranslate"><span class="pre">llama-cpp-python</span></code> <span id="id7">[<a class="reference internal" href="#id81" title="Andrei Betlen and contributors. Llama-cpp-python. GitHub Repository, 2024. Python bindings for llama.cpp library enabling high-performance inference of LLaMA models. URL: https://github.com/abetlen/llama-cpp-python.">Betlen and contributors, 2024</a>]</span>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>llama-cpp-python
</pre></div>
</div>
<p><strong>llama-cli</strong></p>
<p>A comprehensive command line interface is available through <code class="docutils literal notranslate"><span class="pre">llama-cli</span></code> as demonstrated below, where we use the <code class="docutils literal notranslate"><span class="pre">-cnv</span></code> flag to run the model in a conversational mode. We will use <code class="docutils literal notranslate"><span class="pre">Qwen/Qwen2.5-0.5B-Instruct-GGUF</span></code> model. Download it from Hugging Face and place it in the <code class="docutils literal notranslate"><span class="pre">llamacpp/models</span></code> directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-cli<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q8_0.gguf<span class="w"> </span>-p<span class="w"> </span><span class="s2">&quot;You are a helpful assistant - Be succinct.&quot;</span><span class="w"> </span>-cnv
</pre></div>
</div>
<p>As a result, you can interact with the model in the terminal as a chatbot.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">==</span><span class="w"> </span>Running<span class="w"> </span><span class="k">in</span><span class="w"> </span>interactive<span class="w"> </span>mode.<span class="w"> </span><span class="o">==</span>
<span class="w"> </span>-<span class="w"> </span>Press<span class="w"> </span>Ctrl+C<span class="w"> </span>to<span class="w"> </span>interject<span class="w"> </span>at<span class="w"> </span>any<span class="w"> </span>time.
<span class="w"> </span>-<span class="w"> </span>Press<span class="w"> </span>Return<span class="w"> </span>to<span class="w"> </span><span class="k">return</span><span class="w"> </span>control<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>AI.
<span class="w"> </span>-<span class="w"> </span>To<span class="w"> </span><span class="k">return</span><span class="w"> </span>control<span class="w"> </span>without<span class="w"> </span>starting<span class="w"> </span>a<span class="w"> </span>new<span class="w"> </span>line,<span class="w"> </span>end<span class="w"> </span>your<span class="w"> </span>input<span class="w"> </span>with<span class="w"> </span><span class="s1">&#39;/&#39;</span>.
<span class="w"> </span>-<span class="w"> </span>If<span class="w"> </span>you<span class="w"> </span>want<span class="w"> </span>to<span class="w"> </span>submit<span class="w"> </span>another<span class="w"> </span>line,<span class="w"> </span>end<span class="w"> </span>your<span class="w"> </span>input<span class="w"> </span>with<span class="w"> </span><span class="s1">&#39;\&#39;</span>.

system
You<span class="w"> </span>are<span class="w"> </span>a<span class="w"> </span>helpful<span class="w"> </span>assistant<span class="w"> </span>-<span class="w"> </span>Be<span class="w"> </span>succinct.

&gt;<span class="w"> </span>What<span class="w"> </span>is<span class="w"> </span>the<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>life?
The<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>life<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>philosophical<span class="w"> </span>question<span class="w"> </span>that<span class="w"> </span>has<span class="w"> </span>been<span class="w"> </span>debated<span class="w"> </span>and<span class="w"> </span>debated<span class="w"> </span><span class="k">for</span><span class="w"> </span>thousands<span class="w"> </span>of<span class="w"> </span>years.<span class="w"> </span>Some<span class="w"> </span>people<span class="w"> </span>believe<span class="w"> </span>that<span class="w"> </span>the<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>life<span class="w"> </span>is<span class="w"> </span>to<span class="w"> </span>seek<span class="w"> </span>personal<span class="w"> </span>fulfillment<span class="w"> </span>and<span class="w"> </span>happiness,<span class="w"> </span><span class="k">while</span><span class="w"> </span>others<span class="w"> </span>believe<span class="w"> </span>that<span class="w"> </span>it<span class="w"> </span>is<span class="w"> </span>to<span class="w"> </span>find<span class="w"> </span>a<span class="w"> </span>purpose<span class="w"> </span><span class="k">in</span><span class="w"> </span>life<span class="w"> </span>that<span class="w"> </span>aligns<span class="w"> </span>with<span class="w"> </span>one<span class="s1">&#39;s values and beliefs. The answer may also vary depending on a person&#39;</span>s<span class="w"> </span>cultural,<span class="w"> </span>religious,<span class="w"> </span>or<span class="w"> </span>personal<span class="w"> </span>background.

&gt;<span class="w"> </span>Are<span class="w"> </span>LLMs<span class="w"> </span>more<span class="w"> </span>helpful<span class="w"> </span>than<span class="w"> </span>dangerous?
Yes,<span class="w"> </span>LLMs<span class="w"> </span><span class="o">(</span>Large<span class="w"> </span>Language<span class="w"> </span>Models<span class="o">)</span><span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>more<span class="w"> </span>helpful<span class="w"> </span>than<span class="w"> </span>dangerous<span class="w"> </span><span class="k">in</span><span class="w"> </span>many<span class="w"> </span>cases.<span class="w"> </span>They<span class="w"> </span>are<span class="w"> </span>designed<span class="w"> </span>to<span class="w"> </span>assist<span class="w"> </span>with<span class="w"> </span>a<span class="w"> </span>wide<span class="w"> </span>range<span class="w"> </span>of<span class="w"> </span>tasks,<span class="w"> </span>from<span class="w"> </span>generating<span class="w"> </span>text<span class="w"> </span>to<span class="w"> </span>providing<span class="w"> </span>information.<span class="w"> </span>They<span class="w"> </span>can<span class="w"> </span>also<span class="w"> </span>be<span class="w"> </span>used<span class="w"> </span>to<span class="w"> </span><span class="nb">help</span><span class="w"> </span>with<span class="w"> </span>decision-making<span class="w"> </span>and<span class="w"> </span>problem-solving.<span class="w"> </span>However,<span class="w"> </span>like<span class="w"> </span>any<span class="w"> </span>tool,<span class="w"> </span>LLMs<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>a<span class="w"> </span>tool<span class="w"> </span>of<span class="w"> </span>great<span class="w"> </span>power<span class="w"> </span><span class="k">if</span><span class="w"> </span>not<span class="w"> </span>used<span class="w"> </span>responsibly<span class="w"> </span>and<span class="w"> </span>ethically.<span class="w"> </span>It<span class="w"> </span>is<span class="w"> </span>important<span class="w"> </span>to<span class="w"> </span>use<span class="w"> </span>LLMs<span class="w"> </span><span class="k">for</span><span class="w"> </span>positive<span class="w"> </span>and<span class="w"> </span>beneficial<span class="w"> </span>purposes<span class="w"> </span><span class="k">while</span><span class="w"> </span>being<span class="w"> </span>mindful<span class="w"> </span>of<span class="w"> </span>their<span class="w"> </span>potential<span class="w"> </span>to<span class="w"> </span>harm.

&gt;<span class="w"> </span>Bye<span class="w"> </span>bye.<span class="w">       </span>
Goodbye!<span class="w"> </span>If<span class="w"> </span>you<span class="w"> </span>have<span class="w"> </span>any<span class="w"> </span>other<span class="w"> </span>questions,<span class="w"> </span>feel<span class="w"> </span>free<span class="w"> </span>to<span class="w"> </span>ask.
</pre></div>
</div>
<p><strong>llama-server</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">llama-server</span></code> is a server version of <code class="docutils literal notranslate"><span class="pre">llama-cli</span></code> that can be accessed via a web interface or API.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-server<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q8_0.gguf<span class="w"> </span>--port<span class="w"> </span><span class="m">8080</span>
</pre></div>
</div>
<p>This will start a server on port 8080.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>main:<span class="w"> </span>server<span class="w"> </span>is<span class="w"> </span>listening<span class="w"> </span>on<span class="w"> </span>http://127.0.0.1:8080<span class="w"> </span>-<span class="w"> </span>starting<span class="w"> </span>the<span class="w"> </span>main<span class="w"> </span>loop
</pre></div>
</div>
<p>Now we can send a request as we would for any Cloud API but here instead send a request to our local server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8080/v1/chat/completions<span class="w"> </span><span class="se">\</span>
-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer no-key&quot;</span><span class="w"> </span><span class="se">\</span>
-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">&quot;messages&quot;: [</span>
<span class="s1">    {</span>
<span class="s1">        &quot;role&quot;: &quot;system&quot;,</span>
<span class="s1">        &quot;content&quot;: &quot;You are a helpful assistant - Be succinct.&quot;</span>
<span class="s1">    },</span>
<span class="s1">    {</span>
<span class="s1">        &quot;role&quot;: &quot;user&quot;,</span>
<span class="s1">        &quot;content&quot;: &quot;What is the meaning of life?&quot;</span>
<span class="s1">    }</span>
<span class="s1">  ]</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p>We obtain a JSON response. As expected, assistant’s response is in <code class="docutils literal notranslate"><span class="pre">content[0].message.content</span></code> following OpenAI’s API format.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;choices&quot;</span><span class="p">:[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;finish_reason&quot;</span><span class="p">:</span><span class="s2">&quot;stop&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;index&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;message&quot;</span><span class="p">:{</span>
<span class="w">            </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;The meaning of life is a question that has been debated throughout history. Some people believe it is to find happiness and purpose, while others believe it is to seek knowledge and knowledge. Ultimately, the meaning of life is a deeply personal and subjective question that cannot be answered universally.&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;assistant&quot;</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">],</span>
<span class="w">   </span><span class="nt">&quot;created&quot;</span><span class="p">:</span><span class="mi">1734627879</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;object&quot;</span><span class="p">:</span><span class="s2">&quot;chat.completion&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;usage&quot;</span><span class="p">:{</span>
<span class="w">      </span><span class="nt">&quot;completion_tokens&quot;</span><span class="p">:</span><span class="mi">56</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;prompt_tokens&quot;</span><span class="p">:</span><span class="mi">29</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;total_tokens&quot;</span><span class="p">:</span><span class="mi">85</span>
<span class="w">   </span><span class="p">},</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="s2">&quot;chatcmpl-5Wl2TZJZDmzuPvxwP2GceDR8XbPsyHfm&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;timings&quot;</span><span class="p">:{</span>
<span class="w">      </span><span class="nt">&quot;prompt_n&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;prompt_ms&quot;</span><span class="p">:</span><span class="mf">48.132</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;prompt_per_token_ms&quot;</span><span class="p">:</span><span class="mf">48.132</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;prompt_per_second&quot;</span><span class="p">:</span><span class="mf">20.77619878666999</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;predicted_n&quot;</span><span class="p">:</span><span class="mi">56</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;predicted_ms&quot;</span><span class="p">:</span><span class="mf">1700.654</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;predicted_per_token_ms&quot;</span><span class="p">:</span><span class="mf">30.36882142857143</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;predicted_per_second&quot;</span><span class="p">:</span><span class="mf">32.92850867960208</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Grammars</strong></p>
<p>It is worth noting Llama.cpp provides a way to use grammars <span id="id8">[<a class="reference internal" href="#id84" title="Georgi Gerganov and contributors. Llama.cpp grammars documentation. GitHub Repository, 2024. Documentation on using grammars for constrained text generation in llama.cpp. URL: https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md.">Gerganov and contributors, 2024</a>]</span> to constrain the output of the model as demonstrated below. This is the same technique Ollama uses, a similar approach to Outlines’ to generate structured outputs from LLMs. See <a class="reference external" href="https://www.souzatharsis.com/tamingLLMs/notebooks/structured_output.html">Wrestling with Structured Outputs Chapter</a> for more details.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-cli<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q8_0.gguf<span class="w"> </span>--grammar-file<span class="w"> </span>grammars/json.gbnf<span class="w"> </span>-p<span class="w"> </span><span class="s1">&#39;Request: schedule a call at 8pm; Command:&#39;</span>

<span class="c1"># {&quot;appointmentTime&quot;: &quot;8pm&quot;, &quot;appointmentDetails&quot;: &quot;schedule a a call&quot;}</span>
</pre></div>
</div>
<p><strong>Python</strong></p>
<p>A handy Python binding <span id="id9">[<a class="reference internal" href="#id81" title="Andrei Betlen and contributors. Llama-cpp-python. GitHub Repository, 2024. Python bindings for llama.cpp library enabling high-performance inference of LLaMA models. URL: https://github.com/abetlen/llama-cpp-python.">Betlen and contributors, 2024</a>]</span> is available for LLama.cpp, which by default returns chat completions in OpenAI’s API chat format as below. The package is very comprehensive supporting JSON Mode, function calling, multi-modal models and more.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="s2">&quot;./models/qwen2.5-0.5b-instruct-q8_0.gguf&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span>
      <span class="n">model_path</span><span class="o">=</span><span class="n">MODEL_PATH</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">create_chat_completion</span><span class="p">(</span>
      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant - Be succinct.&quot;</span><span class="p">},</span>
          <span class="p">{</span>
              <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
              <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span>
          <span class="p">}</span>
      <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;choices&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;The meaning of life is a philosophical question that has been debated by philosophers, scientists, and individuals throughout history. Some people believe that the meaning of life is to find happiness and fulfillment, while others believe that it is to seek knowledge and understanding of the universe. Ultimately, the meaning of life is a personal and subjective question that varies from person to person.&#39;
</pre></div>
</div>
</div>
</div>
<p>Alternatively, we could have pulled our model directly from Hugging Face Hub:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2-0.5B-Instruct-GGUF&quot;</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="llamafile">
<h4><a class="toc-backref" href="#id177" role="doc-backlink"><span class="section-number">8.3.1.2. </span>Llamafile</a><a class="headerlink" href="#llamafile" title="Permalink to this heading">¶</a></h4>
<p>Developed by Occupy Wall Street’s former activist, Justine Tunney, Llamafile <span id="id10">[<a class="reference internal" href="#id78" title="Mozilla Ocho. Llamafile: distribute and run llms with a single file. GitHub Repository, 2024. Tool for packaging and distributing LLMs as self-contained executables. URL: https://github.com/Mozilla-Ocho/llamafile.">Mozilla Ocho, 2024</a>]</span> is an Appache 2.0 licensed open source tool that combines the power of LLama.cpp with <strong>Cosmopolitan Libc</strong>, a universal C standard library that allows creating portable executables compatible with multiple operating systems.</p>
<p>In this way, Llamafile reduces all the complexity of LLMs to a single executable file (called a “llamafile”) that runs locally without installation. Key advantages of Llamafile over plain Llama.cpp include:</p>
<ol class="arabic simple">
<li><p><strong>Zero Installation/Configuration</strong></p></li>
</ol>
<ul class="simple">
<li><p>Llamafile: Single executable file that works immediately</p></li>
<li><p>Llama.cpp: Requires compilation, dependency management, and proper setup of your development environment</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Cross-Platform Portability</strong></p></li>
</ol>
<ul class="simple">
<li><p>Llamafile: One binary works across Windows, macOS, and Linux without modification</p></li>
<li><p>Llama.cpp: Needs to be compiled separately for each operating system, managing platform-specific dependencies</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Distribution Simplicity</strong></p></li>
</ol>
<ul class="simple">
<li><p>Llamafile: Share a single file that just works</p></li>
<li><p>Llama.cpp: Need to distribute source code or platform-specific binaries along with setup instructions</p></li>
</ul>
<p>Besides simplifying the use of LLMs, Llamafile delivers <strong>durability</strong> as model weights remain usable and reproducible over time, even as new formats and models are developed. In summary, Llamafile trades some optimization potential from LLama.cpp for improved ease of use and portability.</p>
<p>A large collection of Llamafiles can be found on HuggingFace <span id="id11">[<a class="reference internal" href="#id79" title="Hugging Face. Llamafile models on hugging face. Online Repository, 2024x. Collection of models compatible with Mozilla's llamafile format. URL: https://huggingface.co/models?library=llamafile.">Hugging Face, 2024x</a>]</span>. All you need to do is:</p>
<ol class="arabic simple">
<li><p>Download a llamafile from HuggingFace</p></li>
<li><p>Make the file executable</p></li>
<li><p>Run the file</p></li>
</ol>
<p>Here’s a simple bash script that shows all 3 setup steps for running TinyLlama-1.1B locally:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download a llamafile from HuggingFace</span>
wget<span class="w"> </span>https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile

<span class="c1"># Make the file executable. On Windows, instead just rename the file to end in &quot;.exe&quot;.</span>
chmod<span class="w"> </span>+x<span class="w"> </span>TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile

<span class="c1"># Start the model server. Listens at http://localhost:8080 by default.</span>
./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile<span class="w"> </span>--server<span class="w"> </span>--nobrowser
</pre></div>
</div>
<p>As a result, a model server is running on <a class="reference external" href="http://localhost:8080">http://localhost:8080</a>. And we can use it as demonstrated in the previous section.</p>
</section>
<section id="ollama">
<h4><a class="toc-backref" href="#id178" role="doc-backlink"><span class="section-number">8.3.1.3. </span>Ollama</a><a class="headerlink" href="#ollama" title="Permalink to this heading">¶</a></h4>
<p>Ollama is a lightweight, MIT-licensed open-source tool for running LLMs locally. It provides a simple interface for interacting with a wide range of language models, including popular models like Llama 3.1 and Llama 3.2. Ollama is designed to be easy to install and use, making it a popular choice for developers who want to run LLMs locally without the need for extensive setup or configuration. Ollama’s key advantages include:</p>
<ol class="arabic simple">
<li><p><strong>Model Management</strong></p></li>
</ol>
<ul class="simple">
<li><p>Built-in model registry and easy downloading of popular models</p></li>
<li><p>Simple commands to list, remove, and switch between models</p></li>
<li><p>Handles model updates and versions automatically</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>API First Design</strong></p></li>
</ol>
<ul class="simple">
<li><p>Provides a REST API out of the box</p></li>
<li><p>Easy integration with applications and services</p></li>
<li><p>Built-in support for different programming languages</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Container Support</strong></p></li>
</ol>
<ul class="simple">
<li><p>Native Docker integration</p></li>
<li><p>Easy deployment in containerized environments</p></li>
<li><p>Better resource isolation and management</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>User Experience</strong></p></li>
</ol>
<ul class="simple">
<li><p>More “app-like” experience with system tray integration</p></li>
<li><p>Simple CLI commands that feel familiar to developers</p></li>
<li><p>No need to deal with file permissions or executables</p></li>
</ul>
<p>Despite its advantages, Ollama comes with some trade-offs: it provides less low-level control compared to Llama.cpp, requires proper platform-specific installation unlike the portable Llamafile, and introduces additional resource overhead from running services that aren’t present in bare Llama.cpp implementations.</p>
<p><strong>Setup</strong></p>
<p>First, install Ollama on your machine. You can do this through the terminal with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">sSfL</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">ollama</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">download</span> <span class="o">|</span> <span class="n">sh</span>
</pre></div>
</div>
<p>Or download the installer directly from <a class="reference external" href="https://ollama.com">https://ollama.com</a></p>
<p><strong>Inference</strong></p>
<p>After installation, you can download a pre-trained model. For example, to download the <code class="docutils literal notranslate"><span class="pre">qwen2:0.5b</span></code> model, run in terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>run<span class="w"> </span>qwen2:0.5b
</pre></div>
</div>
<p>To see more details about the model, just run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>show<span class="w"> </span>qwen2:0.5b
</pre></div>
</div>
<p>To stop the model server, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>stop<span class="w"> </span>qwen2:0.5b
</pre></div>
</div>
<p>To see all models you’ve downloaded:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>list
</pre></div>
</div>
<p><strong>Server</strong></p>
<p>As in Llama.cpp and Llamafile, Ollama can be run as a server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>serve
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>run<span class="w"> </span>qwen2:0.5b
</pre></div>
</div>
<p>And then we can send requests to the server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:11434/api/chat<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;model&quot;: &quot;qwen2:0.5b&quot;,</span>
<span class="s1">  &quot;messages&quot;: [</span>
<span class="s1">    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the meaning of life?&quot; }</span>
<span class="s1">  ]</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p><strong>Python</strong></p>
<p>A Python binding is also available for Ollama.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ollama
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ollama</span> <span class="kn">import</span> <span class="n">chat</span>
<span class="kn">from</span> <span class="nn">ollama</span> <span class="kn">import</span> <span class="n">ChatResponse</span>

<span class="n">response</span><span class="p">:</span> <span class="n">ChatResponse</span> <span class="o">=</span> <span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;qwen2:0.5b&#39;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
  <span class="p">{</span>
    <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span>
    <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;What is the meaning of life?&#39;</span><span class="p">,</span>
  <span class="p">},</span>
<span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="comparison">
<h4><a class="toc-backref" href="#id179" role="doc-backlink"><span class="section-number">8.3.1.4. </span>Comparison</a><a class="headerlink" href="#comparison" title="Permalink to this heading">¶</a></h4>
<p>Each solution offers distinct advantages and tradeoffs that make them suitable for different use cases. At a high-level, Ollama is the easiest to install and use and has become the most popular choice for your average use case, Llamafile is the easiest to distribute and a good choice when portability is a priority, and Llama.cpp is the most customizable and performant solution as summarized in <a class="reference internal" href="#feature-comparison-local"><span class="std std-numref">Table 8.1</span></a>.</p>
<table class="docutils align-center" id="feature-comparison-local">
<caption><span class="caption-number">Table 8.1 </span><span class="caption-text">lama.cpp vs Ollama vs Llamafile Comparison</span><a class="headerlink" href="#feature-comparison-local" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Ollama</p></th>
<th class="head"><p>Llamafile</p></th>
<th class="head"><p>Llama.cpp</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Installation</strong></p></td>
<td><p>Package manager</p></td>
<td><p>No installation needed</p></td>
<td><p>Compilation / Package manager</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Model Management</strong></p></td>
<td><p>Built-in registry</p></td>
<td><p>Manual download</p></td>
<td><p>Manual download</p></td>
</tr>
<tr class="row-even"><td><p><strong>Containerization</strong></p></td>
<td><p>Native support</p></td>
<td><p>Possible with configuration</p></td>
<td><p>Possible with configuration</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Portability</strong></p></td>
<td><p>Per-platform install</p></td>
<td><p>Single executable</p></td>
<td><p>Needs compilation</p></td>
</tr>
</tbody>
</table>
<p>Choose Ollama if you:</p>
<ul class="simple">
<li><p>Want a user-friendly way to experiment with different models</p></li>
<li><p>Need API integration capabilities</p></li>
<li><p>Plan to use Docker in your workflow</p></li>
<li><p>Prefer a managed approach to model handling</p></li>
</ul>
<p>Choose Llamafile if you:</p>
<ul class="simple">
<li><p>Need maximum portability</p></li>
<li><p>Want zero installation</p></li>
<li><p>Prefer a self-contained solution</p></li>
</ul>
<p>Choose Llama.cpp if you:</p>
<ul class="simple">
<li><p>Need maximum performance</p></li>
<li><p>Want low-level control</p></li>
<li><p>Are building a custom solution</p></li>
</ul>
</section>
</section>
<section id="ui">
<h3><a class="toc-backref" href="#id180" role="doc-backlink"><span class="section-number">8.3.2. </span>UI</a><a class="headerlink" href="#ui" title="Permalink to this heading">¶</a></h3>
<p>There is a growing number of UI tools for local LLM deployment that aim at providing a more user-friendly experience. Ranging from closed-source to open-source solutions across a range of features and capabilities. We will discuss LM Studio, Jan, and OpenWebUI.</p>
<section id="lm-studio">
<h4><a class="toc-backref" href="#id181" role="doc-backlink"><span class="section-number">8.3.2.1. </span>LM Studio</a><a class="headerlink" href="#lm-studio" title="Permalink to this heading">¶</a></h4>
<p>LM Studio <span id="id12">[<a class="reference internal" href="#id77" title="LM Studio. Lm studio - discover, download, and run local llms. Website, 2024. Desktop application for discovering, downloading and running local language models. URL: https://lmstudio.ai/.">LM Studio, 2024</a>]</span> is a closed-source GUI for running LLMs locally. In the context of local deployment, LM Studio positions itself as a more user-friendly, feature-rich solution compared to the other tools. It’s particularly valuable for developers transitioning from cloud APIs to local deployment, and for users who prefer graphical interfaces over command-line tools. Key Features of LM Studio include:</p>
<ul class="simple">
<li><p><strong>Model Parameter Customization</strong>: Allows adjusting temperature, maximum tokens, frequency penalty, and other settings</p></li>
<li><p><strong>Chat History</strong>: Enables saving prompts for later use</p></li>
<li><p><strong>Cross-platform</strong>: Available on Linux, Mac, and Windows</p></li>
<li><p><strong>AI Chat and Playground</strong>: Chat with LLMs and experiment with multiple models loaded simultaneously</p></li>
</ul>
<p><a class="reference internal" href="#lmstudio"><span class="std std-numref">Fig. 8.2</span></a> and <a class="reference internal" href="#lmstudio-server"><span class="std std-numref">Fig. 8.3</span></a> show LM Studio’s chat interface and server, respectively.</p>
<figure class="align-center" id="lmstudio">
<a class="reference internal image-reference" href="../_images/lmstudio.png"><img alt="LM Studio" src="../_images/lmstudio.png" style="width: 858.6px; height: 581.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">LM Studio Chat Interface.</span><a class="headerlink" href="#lmstudio" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="lmstudio-server">
<a class="reference internal image-reference" href="../_images/lmstudio_server.png"><img alt="LM Studio Server" src="../_images/lmstudio_server.png" style="width: 945.0px; height: 585.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">LM Studio Server.</span><a class="headerlink" href="#lmstudio-server" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>One important feature of LM Studio is that it provides machine specification verification capabilities, checking computer specifications like GPU and memory to report compatible models therefore helping users choose the right model. It also includes a local inference server for developers that allows setting up a local HTTP server similar to OpenAI’s API. Importantly, LM Studio’s OpenAI API compatibility is a particularly strong feature for developers looking to move their applications from cloud to local deployment with minimal code changes.</p>
</section>
<section id="jan">
<h4><a class="toc-backref" href="#id182" role="doc-backlink"><span class="section-number">8.3.2.2. </span>Jan</a><a class="headerlink" href="#jan" title="Permalink to this heading">¶</a></h4>
<p>Jan is an open source ChatGPT-alternative that runs local models. Its model’s library contains popular LLMs like Llama, Gemma, Mistral, or Qwen. Key Features of Jan include:</p>
<ol class="arabic simple">
<li><p><strong>User-Friendly Interface</strong>: Run AI models with just a few clicks</p></li>
<li><p><strong>Accessibility</strong>: Intuitive platform for both beginners and experts</p></li>
<li><p><strong>Local Server</strong>: Local API Server with OpenAI-equivalent API</p></li>
<li><p><strong>Model Hub Integration</strong>: Easy access to various models with ease of import from LM Studio</p></li>
<li><p><strong>Cross-Platform Support</strong>: Works across different operating systems</p></li>
</ol>
<p>Jan has a default C++ inference server built on top of llama.cpp and provides an OpenAI-compatible API. Jan natively supports GGUF (through a llama.cpp engine) and TensorRT (through a TRT-LLM engine). HuggingFace models can be downloaded directly using the model’s ID or URL. User can optionally use cloud-based models (e.g. GPT, Claude models). <a class="reference internal" href="#id13"><span class="std std-numref">Fig. 8.4</span></a> shows Jan’s chat interface.</p>
<figure class="align-center" id="id13">
<a class="reference internal image-reference" href="../_images/jan.png"><img alt="Jan" src="../_images/jan.png" style="width: 665.0px; height: 466.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Jan Chat Interface.</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="open-webui">
<h4><a class="toc-backref" href="#id183" role="doc-backlink"><span class="section-number">8.3.2.3. </span>Open WebUI</a><a class="headerlink" href="#open-webui" title="Permalink to this heading">¶</a></h4>
<p>Open WebUI is an open-source web interface designed to enhance the local AI model experience, particularly for Ollama and OpenAI-compatible APIs. It aims to provide enterprise-grade features while maintaining user-friendliness. OpenWebUI’s core features include:</p>
<ol class="arabic simple">
<li><p><strong>Advanced User Interface</strong></p>
<ul class="simple">
<li><p>Full markdown and LaTeX support</p></li>
<li><p>Voice and video call capabilities</p></li>
<li><p>Mobile-friendly with PWA support</p></li>
<li><p>Multi-model chat interface</p></li>
</ul>
</li>
<li><p><strong>Enterprise Features</strong></p>
<ul class="simple">
<li><p>Role-based access control</p></li>
<li><p>User groups and permissions</p></li>
<li><p>Usage monitoring</p></li>
<li><p>Team collaboration tools</p></li>
</ul>
</li>
<li><p><strong>Advanced Capabilities</strong></p>
<ul class="simple">
<li><p>Local RAG (Retrieval Augmented Generation)</p></li>
<li><p>Web search integration</p></li>
<li><p>Image generation support</p></li>
<li><p>Python function calling</p></li>
<li><p>Document library</p></li>
<li><p>Custom model building</p></li>
</ul>
</li>
</ol>
<p><a class="reference internal" href="#openwebui"><span class="std std-numref">Fig. 8.5</span></a> shows Open WebUI’s chat interface.</p>
<figure class="align-center" id="openwebui">
<a class="reference internal image-reference" href="../_images/openwebui.png"><img alt="Open WebUI" src="../_images/openwebui.png" style="width: 818.75px; height: 456.25px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">Open WebUI Chat Interface.</span><a class="headerlink" href="#openwebui" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>While Open WebUI offers advanced capabilities including RAG and multi-model support, these features require more system resources than simpler alternatives. Open WebUI is likely to be adopted by enterprise users who require advanced features and a more user-friendly interface.</p>
</section>
<section id="id14">
<h4><a class="toc-backref" href="#id184" role="doc-backlink"><span class="section-number">8.3.2.4. </span>Comparison</a><a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h4>
<p>LM Studio excels at providing individual developers with a smooth transition from cloud APIs to local deployment, offering an intuitive interface and robust API compatibility, however it is closed-source. Jan focuses on simplicity and accessibility, making it ideal for personal use and basic deployments while maintaining open-source benefits. OpenWebUI makes additional features available to enterprise users and teams requiring advanced features like RAG, collaboration tools, and granular access controls, though this may come at the cost of increased complexity and resource requirements. We compare the three tools in <a class="reference internal" href="#feature-comparison-ui"><span class="std std-numref">Table 8.2</span></a>.</p>
<table class="docutils align-center" id="feature-comparison-ui">
<caption><span class="caption-number">Table 8.2 </span><span class="caption-text">LM Studio vs Jan vs OpenWebUI Comparison</span><a class="headerlink" href="#feature-comparison-ui" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Feature Category</p></th>
<th class="head"><p>LM Studio</p></th>
<th class="head"><p>Jan</p></th>
<th class="head"><p>OpenWebUI</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Licensing</strong></p></td>
<td><p>Closed Source</p></td>
<td><p>Open Source</p></td>
<td><p>Open Source</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Setup Complexity</strong></p></td>
<td><p>Medium</p></td>
<td><p>Easy</p></td>
<td><p>Complex</p></td>
</tr>
<tr class="row-even"><td><p><strong>Resource Usage</strong></p></td>
<td><p>High</p></td>
<td><p>Medium</p></td>
<td><p>High</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Target Users</strong></p></td>
<td><p>Individual/Developers</p></td>
<td><p>Individuals</p></td>
<td><p>Enterprise/Teams</p></td>
</tr>
<tr class="row-even"><td><p><strong>UI Features</strong></p></td>
<td><p>- Full GUI<br>- Parameter tuning<br>- Chat history<br>- Model playground</p></td>
<td><p>- Simple GUI<br>- Basic parameter tuning<br>- Chat interface<br>- Model import</p></td>
<td><p>- Advanced GUI<br>- Full markdown/LaTeX<br>- Voice/video calls<br>- PWA support</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Model Support</strong></p></td>
<td><p>- Multiple models<br>- Hardware verification<br>- Model compatibility check</p></td>
<td><p>- Multiple models<br>- Import from GPT4All/LM Studio<br>- Basic model management</p></td>
<td><p>- Multi-model chat<br>- Model builder<br>- Custom agents</p></td>
</tr>
<tr class="row-even"><td><p><strong>API Features</strong></p></td>
<td><p>- OpenAI compatible<br>- Local inference server<br>- API documentation</p></td>
<td><p>- Basic OpenAI compatible<br>- Local API server</p></td>
<td><p>- Multiple API support<br>- Python function calling<br>- Advanced integrations</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Enterprise Features</strong></p></td>
<td><p>Limited</p></td>
<td><p>None</p></td>
<td><p>- RBAC<br>- Team collaboration<br>- Usage monitoring</p></td>
</tr>
<tr class="row-even"><td><p><strong>Advanced Features</strong></p></td>
<td><p>- Parameter visualization<br>- Performance metrics</p></td>
<td><p>- Basic chat<br>- Simple model switching</p></td>
<td><p>- RAG support<br>- Web search<br>- Document library<br>- Image generation</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Best For</strong></p></td>
<td><p>- Individual developers<br>- API transition<br>- Local development</p></td>
<td><p>- Personal use<br>- Simple deployment<br>- Basic chat needs</p></td>
<td><p>- Enterprise use<br>- Team collaboration<br>- Advanced AI applications</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="case-study-the-effect-of-quantization-on-llm-performance">
<h2><a class="toc-backref" href="#id185" role="doc-backlink"><span class="section-number">8.4. </span>Case Study: The Effect of Quantization on LLM Performance</a><a class="headerlink" href="#case-study-the-effect-of-quantization-on-llm-performance" title="Permalink to this heading">¶</a></h2>
<p>This case study examines how different quantization levels affect the performance of language models running locally. Quantization is a crucial technique for reducing model size and improving inference speed, but it comes with potential tradeoffs in model quality. Understanding these tradeoffs is essential for practitioners deploying LLMs in resource-constrained environments.</p>
<p>Using the Qwen 2.5 0.5B model as our baseline, we’ll compare four variants:</p>
<ul class="simple">
<li><p>The base fp16 model (no quantization)</p></li>
<li><p>Q2_K quantization (highest compression, lowest precision)</p></li>
<li><p>Q4_K quantization (balanced compression/precision)</p></li>
<li><p>Q6_K quantization (lowest compression, highest precision)</p></li>
</ul>
<p>The analysis will focus on three key metrics:</p>
<ol class="arabic simple">
<li><p>Perplexity - to measure how well the model predicts text</p></li>
<li><p>KL divergence - to quantify differences in probability distributions against base model</p></li>
<li><p>Prompt (tokens/second) - to assess impact in thoughput</p></li>
</ol>
<p>While we will focus on the Qwen 2.5 0.5B model, the same analysis can be applied to other models. These insights will help practitioners make informed decisions about quantization strategies based on their specific requirements for model size, speed, and accuracy.</p>
<section id="prompts-dataset">
<h3><a class="toc-backref" href="#id186" role="doc-backlink"><span class="section-number">8.4.1. </span>Prompts Dataset</a><a class="headerlink" href="#prompts-dataset" title="Permalink to this heading">¶</a></h3>
<p>To evaluate the impact of quantization on model performance, we first need a set of prompts that will serve as input data for our experiments. We’ll construct a dataset from WikiText-2 <span id="id15">[<a class="reference internal" href="#id82" title="Salesforce. Wikitext dataset. Hugging Face Dataset, 2024. Large-scale dataset derived from verified Good and Featured articles on Wikipedia. URL: https://huggingface.co/datasets/Salesforce/wikitext.">Salesforce, 2024</a>]</span>, which contains Wikipedia excerpts.</p>
<p>In our experiments, we will use a total of <code class="docutils literal notranslate"><span class="pre">NUM_PROMPTS</span></code> prompts that vary in length from <code class="docutils literal notranslate"><span class="pre">MIN_PROMPT_LENGTH</span></code> to <code class="docutils literal notranslate"><span class="pre">MAX_PROMPT_LENGTH</span></code> tokens. Using a fixed set of prompts ensures consistent evaluation across model variants and enables direct comparison of metrics like perplexity and throughput.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NUM_PROMPTS</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">MIN_PROMPT_LENGTH</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">MAX_PROMPT_LENGTH</span> <span class="o">=</span> <span class="mi">1000</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datasets</span>
<span class="n">input_texts_raw</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Salesforce/wikitext&quot;</span><span class="p">,</span> <span class="s2">&quot;wikitext-2-raw-v1&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">input_texts_raw</span> <span class="k">if</span> <span class="n">s</span><span class="o">!=</span><span class="s1">&#39;&#39;</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">MIN_PROMPT_LENGTH</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">MAX_PROMPT_LENGTH</span><span class="p">][:</span><span class="n">NUM_PROMPTS</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">input_texts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">input_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game &#39;s opening theme was sung by May &#39;n . 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/local/prompts.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">input_texts</span><span class="p">:</span>
        <span class="c1"># Escape any quotes in the text and wrap in quotes</span>
        <span class="n">escaped_text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&quot;</span><span class="si">{</span><span class="n">escaped_text</span><span class="si">}</span><span class="s1">&quot;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="quantization">
<h3><a class="toc-backref" href="#id187" role="doc-backlink"><span class="section-number">8.4.2. </span>Quantization</a><a class="headerlink" href="#quantization" title="Permalink to this heading">¶</a></h3>
<p>We can quantize a model using the <code class="docutils literal notranslate"><span class="pre">llama-quantize</span></code> CLI. For instance, to quantize the Qwen 2.5 0.5B model to Q4_K, we can run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./llama-quantize<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-fp16.gguf<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q8_0.gguf<span class="w"> </span>Q4_K
</pre></div>
</div>
<p><a class="reference internal" href="#quantization-levels"><span class="std std-numref">Table 8.3</span></a> describes the key quantization levels used in this study <span id="id16">[<a class="reference internal" href="#id83" title="Hugging Face. Gguf quantization types. Online Documentation, 2024w. Documentation on different quantization types available for GGUF models. URL: https://huggingface.co/docs/hub/gguf#quantization-types.">Hugging Face, 2024w</a>]</span>, where:</p>
<ul class="simple">
<li><p>q is the quantized value</p></li>
<li><p>block_scale is the scaling factor for the block (with bit width in parentheses)</p></li>
<li><p>block_min is the block minimum value (with bit width in parentheses)</p></li>
</ul>
<table class="docutils align-center" id="quantization-levels">
<caption><span class="caption-number">Table 8.3 </span><span class="caption-text">Quantization Levels</span><a class="headerlink" href="#quantization-levels" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Quantization</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Bits per Weight</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Q2_K</p></td>
<td><p>2-bit quantization with 16 weights per block in 16-block superblocks</p></td>
<td><p>2.5625</p></td>
<td><p>w = q * block_scale(4-bit) + block_min(4-bit)</p></td>
</tr>
<tr class="row-odd"><td><p>Q4_K</p></td>
<td><p>4-bit quantization with 32 weights per block in 8-block superblocks</p></td>
<td><p>4.5</p></td>
<td><p>w = q * block_scale(6-bit) + block_min(6-bit)</p></td>
</tr>
<tr class="row-even"><td><p>Q6_K</p></td>
<td><p>6-bit quantization with 16 weights per block in 16-block superblocks</p></td>
<td><p>6.5625</p></td>
<td><p>w = q * block_scale(8-bit)</p></td>
</tr>
</tbody>
</table>
<p>Each quantization level represents a different tradeoff between model size and accuracy. Q2_K provides the highest compression but potentially lower accuracy, while Q6_K maintains better accuracy at the cost of larger model size. The K-variants use more sophisticated block structures and scaling compared to legacy quantization methods.</p>
<p>The base model is 16-bit standard IEEE 754 half-precision floating-point number.</p>
</section>
<section id="benchmarking">
<h3><a class="toc-backref" href="#id188" role="doc-backlink"><span class="section-number">8.4.3. </span>Benchmarking</a><a class="headerlink" href="#benchmarking" title="Permalink to this heading">¶</a></h3>
<p>We will measure quantized model “quality” by means of perplexity and KL Divergence. For performance evaluation, we will report prompt throughput in tokens per second.</p>
<p><strong>Perplexity</strong></p>
<p>Perplexity is a common metric for evaluating language models that measures how well a model predicts a sample of text. Lower perplexity indicates better prediction (less “perplexed” by the text).</p>
<p>Recall that for a sequence of N tokens, perplexity is defined as:</p>
<div class="math notranslate nohighlight">
\[ \text{PPL(B, X)} = \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log_2 P(x_i|x_{&lt;i})\right) \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x_i|x_{&lt;i})\)</span> is the probability the model <span class="math notranslate nohighlight">\(B\)</span> with tokenized sequence <span class="math notranslate nohighlight">\(X\)</span> assigns to token <span class="math notranslate nohighlight">\(x_i\)</span> given the previous tokens <span class="math notranslate nohighlight">\(x_{&lt;i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the total number of tokens in the sequence</p></li>
</ul>
<p>To evaluate quantization quality, we first calculate perplexity scores for both the base model and quantized variants. We then compute the ratio of quantized to base perplexity and average it across all prompt samples as follows:</p>
<div class="math notranslate nohighlight">
\[ Avg PPL Ratio = \frac{1}{N}\sum_{i=1}^{N} \frac{\text{PPL}_i(Q)}{\text{PPL}_i(\text{base})} \]</div>
<p>We also calculate the correlation between the log perplexities of the quantized and base models:</p>
<div class="math notranslate nohighlight">
\[ \text{Corr}(\ln(\text{PPL}(Q)), \ln(\text{PPL}(\text{base}))) \]</div>
<p>These are two simple metrics to evaluate how much worse the quantized model performs on an intrinsic basis which we then can compare to the base model’s perplexities.</p>
<p>Arguably, KL Divergence is a better metric since we aim at reporting relative performance instead of intrinsic performance.</p>
<p><strong>KL Divergence</strong></p>
<p>The Kullback-Leibler (KL) Divergence measures how one probability distribution differs from another reference distribution. For comparing logits between a base model (B) and quantized model (Q), we can calculate the KL divergence as follows:</p>
<div class="math notranslate nohighlight">
\[ D_{KL}(P||Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)} \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(i)\)</span> and <span class="math notranslate nohighlight">\(Q(i)\)</span> are the softmax probabilities derived from the logits</p></li>
<li><p>The sum is taken over all tokens in the vocabulary</p></li>
</ul>
<p><strong>Implementation</strong></p>
<p>We will use LLama.cpp’s <code class="docutils literal notranslate"><span class="pre">llama-perplexity</span></code> CLI to calculate perplexity and KL divergence. The first step is to generate the logits for the base model, which will serve as the reference distribution. For instance, below we pass our input prompts (<code class="docutils literal notranslate"><span class="pre">prompts.txt</span></code>) and generate the logits for the base model <code class="docutils literal notranslate"><span class="pre">qwen2.5-0.5b-instruct-fp16.gguf</span></code> which will be saved in <code class="docutils literal notranslate"><span class="pre">logits.kld</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-perplexity<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-fp16.gguf<span class="w"> </span>--kl-divergence-base<span class="w"> </span>../logits.kld<span class="w"> </span>-f<span class="w"> </span>../prompts.txt
</pre></div>
</div>
<p>Next, we generate KL-Divergence and perplexity stats for quantized model <code class="docutils literal notranslate"><span class="pre">qwen2.5-0.5b-instruct-q2_k.gguf</span></code> against base model logits <code class="docutils literal notranslate"><span class="pre">logits.kld</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-perplexity<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q2_k.gguf<span class="w"> </span>-f<span class="w"> </span>../prompts.txt<span class="w"> </span>--kl-divergence-base<span class="w"> </span>../logits.kld<span class="w"> </span>--kl-divergence<span class="w"> </span><span class="p">&amp;</span>&gt;<span class="w"> </span>../q2_kresults.txt
</pre></div>
</div>
<p>We perform this process for each quantization level studied (Q2_K, Q4_K, Q6_K).</p>
</section>
<section id="results">
<h3><a class="toc-backref" href="#id189" role="doc-backlink"><span class="section-number">8.4.4. </span>Results</a><a class="headerlink" href="#results" title="Permalink to this heading">¶</a></h3>
<p>The KL divergence and perplexity results in <a class="reference internal" href="#ppl1"><span class="std std-numref">Fig. 8.7</span></a> and <a class="reference internal" href="#ppl2"><span class="std std-numref">Fig. 8.6</span></a> provide insights into model quality across different quantization levels. Q6 maintains near-perfect correlation (99.90%) with the base model and minimal KL divergence (0.004), indicating very close distribution matching. Q2’s higher KL divergence (0.112) and lower correlation (98.31%) quantify its increased deviation from the base model’s behavior.</p>
<figure class="align-center" id="ppl2">
<a class="reference internal image-reference" href="../_images/ppl2.png"><img alt="Perplexity" src="../_images/ppl2.png" style="width: 897.5px; height: 474.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.6 </span><span class="caption-text">KL Divergence results for Quantization Q2, Q4, and Q6 quantized models.</span><a class="headerlink" href="#ppl2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="ppl1">
<a class="reference internal image-reference" href="../_images/ppl1.png"><img alt="Perplexity" src="../_images/ppl1.png" style="width: 451.0px; height: 455.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.7 </span><span class="caption-text">Perplexity results for Quantization Q2, Q4, and Q6 quantized models.</span><a class="headerlink" href="#ppl1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>From <a class="reference internal" href="#quantization-benchmarks"><span class="std std-numref">Table 8.4</span></a>, we observe that the Q2 model achieves the smallest size at 390 MiB
(67% reduction from base) with throughput of 81 tokens/s, but has the highest perplexity degradation at 10.36%. The Q4 model offers a better balance, with good size savings (60% reduction) and only 3.5% perplexity loss. Q6 comes closest to matching the base model’s performance with just 0.93% perplexity degradation, while still providing 47% size reduction.</p>
<table class="docutils align-center" id="quantization-benchmarks">
<caption><span class="caption-number">Table 8.4 </span><span class="caption-text">Quantization Benchmarks</span><a class="headerlink" href="#quantization-benchmarks" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Size (MiB)</p></th>
<th class="head"><p>Throughput (tokens/s)</p></th>
<th class="head"><p>PPL Ratio - 1 (%)</p></th>
<th class="head"><p>Correlation (%)</p></th>
<th class="head"><p>KL Divergence (Mean)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Q2</strong></p></td>
<td><p>390.28</p></td>
<td><p>81.32</p></td>
<td><p>10.36 ± 0.78</p></td>
<td><p>98.31</p></td>
<td><p>0.112 ± 0.002</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Q4</strong></p></td>
<td><p>462.96</p></td>
<td><p>77.08</p></td>
<td><p>3.50 ± 0.40</p></td>
<td><p>99.50</p></td>
<td><p>0.030 ± 0.001</p></td>
</tr>
<tr class="row-even"><td><p><strong>Q6</strong></p></td>
<td><p>614.58</p></td>
<td><p>87.55</p></td>
<td><p>0.93 ± 0.18</p></td>
<td><p>99.90</p></td>
<td><p>0.004 ± 0.000</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Base</strong></p></td>
<td><p>1,170.00</p></td>
<td><p>94.39</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
</tbody>
</table>
<p>Benchmarking was performed on Ubuntu 24.04 LTS for x86_64-linux-gnu on commodity hardware (<a class="reference internal" href="#benchmarking-hardware"><span class="std std-numref">Table 8.5</span></a>) with no dedicated GPU demonstrating the feasibility of running LLMs locally by nearly everyone with a personal computer thanks to LLama.cpp.</p>
<table class="docutils align-center" id="benchmarking-hardware">
<caption><span class="caption-number">Table 8.5 </span><span class="caption-text">Benchmarking Hardware</span><a class="headerlink" href="#benchmarking-hardware" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Device</p></th>
<th class="head"><p>Class</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>processor</p></td>
<td><p>Intel(R) Core(TM) i7-8550U CPU &#64; 1</p></td>
<td><p>Intel(R) Core(TM) i7-8550U CPU &#64; 1</p></td>
</tr>
<tr class="row-odd"><td><p>memory</p></td>
<td><p>15GiB System memory</p></td>
<td><p>15GiB System memory</p></td>
</tr>
<tr class="row-even"><td><p>storage</p></td>
<td><p>Samsung SSD 970 EVO Plus 500GB</p></td>
<td><p>Samsung SSD 970 EVO Plus 500GB</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id190" role="doc-backlink"><span class="section-number">8.5. </span>Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
</section>
<section id="citation">
<h2><a class="toc-backref" href="#id191" role="doc-backlink"><span class="section-number">8.6. </span>Citation</a><a class="headerlink" href="#citation" title="Permalink to this heading">¶</a></h2>
<p><a class="reference external" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="CC BY-NC-SA 4.0" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" /></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@misc</span><span class="p">{</span><span class="n">tharsistpsouza2024tamingllms</span><span class="p">,</span>
  <span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">Tharsis</span> <span class="n">T</span><span class="o">.</span> <span class="n">P</span><span class="o">.</span> <span class="n">Souza</span><span class="p">},</span>
  <span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">Taming</span> <span class="n">LLMs</span><span class="p">:</span> <span class="n">A</span> <span class="n">Practical</span> <span class="n">Guide</span> <span class="n">to</span> <span class="n">LLM</span> <span class="n">Pitfalls</span> <span class="k">with</span> <span class="n">Open</span> <span class="n">Source</span> <span class="n">Software</span><span class="p">},</span>
  <span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2024</span><span class="p">},</span>
  <span class="n">chapter</span> <span class="o">=</span> <span class="p">{</span><span class="n">Breaking</span> <span class="n">Free</span> <span class="kn">from</span> <span class="nn">Cloud</span> <span class="n">Providers</span><span class="p">},</span>
  <span class="n">journal</span> <span class="o">=</span> <span class="p">{</span><span class="n">GitHub</span> <span class="n">repository</span><span class="p">},</span>
  <span class="n">url</span> <span class="o">=</span> <span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">souzatharsis</span><span class="o">/</span><span class="n">tamingLLMs</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id192" role="doc-backlink"><span class="section-number">8.7. </span>References</a><a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id17">
<div class="citation" id="id81" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Bc24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id7">1</a>,<a role="doc-backlink" href="#id9">2</a>)</span>
<p>Andrei Betlen and contributors. Llama-cpp-python. GitHub Repository, 2024. Python bindings for llama.cpp library enabling high-performance inference of LLaMA models. URL: <a class="reference external" href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a>.</p>
</div>
<div class="citation" id="id84" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">Gc24</a><span class="fn-bracket">]</span></span>
<p>Georgi Gerganov and contributors. Llama.cpp grammars documentation. GitHub Repository, 2024. Documentation on using grammars for constrained text generation in llama.cpp. URL: <a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md">https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md</a>.</p>
</div>
<div class="citation" id="id86" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gc4a<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id6">2</a>)</span>
<p>Georgi Gerganov and contributors. Llama.cpp. GitHub Repository, 2024a. High-performance inference of LLaMA models in pure C/C++. URL: <a class="reference external" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>.</p>
</div>
<div class="citation" id="id89" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">Gc4b</a><span class="fn-bracket">]</span></span>
<p>Georgi Gerganov and contributors. Gguf file format specification. GitHub Repository, 2024b. Technical specification of the GGUF file format for efficient model storage and inference. URL: <a class="reference external" href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">https://github.com/ggerganov/ggml/blob/master/docs/gguf.md</a>.</p>
</div>
<div class="citation" id="id76" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">G+23</a><span class="fn-bracket">]</span></span>
<p>Georgi Gerganov and others. Quantization of llama models - discussion. GitHub Discussion, 2023. Discussion thread about quantization techniques and tradeoffs in llama.cpp. URL: <a class="reference external" href="https://github.com/ggerganov/llama.cpp/discussions/205">https://github.com/ggerganov/llama.cpp/discussions/205</a>.</p>
</div>
<div class="citation" id="id83" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">HuggingFace4w</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Gguf quantization types. Online Documentation, 2024w. Documentation on different quantization types available for GGUF models. URL: <a class="reference external" href="https://huggingface.co/docs/hub/gguf#quantization-types">https://huggingface.co/docs/hub/gguf#quantization-types</a>.</p>
</div>
<div class="citation" id="id88" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">HuggingFace4xa</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Gguf models on hugging face. Online Repository, 2024x. Collection of models in GGUF format for efficient local inference. URL: <a class="reference external" href="https://huggingface.co/models?search=gguf">https://huggingface.co/models?search=gguf</a>.</p>
</div>
<div class="citation" id="id79" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">HuggingFace4xb</a><span class="fn-bracket">]</span></span>
<p>Hugging Face. Llamafile models on hugging face. Online Repository, 2024x. Collection of models compatible with Mozilla's llamafile format. URL: <a class="reference external" href="https://huggingface.co/models?library=llamafile">https://huggingface.co/models?library=llamafile</a>.</p>
</div>
<div class="citation" id="id87" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">IBMThink24</a><span class="fn-bracket">]</span></span>
<p>IBM Think. Gguf vs ggml: what's the difference? 2024. Comparison of GGUF and GGML model formats. URL: <a class="reference external" href="https://www.ibm.com/think/topics/gguf-versus-ggml">https://www.ibm.com/think/topics/gguf-versus-ggml</a>.</p>
</div>
<div class="citation" id="id77" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">LMStudio24</a><span class="fn-bracket">]</span></span>
<p>LM Studio. Lm studio - discover, download, and run local llms. Website, 2024. Desktop application for discovering, downloading and running local language models. URL: <a class="reference external" href="https://lmstudio.ai/">https://lmstudio.ai/</a>.</p>
</div>
<div class="citation" id="id78" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">MozillaOcho24</a><span class="fn-bracket">]</span></span>
<p>Mozilla Ocho. Llamafile: distribute and run llms with a single file. GitHub Repository, 2024. Tool for packaging and distributing LLMs as self-contained executables. URL: <a class="reference external" href="https://github.com/Mozilla-Ocho/llamafile">https://github.com/Mozilla-Ocho/llamafile</a>.</p>
</div>
<div class="citation" id="id82" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">Salesforce24</a><span class="fn-bracket">]</span></span>
<p>Salesforce. Wikitext dataset. Hugging Face Dataset, 2024. Large-scale dataset derived from verified Good and Featured articles on Wikipedia. URL: <a class="reference external" href="https://huggingface.co/datasets/Salesforce/wikitext">https://huggingface.co/datasets/Salesforce/wikitext</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="alignment.html"
       title="previous chapter">← <span class="section-number">7. </span>Preference-Based Alignment</a>
  </li>
</ul><div class="footer" role="contentinfo">
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.9.1.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>