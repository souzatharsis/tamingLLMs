<!DOCTYPE html>
<html  lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1"><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

      <title>8. Local LLMs in Practice</title>
    
          <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
          <link rel="stylesheet" href="../_static/theme.css " type="text/css" />
          <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
          <link rel="stylesheet" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-thebe.css" type="text/css" />
          <link rel="stylesheet" href="../_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" type="text/css" />
      
      <!-- sphinx script_files -->
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script src="../_static/clipboard.min.js"></script>
        <script src="../_static/copybutton.js"></script>
        <script src="../_static/scripts/sphinx-book-theme.js"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../_static/togglebutton.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script src="../_static/design-tabs.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
        <script async="async" src="../_static/sphinx-thebe.js"></script>
        <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

      
      <!-- bundled in js (rollup iife) -->
      <!-- <script src="../_static/theme-vendors.js"></script> -->
      <script src="../_static/theme.js" defer></script>
    
      <link rel="shortcut icon" href="../../_static/taming.ico"/>
  <link rel="index" title="Index" href="../genindex.html" />
  <link rel="search" title="Search" href="../search.html" />
  <link rel="next" title="9. The Falling Cost Paradox" href="cost.html" />
  <link rel="prev" title="7. Preference-Based Alignment" href="alignment.html" /> 
  </head>

  <body>
    <div id="app">
    <div class="theme-container" :class="pageClasses"><navbar @toggle-sidebar="toggleSidebar">
  <router-link to="../markdown/toc.html" class="home-link">
    
      <span class="site-name">Taming LLMs</span>
    
  </router-link>

  <div class="links">
    <navlinks class="can-hide">



  
    <div class="nav-item">
      <a href="https://tamingllm.substack.com/"
        class="nav-link external">
          Newsletter <outboundlink></outboundlink>
      </a>
    </div>
  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

    </navlinks>
  </div>
</navbar>

      
      <div class="sidebar-mask" @click="toggleSidebar(false)">
      </div>
        <sidebar @toggle-sidebar="toggleSidebar">
          
          <navlinks>
            



  
    <div class="nav-item">
      <a href="https://tamingllm.substack.com/"
        class="nav-link external">
          Newsletter <outboundlink></outboundlink>
      </a>
    </div>
  
    <div class="nav-item">
      <a href="https://github.com/souzatharsis/tamingllms"
        class="nav-link external">
          Github <outboundlink></outboundlink>
      </a>
    </div>
  

            
          </navlinks><div id="searchbox" class="searchbox" role="search">
  <div class="caption"><span class="caption-text">Quick search</span>
    <div class="searchformwrapper">
      <form class="search" action="../search.html" method="get">
        <input type="text" name="q" />
        <input type="submit" value="Search" />
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div><div class="sidebar-links" role="navigation" aria-label="main navigation">
  
    <div class="sidebar-group">
      <p class="caption">
        <span class="caption-text"><a href="../markdown/toc.html#taming-llms">taming llms</a></span>
      </p>
      <ul class="current">
        
          <li class="toctree-l1 ">
            
              <a href="../markdown/preface.html" class="reference internal ">Preface</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="../markdown/intro.html" class="reference internal ">About the Book</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="evals.html" class="reference internal ">The Evals Gap</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="structured_output.html" class="reference internal ">Structured Output</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="input.html" class="reference internal ">Managing Input Data</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="safety.html" class="reference internal ">Safety</a>
            

            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="alignment.html" class="reference internal ">Preference-Based Alignment</a>
            

            
          </li>

        
          <li class="toctree-l1 current">
            
              <a href="#" class="reference internal current">Local LLMs in Practice</a>
            

            
              <ul>
                
                  <li class="toctree-l2"><a href="#introduction" class="reference internal">Introduction</a></li>
                
                  <li class="toctree-l2"><a href="#choosing-your-model" class="reference internal">Choosing your Model</a></li>
                
                  <li class="toctree-l2"><a href="#tools-for-local-llm-deployment" class="reference internal">Tools for Local LLM Deployment</a></li>
                
                  <li class="toctree-l2"><a href="#case-study-the-effect-of-quantization-on-llm-performance" class="reference internal">Case Study: The Effect of Quantization on LLM Performance</a></li>
                
                  <li class="toctree-l2"><a href="#conclusion" class="reference internal">Conclusion</a></li>
                
                  <li class="toctree-l2"><a href="#references" class="reference internal">References</a></li>
                
              </ul>
            
          </li>

        
          <li class="toctree-l1 ">
            
              <a href="cost.html" class="reference internal ">The Falling Cost Paradox</a>
            

            
          </li>

        
      </ul>
    </div>
  
</div>
        </sidebar>

      <page>
          <div class="body-header" role="navigation" aria-label="navigation">
  
  <ul class="breadcrumbs">
    <li><a href="../markdown/toc.html">Docs</a> &raquo;</li>
    
    <li><span class="section-number">8. </span>Local LLMs in Practice</li>
  </ul>
  

  <ul class="page-nav">
  <li class="prev">
    <a href="alignment.html"
       title="previous chapter">← <span class="section-number">7. </span>Preference-Based Alignment</a>
  </li>
  <li class="next">
    <a href="cost.html"
       title="next chapter"><span class="section-number">9. </span>The Falling Cost Paradox →</a>
  </li>
</ul>
  
</div>
<hr>
          <div class="content" role="main" v-pre>
            
  <section class="tex2jax_ignore mathjax_ignore" id="local-llms-in-practice">
<span id="local"></span><h1><a class="toc-backref" href="#id323" role="doc-backlink"><span class="section-number">8. </span>Local LLMs in Practice</a><a class="headerlink" href="#local-llms-in-practice" title="Permalink to this heading">¶</a></h1>
<blockquote class="epigraph">
<div><p>Freedom is something that dies unless it’s used.</p>
<p class="attribution">—Hunter S. Thompson</p>
</div></blockquote>
<nav class="contents" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#local-llms-in-practice" id="id323">Local LLMs in Practice</a></p>
<ul>
<li><p><a class="reference internal" href="#introduction" id="id324">Introduction</a></p></li>
<li><p><a class="reference internal" href="#choosing-your-model" id="id325">Choosing your Model</a></p>
<ul>
<li><p><a class="reference internal" href="#task-suitability" id="id326">Task Suitability</a></p></li>
<li><p><a class="reference internal" href="#performance-cost" id="id327">Performance &amp; Cost</a></p></li>
<li><p><a class="reference internal" href="#licensing" id="id328">Licensing</a></p></li>
<li><p><a class="reference internal" href="#community-support" id="id329">Community Support</a></p></li>
<li><p><a class="reference internal" href="#customization" id="id330">Customization</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#tools-for-local-llm-deployment" id="id331">Tools for Local LLM Deployment</a></p>
<ul>
<li><p><a class="reference internal" href="#serving-models" id="id332">Serving Models</a></p>
<ul>
<li><p><a class="reference internal" href="#llama-cpp" id="id333">LLama.cpp</a></p></li>
<li><p><a class="reference internal" href="#llamafile" id="id334">Llamafile</a></p></li>
<li><p><a class="reference internal" href="#ollama" id="id335">Ollama</a></p></li>
<li><p><a class="reference internal" href="#comparison" id="id336">Comparison</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#ui" id="id337">UI</a></p>
<ul>
<li><p><a class="reference internal" href="#lm-studio" id="id338">LM Studio</a></p></li>
<li><p><a class="reference internal" href="#jan" id="id339">Jan</a></p></li>
<li><p><a class="reference internal" href="#open-webui" id="id340">Open WebUI</a></p></li>
<li><p><a class="reference internal" href="#id37" id="id341">Comparison</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#case-study-the-effect-of-quantization-on-llm-performance" id="id342">Case Study: The Effect of Quantization on LLM Performance</a></p>
<ul>
<li><p><a class="reference internal" href="#prompts-dataset" id="id343">Prompts Dataset</a></p></li>
<li><p><a class="reference internal" href="#quantization" id="id344">Quantization</a></p></li>
<li><p><a class="reference internal" href="#benchmarking" id="id345">Benchmarking</a></p></li>
<li><p><a class="reference internal" href="#results" id="id346">Results</a></p></li>
<li><p><a class="reference internal" href="#takeaways" id="id347">Takeaways</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#conclusion" id="id348">Conclusion</a></p></li>
<li><p><a class="reference internal" href="#references" id="id349">References</a></p></li>
</ul>
</li>
</ul>
</nav>
<section id="introduction">
<h2><a class="toc-backref" href="#id324" role="doc-backlink"><span class="section-number">8.1. </span>Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>Running Open Source LLMs locally versus depending on proprietary cloud-based models represents more than just a technical choice - it’s a fundamental re-imagining of how we interact with AI technology, putting control back in the hands of users.</p>
<p>Privacy concerns are a key driver for running LLMs locally. Individual users may want to process personal documents, photos, emails, and chat messages without sharing sensitive data with third parties. For enterprise use cases, organizations handling medical records must comply with HIPAA regulations that require data to remain on-premise. Similarly, businesses processing confidential documents and intellectual property, as well as organizations subject to GDPR and other privacy regulations, need to maintain strict control over their data processing pipeline.</p>
<p>Cost considerations are another key driver. Organizations and individual consumers can better control expenses by matching model capabilities to their specific needs rather than paying for multiple cloud API subscriptions. For organizations with high-volume applications, this customization and control over costs becomes especially valuable compared to the often prohibitive per-request pricing of cloud solutions. For consumers, running multiple open source models locally eliminates the need to maintain separate subscriptions to access different model capabilities.</p>
<p>Applications with stringent latency requirements form another important category. Real-time systems where network delays would be unacceptable, edge computing scenarios demanding quick responses, and interactive applications requiring sub-second performance all benefit from local deployment. This extends to embedded systems in IoT devices where cloud connectivity might be unreliable or impractical. Further, the emergence of Small Language Models (SLMs) has made edge deployment increasingly viable, enabling sophisticated language capabilities on resource-constrained devices like smartphones, tablets and IoT sensors.</p>
<p>Running open source models locally also enables fine-grained optimization of resource usage and model characteristics based on target use case. Organizations and researchers can perform specialized domain adaptation through model modifications, experiment with different architectures and parameters, and integrate models with proprietary systems and workflows. This flexibility is particularly valuable for developing novel applications that require direct model access and manipulation.</p>
<p>However, local deployment introduces its own set of challenges and considerations. In this Chapter, we explore the landscape of local LLM deployment focused on Open Source models and tools. When choosing a local open source model, organizations must carefully evaluate several interconnected factors, from task suitability and performance requirements to resource constraints and licensing.</p>
<p>We also cover key tools enabling local model serving and inference, including open source solutions such as LLama.cpp, Llamafile, and Ollama, along with user-friendly frontend interfaces that make local LLM usage more accessible. We conclude with a detailed case study, analyzing how different quantization approaches impact model performance in resource-constrained environments. This analysis reveals the critical tradeoffs between model size, inference speed, and output quality that practitioners must navigate.</p>
</section>
<section id="choosing-your-model">
<span id="local-model-selection"></span><h2><a class="toc-backref" href="#id325" role="doc-backlink"><span class="section-number">8.2. </span>Choosing your Model</a><a class="headerlink" href="#choosing-your-model" title="Permalink to this heading">¶</a></h2>
<p>The landscape of open source LLMs is rapidly evolving, with new models emerging by the day. While proprietary LLMs have garnered significant attention, open source LLMs are gaining traction due to their flexibility, customization options, and cost-effectiveness.</p>
<p>It is important to observe long-term strategic considerations when choosing a model. These entails prioritization dimensions that may enable competitive advantage in the long-term, including:</p>
<ol class="arabic simple">
<li><p><strong>Managed Services Support</strong>: You may start experimenting locally with LLMs but eventually you will need to deployment options: either host models yourself or consider managed services. Cloud providers like AWS Bedrock, SambaNova and <a class="reference external" href="http://Together.ai">Together.ai</a> can simplify deployment and management but model family support varies along with varying SLAs for model availability, support and model serving <span id="id1">[<a class="reference internal" href="#id81" title="Artificial Analysis. Llm provider leaderboards. https://artificialanalysis.ai/leaderboards/providers, 2024. Accessed: 2024.">Analysis, 2024</a>]</span>. One should evaluate the availability of managed services for your target model family.</p></li>
<li><p><strong>Vendor Long-Term Viability</strong>: Consider vendor’s long-term strategy and transparency around future development. Evaluate factors like funding, market position, and development velocity to assess whether the vendor will remain a reliable partner. Further, transparency around long-term strategy and roadmap is a critical consideration when choosing a model vendor partner.</p></li>
<li><p><strong>Single-Provider Lock-in</strong>: Users and organizations should avoid the risk of lock-in by remaining flexible with your choice of LLM providers. Today’s winning models are not guaranteed to be the same in the future.</p></li>
<li><p><strong>Time-to-market and Customization</strong>: As the same models are available to everyone, base capabilities are becoming commoditized. As a consequence, competitive advantage comes from the application layer. Hence, the ability to iterate fast while customizing to your specific domain becomes a critical strategic consideration when choosing a model.</p></li>
<li><p><strong>Data Competitive Edge</strong>: As the cost of (pre-trained) general intelligence decays rapidly, proprietary data becomes competitive advantage. Hence, the ability to add unique, custom, domain-specific datasets to base models is a critical consideration that will separate winners from losers.</p></li>
</ol>
<p>In this section, we aim to provide a comprehensive set of considerations to selecting the right open-source LLM for your specific needs, emphasizing the importance of aligning the LLM’s capabilities with the intended task and considering resources constraints.</p>
<section id="task-suitability">
<h3><a class="toc-backref" href="#id326" role="doc-backlink"><span class="section-number">8.2.1. </span>Task Suitability</a><a class="headerlink" href="#task-suitability" title="Permalink to this heading">¶</a></h3>
<p>When evaluating an open source LLM, task suitability is a critical first consideration. A model that performs well on general benchmarks may struggle with specific domain tasks. Understanding the intended use case helps narrow down model options based on their demonstrated strengths.</p>
<p><strong>Task Categories</strong></p>
<p>When determining which LLM task to prioritize, carefully consider your specific use case and end-user needs. Different applications require distinct model capabilities and optimizations. Common LLM Task Categories include:</p>
<ul class="simple">
<li><p><strong>Text Summarization</strong>: Condensing documents into concise summaries that capture key information.</p></li>
<li><p><strong>Question Answering</strong>: Providing accurate responses by extracting relevant information from knowledge bases.</p></li>
<li><p><strong>Text Generation</strong>: Creating high-quality content across formats, from documentation to creative writing.</p></li>
<li><p><strong>Code Generation</strong>: Writing clean, documented code in multiple programming languages.</p></li>
<li><p><strong>Language Translation</strong>: Converting text between languages while preserving meaning and nuance.</p></li>
<li><p><strong>Dialogue Systems</strong>: Enabling natural conversations for customer support and interactive learning.</p></li>
<li><p><strong>Text Classification</strong>: Categorizing and labeling text data for sentiment analysis, topic modeling, and content moderation.</p></li>
<li><p><strong>Named Entity Recognition</strong>: Identifying and extracting specific entities from text, such as people, organizations, and locations.</p></li>
</ul>
<p><a class="reference internal" href="#task-number"><span class="std std-numref">Fig. 8.1</span></a> shows the number models per task category available at Hugging Face as of December 22, 2024 <span id="id2">[<a class="reference internal" href="#id62" title="HuggingFace. Open source ai year in review 2024. https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024, 2024t. Accessed: 2024.">HuggingFace, 2024t</a>]</span>. Text generation is by far the most popular task category.</p>
<figure class="align-center" id="task-number">
<a class="reference internal image-reference" href="../_images/task_number.png"><img alt="Task Number" src="../_images/task_number.png" style="width: 471.20000000000005px; height: 270.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.1 </span><span class="caption-text">Number of models per task category from Hugging Face as of December 22, 2024 <span id="id3">[<a class="reference internal" href="#id62" title="HuggingFace. Open source ai year in review 2024. https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024, 2024t. Accessed: 2024.">HuggingFace, 2024t</a>]</span>.</span><a class="headerlink" href="#task-number" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>Model Types</strong></p>
<p>Open source LLMs can be broadly categorized into three main types as far as they level of customization is concerned, each with distinct characteristics and use cases (see <a class="reference internal" href="#model-types"><span class="std std-numref">Fig. 8.2</span></a>):</p>
<ul class="simple">
<li><p><strong>Base Models</strong>: These foundation models provide broad language understanding capabilities but typically require additional fine-tuning to excel at specific tasks. They serve as versatile starting points for customization. Examples: meta-llama/Llama-2-70b, Qwen/Qwen2.5-72B</p></li>
<li><p><strong>Instruction-Tuned Models</strong>: Enhanced through fine-tuning on instruction-following datasets, these models excel at interpreting and executing explicit prompts and commands. They bridge the gap between general language capabilities and practical task execution. Chat models are a good example of this category. Examples: meta-llama/Llama-2-70b-chat-hf (Chat), Qwen/Qwen2.5-72B-Instruct</p></li>
<li><p><strong>Domain-Adapted Models</strong>: Specialized for particular fields through targeted fine-tuning and/or preference-alignment on domain-specific data. Examples: Med-PaLM 2 for healthcare, BloombergGPT for finance.</p></li>
</ul>
<figure class="align-center" id="model-types">
<a class="reference internal image-reference" href="../_images/model_types.svg"><img alt="Model Types" height="240" src="../_images/model_types.svg" width="924" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.2 </span><span class="caption-text">Model Types.</span><a class="headerlink" href="#model-types" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The Llama 2 model family <span id="id4">[<a class="reference internal" href="#id229" title="Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: open foundation and fine-tuned chat models. 2023. URL: https://arxiv.org/abs/2307.09288, arXiv:2307.09288.">Touvron <em>et al.</em>, 2023</a>]</span> illustrates these distinctions well. The base Llama 2, trained on 2 trillion tokens of public data, demonstrates general-purpose capabilities across text generation and translation tasks. Its chat-optimized instruction-tuned variant, Llama 2-Chat, underwent additional fine-tuning on over 1 million human-annotated conversational examples, making it particularly adept at natural dialogue.</p>
<p>Benchmark results <span id="id5">[<a class="reference internal" href="#id162" title="Meta AI. Llama-2-70b-chat-hf. HuggingFace Model, 2024c. 70 billion parameter chat model from Meta's Llama 2 family. URL: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf.">Meta AI, 2024c</a>]</span> in <a class="reference internal" href="#llama2-benchmark"><span class="std std-numref">Table 8.1</span></a> highlight the impact of model specialization. On the TruthfulQA <span id="id6">[<a class="reference internal" href="safety.html#id159" title="Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: measuring how models mimic human falsehoods. 2022. URL: https://arxiv.org/abs/2109.07958, arXiv:2109.07958.">Lin <em>et al.</em>, 2022</a>]</span> and Toxigen <span id="id7">[<a class="reference internal" href="#id167" title="Khalid Alnajjar and others. Toxigen dataset. Papers with Code Dataset, 2024. Dataset for evaluating and mitigating toxic language generation in language models. URL: https://paperswithcode.com/dataset/toxigen.">Alnajjar and others, 2024</a>]</span> benchmarks measuring truthful and informative responses. We observe that the chat-optimized variants show substantially improved truthfulness. Similarly, on the ToxiGen benchmark measuring toxic content generation, Llama 2-Chat models demonstrate near-zero toxicity compared to base models’ 21-26% rates.</p>
<table class="docutils align-center" id="llama2-benchmark">
<caption><span class="caption-number">Table 8.1 </span><span class="caption-text">Benchmark results for Llama 2 family of models.</span><a class="headerlink" href="#llama2-benchmark" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Size</p></th>
<th class="head"><p>TruthfulQA</p></th>
<th class="head"><p>Toxigen</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Llama 2</p></td>
<td><p>7B</p></td>
<td><p>33.29</p></td>
<td><p>21.25</p></td>
</tr>
<tr class="row-odd"><td><p>Llama 2</p></td>
<td><p>13B</p></td>
<td><p>41.86</p></td>
<td><p>26.10</p></td>
</tr>
<tr class="row-even"><td><p>Llama 2</p></td>
<td><p>70B</p></td>
<td><p>50.18</p></td>
<td><p>24.60</p></td>
</tr>
<tr class="row-odd"><td><p>Llama-2-Chat</p></td>
<td><p>7B</p></td>
<td><p>57.04</p></td>
<td><p>0.00</p></td>
</tr>
<tr class="row-even"><td><p>Llama-2-Chat</p></td>
<td><p>13B</p></td>
<td><p>62.18</p></td>
<td><p>0.00</p></td>
</tr>
<tr class="row-odd"><td><p>Llama-2-Chat</p></td>
<td><p>70B</p></td>
<td><p>64.14</p></td>
<td><p>0.01</p></td>
</tr>
</tbody>
</table>
<p>While Llama family of models exhibits strong performance across general knowledge, instruction following, and specialized domains, purpose-built models may still outperform it in highly specific applications. Qwen/Qwen2.5-Coder-32B-Instruct <span id="id8">[<a class="reference internal" href="#id83" title="Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, and others. Qwen2.5 - coder technical report. arXiv preprint arXiv:2409.12186, 2024.">Hui <em>et al.</em>, 2024</a>]</span> is an example of a purpose-built model that demonstrates significant performance on the specific task of code generation.</p>
<p><strong>Model Features</strong></p>
<p>Model features can either enable or limit the feasibility of specific use cases. Understanding features of your candidate models is crucial for determining whether a model is suitable for your application. For example:</p>
<ul class="simple">
<li><p><strong>Context Length</strong>: The model’s ability to process longer text sequences directly impacts task suitability. A legal contract analysis systems requiring the model to reason about a 5000-page document would be impractical with a model limited to 2,048 tokens, while models supporting 2M tokens could handle this task effectively without the need for other techniques e.g. context chunking.</p></li>
<li><p><strong>Output Control</strong>: Some tasks require precise, factual and structured outputs while others allow more creative, unstructured generation. Models vary in their output reliability. Grammar constraints and other control mechanisms may be needed to ensure reliable outputs. See Chapter <a class="reference internal" href="structured_output.html#structure"><span class="std std-ref">Structured Output</span></a> for more details.</p></li>
<li><p><strong>Caching</strong>: Models that support caching can speed up inference at lower costs. This becomes particularly important for applications requiring cost-effective real-time responses.</p></li>
<li><p><strong>Multi-modal Capabilities</strong>: Some applications fundamentally require multi-modal processing. A medical diagnosis assistant analyzing both patient records and X-ray images would be impossible to implement with a text-only model, necessitating a multi-modal model that can process both text and images coherently.</p></li>
<li><p><strong>Output Token Length</strong>: The model’s capacity to generate longer responses affects its suitability for content generation tasks. A model excelling at concise responses may struggle with long-form content creation like technical documentation or detailed analysis reports.</p></li>
</ul>
</section>
<section id="performance-cost">
<h3><a class="toc-backref" href="#id327" role="doc-backlink"><span class="section-number">8.2.2. </span>Performance &amp; Cost</a><a class="headerlink" href="#performance-cost" title="Permalink to this heading">¶</a></h3>
<p>General benchmarks are useful for comparing models across different standard tasks. Open Source models are becoming more competitive with proprietary models with LLama, Qwen, DeepSeek and Mistral model families being some of the most powerful open source models available today.</p>
<p>Qwen model family <span id="id9">[<a class="reference internal" href="#id214" title="Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. 2024. URL: https://arxiv.org/abs/2412.15115, arXiv:2412.15115.">Qwen <em>et al.</em>, 2024</a>]</span> emerged in 2024 as a model family achieving competitive performance with relatively smaller parameter counts compared to its competitors. The flagship Qwen2.5-72B-Instruct model demonstrates performance comparable to the much larger Llama-3-405B-Instruct while being about 5 times smaller. The models excel in specialized tasks like mathematics and coding, handle structured data effectively, and offer enhanced support for tool use and long-text generation as shown in <a class="reference internal" href="#qwen-perf"><span class="std std-numref">Fig. 8.3</span></a>.</p>
<figure class="align-center" id="qwen-perf">
<a class="reference internal image-reference" href="../_images/qwen_perf.png"><img alt="Qwen Performance" src="../_images/qwen_perf.png" style="width: 764.0px; height: 551.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.3 </span><span class="caption-text">Qwen Performance.</span><a class="headerlink" href="#qwen-perf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#perf"><span class="std std-numref">Fig. 8.4</span></a> shows a comparison including reference proprietary models such as GPT-40, Gemini 1.5 Pro and Claude 3.5 Sonnet. Leading models vary per domain but all top ranking models are proprietary. However, open source models do show competitive performance with Qwen and LLama models leading the pack, overall.</p>
<figure class="align-center" id="perf">
<a class="reference internal image-reference" href="../_images/perf_.png"><img alt="Performance Comparison including proprietary models." src="../_images/perf_.png" style="width: 1219.2px; height: 594.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.4 </span><span class="caption-text">Performance Comparison including proprietary models.</span><a class="headerlink" href="#perf" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Also from China, DeepSeek-V3 <span id="id10">[<a class="reference internal" href="#id172" title="DeepSeek. Deepseek-v3 technical report. Technical Report, 2024. URL: https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf.">DeepSeek, 2024</a>]</span> represents a major breakthrough in open source language models, emerging as arguably the most capable open source large language model available as of the end of 2024. With 671 billion parameters and 37 billion active MoE (Mixture of Experts) parameters, it achieves performance on par with leading proprietary models like Claude 3.5 Sonnet and GPT 4o as shown in <a class="reference internal" href="#deep"><span class="std std-numref">Fig. 8.5</span></a>. The model demonstrates impressive cost efficiency metrics (see <a class="reference internal" href="#deep2"><span class="std std-numref">Fig. 8.6</span></a>), processing input tokens at <span class="math notranslate nohighlight">\(0.27 per million and output tokens at \)</span>1.1 per million, while maintaining a generation speed of 60 tokens per second (3x faster than DeepSeek-V2).</p>
<p>What makes DeepSeek-V3 particularly remarkable is that these capabilities were achieved with a relatively modest training budget of just $5.5 million, used to train on 14.8 trillion tokens. This efficiency in training demonstrates the potential for open source models to compete with proprietary alternatives at a fraction of the cost. The model’s release marks a significant milestone in the democratization of advanced AI capabilities, challenging the dominance of proprietary models within big tech. One should be cautious though as the model has not yet been battle-tested in the wild but this is an exciting development demonstrating the potential of open source models to compete with proprietary alternatives.</p>
<figure class="align-center" id="deep">
<a class="reference internal image-reference" href="../_images/deep.jpeg"><img alt="DeepSeek-V3" src="../_images/deep.jpeg" style="width: 861.9px; height: 752.7px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.5 </span><span class="caption-text">DeepSeek-V3 Performance Comparison</span><a class="headerlink" href="#deep" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="deep2">
<a class="reference internal image-reference" href="../_images/deep2.jpeg"><img alt="DeepSeek-V3 Cost Benefit Analysis" src="../_images/deep2.jpeg" style="width: 832.0px; height: 440.7px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.6 </span><span class="caption-text">DeepSeek-V3 Cost Benefit Analysis</span><a class="headerlink" href="#deep2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>While standard benchmarks provide valuable initial insights, they should be interpreted cautiously since models can be specifically optimized for these popular tests without necessarily performing well in target use cases. This necessitates developing custom evaluation frameworks with real-world validation - creating test datasets representing actual usage scenarios, defining metrics aligned with business objectives, and establishing clear baselines and improvement targets. Only through such rigorous testing can practitioners truly understand how well a model will perform in their specific context.</p>
<p>In that way, after identifying candidate models, it’s essential to rigorously evaluate their capabilities against unique use case requirements and constraints, as models that excel in standardized tests may struggle with the nuanced demands of real-world applications. Chapter <a class="reference internal" href="evals.html#evals"><span class="std std-ref">The Evals Gap</span></a> explores this critical challenge in detail, providing frameworks and best practices for comprehensive model evaluation.</p>
<p>Model quality performance should not be evaluated in isolation. It is important to also consider the cost of running the model once it’s deployed as well as its computational performance. This depends on the model size, hardware, and the platform used (self-hosted vs. managed services). Key metrics include:</p>
<ul class="simple">
<li><p><strong>Cost-Related</strong>:</p>
<ul>
<li><p><strong>Cost Per Output Token (CPOT)</strong>: This metric measures the cost of text generation.</p></li>
<li><p><strong>Cost Per Input Token (CPIT)</strong>: This metric measures the cost for input prompt processing.</p></li>
<li><p><strong>Total Cost of Ownership (TCO)</strong>: Consider the full lifecycle cost, including development, deployment, maintenance, infrastructure, and ongoing iteration.</p></li>
</ul>
</li>
<li><p><strong>Time-Related</strong>:</p>
<ul>
<li><p><strong>Time Per Output Token (TPOT)</strong>: This metric measures the speed of text generation and is crucial for user experience, especially in interactive applications.</p></li>
<li><p><strong>Time to First Token (TTFT)</strong>: Essential for streaming applications like chatbots, as it measures how quickly the model begins generating a response.</p></li>
<li><p><strong>Latency</strong>: Time to first token of tokens received, in seconds, after API request sent. For models which do not support streaming, this represents time to receive the completion.</p></li>
</ul>
</li>
</ul>
<p><a class="reference internal" href="#p2"><span class="std std-numref">Fig. 8.7</span></a> shows a comparison of quality now with the added dimension of cost. Quality is measured as an average of scores from MMLU, GPQA, Math &amp; HumanEval benchmarks <span id="id11">[<a class="reference internal" href="#id79" title="Artificial Analysis. Methodology. https://artificialanalysis.ai/methodology, 2024. Accessed: December 22, 2024.">Analysis, 2024</a>]</span>. Price is a blend of Cost Per Input Token plus Input &amp; Cost Per Output Token (3:1 ratio). Reported numbers represent median across cloud providers <span id="id12">[<a class="reference internal" href="#id78" title="Artificial Analysis. Llm provider leaderboards. https://artificialanalysis.ai/leaderboards/providers, 2024. Accessed: 2024.">Analysis, 2024</a>]</span> supporting these models.</p>
<figure class="align-center" id="p2">
<a class="reference internal image-reference" href="../_images/p2.png"><img alt="Performance Comparison including proprietary models." src="../_images/p2.png" style="width: 810.0px; height: 455.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.7 </span><span class="caption-text">Performance Comparison including proprietary models.</span><a class="headerlink" href="#p2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>We observe Qwen2.5 72B and Llama 3.3 70B offer the best value among Open Source models, providing high quality at a relatively affordable price comparable to GPT-4o mini, for instance. Meanwhile Nova Lite, Nova Micro, and Llama 3.1 8B demonstrate to be budget-friendly options catering to use cases where cost is a significant factor and some compromise on quality is acceptable.</p>
<p>From <a class="reference internal" href="#p1"><span class="std std-numref">Fig. 8.8</span></a> we have evidence that output prices are higher than input prices. This reflects the greater computational resources typically required at inference time for output compared to processing input text (e.g. tokenization, encoding). We also observe a quite significant variation in pricing across different models. Prices range from a few cents per 1M tokens (e.g., Gemini 2.0 Flash, Nova Micro, Nova Lite) to several dollars per 1M tokens (e.g., Claude 3.5 Sonnet, GPT-4o). Mistral large 2 is the most expensive model at <span class="math notranslate nohighlight">\(2/\)</span>6 per 1M input/output tokens while Nova Micro family is the cheapest among Open Source options.</p>
<figure class="align-center" id="p1">
<a class="reference internal image-reference" href="../_images/p1.png"><img alt="Input and Output Prices" src="../_images/p1.png" style="width: 804.8000000000001px; height: 341.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.8 </span><span class="caption-text">Input and Output Prices Comparison.</span><a class="headerlink" href="#p1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Latency figures in <a class="reference internal" href="#latency"><span class="std std-numref">Fig. 8.9</span></a> put GPT-4o (Nov ‘24) as the best performing model but Llama, Nova Micro, Phi and Mistral model families all have options with latency of half a second or better beating Gemini and Claude models considered as well as GPT-4o mini.</p>
<figure class="align-center" id="latency">
<a class="reference internal image-reference" href="../_images/latency.png"><img alt="Latency Comparison" src="../_images/latency.png" style="width: 799.6px; height: 324.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.9 </span><span class="caption-text">Latency Comparison.</span><a class="headerlink" href="#latency" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>This analysis provides a framework for evaluating key performance considerations when selecting an LLM. While the specific figures for cost, latency, and quality change frequently (often daily) as providers update their offerings and pricing, the fundamental tradeoffs remain relevant. When evaluating model suitability for a specific use case, practitioners should carefully consider:</p>
<ul class="simple">
<li><p>The balance between quality requirements and cost constraints</p></li>
<li><p>Latency requirements for the intended application</p></li>
<li><p>Total cost of ownership including both input and output token costs</p></li>
<li><p>Whether streaming capabilities are needed (TTFT becomes more critical)</p></li>
<li><p>Infrastructure and deployment costs</p></li>
</ul>
<p>Regular re-evaluation of these metrics is recommended as the landscape evolves rapidly. What represents the optimal choice today may change as new models are released and existing ones are updated.</p>
</section>
<section id="licensing">
<h3><a class="toc-backref" href="#id328" role="doc-backlink"><span class="section-number">8.2.3. </span>Licensing</a><a class="headerlink" href="#licensing" title="Permalink to this heading">¶</a></h3>
<p>When evaluating open-source LLMs, it’s important to consider licensing and data usage policies. Some models may require attribution or commercial use licenses, while others may be more permissive. Additionally, ensure that the model’s training data is compatible with your intended use case and complies with relevant data protection laws.</p>
<p>The licensing landscape for LLMs spans from highly permissive to custom and restricted usage. <a class="reference internal" href="#open-source-llms"><span class="std std-numref">Table 8.2</span></a> provides a summary of the licensing terms for some of the most popular open source LLMs. We observe two types of licenses:</p>
<ul class="simple">
<li><p><strong>Traditional Open Source</strong>:</p>
<ul>
<li><p>Apache 2.0 (exemplified by Mistral AI’s models) offers comprehensive commercial usage rights with minimal restrictions</p></li>
<li><p>MIT License (used by Microsoft’s Phi-3) provides similar freedoms with simpler terms</p></li>
</ul>
</li>
<li><p><strong>Custom Commercial Licenses</strong>:</p>
<ul>
<li><p>Meta’s LLaMA 3 allows free usage for applications serving under 700 million users</p></li>
<li><p>Alibaba’s Qwen2.5 permits free deployment for services with fewer than 100 million users</p></li>
<li><p>Both restrict using model outputs to train competing LLMs</p></li>
</ul>
</li>
</ul>
<table class="docutils align-center" id="open-source-llms">
<caption><span class="caption-number">Table 8.2 </span><span class="caption-text">Open Source LLMs.</span><a class="headerlink" href="#open-source-llms" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Creator</p></th>
<th class="head"><p>LLM</p></th>
<th class="head"><p>License</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Meta AI</p></td>
<td><p>LLaMA 3</p></td>
<td><p>Custom - Free if under 700M users, cannot use outputs to train other non-LLaMA LLMs</p></td>
</tr>
<tr class="row-odd"><td><p>Microsoft</p></td>
<td><p>Phi-3</p></td>
<td><p>MIT</p></td>
</tr>
<tr class="row-even"><td><p>Mistral AI</p></td>
<td><p>Mistral</p></td>
<td><p>Apache 2.0</p></td>
</tr>
<tr class="row-odd"><td><p>Alibaba</p></td>
<td><p>Qwen2.5</p></td>
<td><p>Custom - Free if under 100M users, cannot use outputs to train other non-Qwen LLMs</p></td>
</tr>
<tr class="row-even"><td><p>Google</p></td>
<td><p>Gemma</p></td>
<td><p>Custom - Free with usage restrictions, models trained on outputs become Gemma derivatives</p></td>
</tr>
<tr class="row-odd"><td><p>DeepSeek</p></td>
<td><p>DeepSeek-V2</p></td>
<td><p>Custom - Free with usage restrictions, models trained on outputs become DeepSeek derivatives</p></td>
</tr>
</tbody>
</table>
<p>When selecting an open-source LLM for deployment, practitioners must carefully evaluate licensing terms that align with intended usage (whether commercial, research, or other). While permissive licenses like Apache 2.0 and MIT allow broad usage rights, custom licenses may impose specific restrictions on commercial applications or model derivatives, making thorough license review essential for sustainable implementation.</p>
<p>The training data sources for LLMs represent another critical consideration. Models vary significantly in their training data foundations - some leverage purely public datasets while others incorporate proprietary or restricted content with the added complexity that public data does not mean free data. These data choices fundamentally impact not only model capabilities but also legal and regulatory compliance.</p>
<p>The legal landscape surrounding LLM training data has grown increasingly complex, particularly regarding copyright infringement concerns. The high-profile lawsuit between OpenAI and The New York Times <span id="id13">[<a class="reference internal" href="#id67" title="Harvard Law Review. Nyt v. openai: the times's about-face. https://harvardlawreview.org/blog/2024/04/nyt-v-openai-the-timess-about-face/, 2024. Accessed: 2024.">Review, 2024</a>]</span> serves as a pivotal example, where the Times claims its copyrighted materials were used without authorization to train language models. This litigation has far-reaching consequences for developers building LLM-powered applications. Should courts rule in favor of copyright holders, model providers may need to withdraw and retrain models containing protected content. These legal uncertainties introduce substantial complexity into LLM implementation strategies, demanding careful consideration during project planning phases.</p>
<p>Recent LLM releases demonstrate varying levels of data transparency. For instance, Qwen2.5’s approach <span id="id14">[<a class="reference internal" href="#id214" title="Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. 2024. URL: https://arxiv.org/abs/2412.15115, arXiv:2412.15115.">Qwen <em>et al.</em>, 2024</a>]</span> illustrates common industry practices in both its achievements and limitations. On the training data scale front, Qwen2.5 does provide some transparency by discussing some training data methodology compared to previous versions such as expanding from 7 trillion to 18 trillion tokens, while implementing sophisticated quality filtering and carefully balancing domain representation through sampling adjustments.</p>
<p>However, like many commercial LLMs, Qwen2.5 exhibits transparency limitations. The report provides incomplete disclosure of data sources and limited information about the proportions of different data types used in training. The preprocessing methodologies remain unclear, and there is minimal discussion of potential biases that may exist in the training data.</p>
<p>Similarly, in the Llama 3 paper <span id="id15">[<a class="reference internal" href="#id208" title="Meta AI. The llama 3 herd of models. 2024c. URL: https://arxiv.org/abs/2407.21783, arXiv:2407.21783.">AI, 2024c</a>]</span>, Meta AI does share some details about the pre-training corpus stating simply stating that it was around 15T multilingual tokens, compared to 1.8T tokens for Llama 2. The exact sources of data used for pre-training and post-training are not explicitly listed.</p>
<p>These gaps in transparency reflect a broader industry challenge in balancing commercial interests with the need for openness and scientific reproducibility.</p>
<p>A significant advancement in open-source language model training data is HuggingFace’s release of the FineWeb datasets. In its first release <span id="id16">[<a class="reference internal" href="#id63" title="Guilherme Penedo, Hynek Kydlicek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: decanting the web for the finest text data at scale. 2024. URL: https://arxiv.org/abs/2406.17557, arXiv:2406.17557.">Penedo <em>et al.</em>, 2024</a>]</span>, FineWeb is made of a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. Additionally, data curation codebase and all of the models trained during our ablation experiments are made available. FineWeb is a fine example of an initiative that helps minimize the gap between proprietary and public knowledge.</p>
</section>
<section id="community-support">
<h3><a class="toc-backref" href="#id329" role="doc-backlink"><span class="section-number">8.2.4. </span>Community Support</a><a class="headerlink" href="#community-support" title="Permalink to this heading">¶</a></h3>
<p>Community support plays a vital role in the open-source LLM ecosystem. Active communities contribute to model development, provide technical assistance, and share valuable resources. When evaluating open-source LLMs, the strength and engagement of the community should be a key consideration, as it directly impacts the model’s long-term viability and practical utility.</p>
<p>The popularity of different model families reflects their community adoption. In 2024, the Qwen and Llama families have emerged as clear favorites, with Qwen2.5-1.5B-Instruct alone representing 35% of total open source models downloads in 2024.</p>
<figure class="align-center" id="downloads">
<a class="reference internal image-reference" href="../_images/downloads.png"><img alt="Hugging Face Downloads" src="../_images/downloads.png" style="width: 1142.3999999999999px; height: 453.59999999999997px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.10 </span><span class="caption-text">Hugging Face Model Downloads in 2024 as of December 22 of the same year <span id="id17">[<a class="reference internal" href="#id62" title="HuggingFace. Open source ai year in review 2024. https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024, 2024t. Accessed: 2024.">HuggingFace, 2024t</a>]</span>.</span><a class="headerlink" href="#downloads" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Strong communities accelerate model innovation through collective effort. When developers and researchers collaborate on model development, they create a powerful ecosystem of continuous improvement. Through transparent sharing of findings, they enable rapid development of novel applications and specialized model variants for specific domains. This collaborative environment naturally leads to the establishment of best practices and frameworks that benefit the entire community. The success of this community-driven approach is evident in models like Qwen2.5-1.5B-Instruct, which has spawned 200+ derivative models through post-training adaptations <span id="id18">[<a class="reference internal" href="#id213" title="Qwen. Qwen2.5-1.5b-instruct. 2024b. Accessed: December 22, 2024. URL: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct.">Qwen, 2024b</a>]</span>.</p>
</section>
<section id="customization">
<h3><a class="toc-backref" href="#id330" role="doc-backlink"><span class="section-number">8.2.5. </span>Customization</a><a class="headerlink" href="#customization" title="Permalink to this heading">¶</a></h3>
<p>Model customization is an important consideration when selecting an open-source LLM. Adapting and fine-tuning to specific use cases can significantly impact practical utility and performance in production environments.</p>
<p>Model providers increasingly offer streamlined fine-tuning services. For example, Mistral demonstrates an accessible approach to model customization.
The code below shows Mistral’s straightforward fine-tuning API. The example shows how to create and start a fine-tuning job with just a few lines of code. The fine-tuning job is configured with the base model “open-mistral-7b” and uses training and validation files from the Ultrachat dataset <span id="id19">[<a class="reference internal" href="#id210" title="HuggingFace. Ultrachat-200k dataset. 2024u. Accessed: 2024. URL: https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k.">HuggingFace, 2024u</a>]</span>. This API design makes it easy to experiment with model customization while maintaining control over the training process.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># create a fine-tuning job</span>
<span class="n">created_jobs</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">fine_tuning</span><span class="o">.</span><span class="n">jobs</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;open-mistral-7b&quot;</span><span class="p">,</span> 
    <span class="n">training_files</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;file_id&quot;</span><span class="p">:</span> <span class="n">ultrachat_chunk_train</span><span class="o">.</span><span class="n">id</span><span class="p">,</span> <span class="s2">&quot;weight&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}],</span>
    <span class="n">validation_files</span><span class="o">=</span><span class="p">[</span><span class="n">ultrachat_chunk_eval</span><span class="o">.</span><span class="n">id</span><span class="p">],</span> 
    <span class="n">hyperparameters</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;training_steps&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
        <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span><span class="mf">0.0001</span>
    <span class="p">},</span>
    <span class="n">auto_start</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="c1"># start a fine-tuning job</span>
<span class="n">client</span><span class="o">.</span><span class="n">fine_tuning</span><span class="o">.</span><span class="n">jobs</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">job_id</span> <span class="o">=</span> <span class="n">created_jobs</span><span class="o">.</span><span class="n">id</span><span class="p">)</span>

<span class="n">created_jobs</span>
</pre></div>
</div>
<p>For more comprehensive customization needs, Hugging Face’s Transformer Reinforcement Learning (TRL) toolkit provides robust capabilities for model adaptation. Built on the Transformers library, TRL supports <span id="id20">[<a class="reference internal" href="#id238" title="HuggingFace. Trl. 2024d. TRL. URL: https://huggingface.co/docs/trl/en/index.">HuggingFace, 2024d</a>]</span>:</p>
<ul class="simple">
<li><p>Supervised Fine-Tuning (SFT)</p></li>
<li><p>Reward Modeling (RM)</p></li>
<li><p>Proximal Policy Optimization (PPO)</p></li>
<li><p>Direct Preference Optimization (DPO)</p></li>
</ul>
<p>In <a class="reference internal" href="alignment.html#alignment-case-study"><span class="std std-ref">Case Study: Aligning a Language Model to a Policy</span></a>, we will explore how to use TRL to fine-tune a model to align with user preferences.</p>
<p>Successful model customization demands managing critical resources throughout the development lifecycle. This includes rigorous dataset preparation and validation to ensure high-quality training data, careful configuration of training infrastructure to optimize computational resources, systematic experimentation iterations while managing associated costs, comprehensive performance evaluation frameworks to measure improvements, and thoughtful deployment architecture planning to ensure smooth production integration. Of course, actual cost of storage and inference should be taken into consideration. <a class="reference internal" href="#mistral-costs"><span class="std std-numref">Table 8.3</span></a> shows as an example the cost of associated with fine-tuning Mistral models <span id="id21">[<a class="reference internal" href="#id61" title="Mistral AI. Mistral technology and pricing. https://mistral.ai/technology/, 2024a. Accessed: 2024.">AI, 2024a</a>]</span>.</p>
<table class="docutils align-center" id="mistral-costs">
<caption><span class="caption-number">Table 8.3 </span><span class="caption-text">Mistral fine-tuning costs as of December 22, 2024.</span><a class="headerlink" href="#mistral-costs" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>One-off training (/M tokens)</p></th>
<th class="head"><p>Storage</p></th>
<th class="head"><p>Input (/M tokens)</p></th>
<th class="head"><p>Output (/M tokens)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Mistral NeMo</p></td>
<td><p>$1</p></td>
<td><p>$2 per month per model</p></td>
<td><p>$0.15</p></td>
<td><p>$0.15</p></td>
</tr>
<tr class="row-odd"><td><p>Mistral Large 24.11</p></td>
<td><p>$9</p></td>
<td><p>$4 per month per model</p></td>
<td><p>$2</p></td>
<td><p>$6</p></td>
</tr>
<tr class="row-even"><td><p>Mistral Small</p></td>
<td><p>$3</p></td>
<td><p>$2 per month per model</p></td>
<td><p>$0.2</p></td>
<td><p>$0.6</p></td>
</tr>
<tr class="row-odd"><td><p>Codestral</p></td>
<td><p>$3</p></td>
<td><p>$2 per month per model</p></td>
<td><p>$0.2</p></td>
<td><p>$0.6</p></td>
</tr>
</tbody>
</table>
<p>Small language models can serve as a lightweight alternative to customization compared to large models. Recent research has shown that smaller models can achieve competitive performance compared to larger models <span id="id22">[<a class="reference internal" href="#id209" title="HuggingFace. Scaling test time compute. 2024v. Accessed: 2024. URL: https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute.">HuggingFace, 2024v</a>, <a class="reference internal" href="#id211" title="Justin Zhao, Timothy Wang, Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky, Piero Molino, Travis Addair, and Devvret Rishi. Lora land: 310 fine-tuned llms that rival gpt-4, a technical report. 2024. URL: https://arxiv.org/abs/2405.00732, arXiv:2405.00732.">Zhao <em>et al.</em>, 2024</a>]</span>. A noteworthy example is Hugging Face’s SmolLM2 <span id="id23">[<a class="reference internal" href="#id82" title="Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín Piqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. Smollm2 - with great data, comes great performance. 2024.">Allal <em>et al.</em>, 2024</a>]</span>, a family of compact language models designed with several key advantages:</p>
<ol class="arabic simple">
<li><p>Compact Sizes:</p></li>
</ol>
<ul class="simple">
<li><p>Available in three sizes: 135M, 360M, and 1.7B parameters</p></li>
<li><p>Small enough to run on-device and local hardware</p></li>
<li><p>Doesn’t require expensive GPU resources</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p>Versatility:</p></li>
</ol>
<ul class="simple">
<li><p>Can perform a wide range of tasks despite small size</p></li>
<li><p>Supports text summarization, rewriting, and function calling</p></li>
<li><p>Can be used for multimodal applications (via SmolVLM)</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p>Easy Integration and Customization:</p></li>
</ol>
<ul class="simple">
<li><p>Supports multiple frameworks like llama.cpp, MLX, MLC, and transformers.js</p></li>
<li><p>Can be fine-tuned using TRL and PEFT for custom applications</p></li>
<li><p>Provides pre-training and fine-tuning scripts for customization</p></li>
<li><p>Includes synthetic data pipelines for creating custom training data</p></li>
</ul>
<p>These models address a crucial need in the AI ecosystem by making language models more accessible and practical for developers who need local, efficient solutions without compromising too much on capability. The provided tools and scripts for customization make it particularly valuable for developers who need to adapt the model for specific use cases or domains.</p>
</section>
</section>
<section id="tools-for-local-llm-deployment">
<h2><a class="toc-backref" href="#id331" role="doc-backlink"><span class="section-number">8.3. </span>Tools for Local LLM Deployment</a><a class="headerlink" href="#tools-for-local-llm-deployment" title="Permalink to this heading">¶</a></h2>
<p>Local LLM deployment tools generally fall into two categories: inference-focused tools that prioritize performance and programmability for technical users requiring production-grade deployments, and user interface (UI) tools that emphasize accessibility through graphical interfaces for non-technical users, trading some performance for ease of use and broader adoption. In the following sections we will explore some of these tools discussing their features, capabilities, and trade-offs.</p>
<section id="serving-models">
<h3><a class="toc-backref" href="#id332" role="doc-backlink"><span class="section-number">8.3.1. </span>Serving Models</a><a class="headerlink" href="#serving-models" title="Permalink to this heading">¶</a></h3>
<p>Serving an LLM model involves making it available for inference by setting up infrastructure to process requests and manage resources efficiently. This serving layer handles several key responsibilities, from loading model weights and managing compute resources to processing requests and optimizing performance. Let’s examine the core components of model serving:</p>
<ol class="arabic simple">
<li><p><strong>Model Loading and Initialization</strong></p></li>
</ol>
<ul class="simple">
<li><p>Loading the trained model weights and parameters into memory</p></li>
<li><p>Initializing any required runtime configurations and optimizations</p></li>
<li><p>Setting up inference pipelines and processing workflows</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Resource Management</strong></p></li>
</ol>
<ul class="simple">
<li><p>Allocating and managing system memory (RAM/VRAM) for model weights</p></li>
<li><p>Handling computational resources like CPU/GPU efficiently</p></li>
<li><p>Implementing caching and batching strategies where appropriate</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Request Processing and Inference</strong></p></li>
</ol>
<ul class="simple">
<li><p>Accepting input requests through defined interfaces</p></li>
<li><p>Converting input text into token vectors <span class="math notranslate nohighlight">\(\mathbf{x} = [x_1, x_2, ..., x_n]\)</span> through tokenization</p></li>
<li><p>Computing probability distributions <span class="math notranslate nohighlight">\(P(x_{n+1}|x_1, x_2, ..., x_n; θ)\)</span> for next tokens</p></li>
<li><p>Performing matrix multiplications and attention computations</p></li>
<li><p>Sampling each new token from the calculated probability distribution</p></li>
<li><p>Post-processing and returning responses</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>Performance Optimization</strong></p></li>
</ol>
<ul class="simple">
<li><p>Implementing techniques like quantization to reduce memory usage</p></li>
<li><p>Optimizing inference speed through batching and caching</p></li>
<li><p>Managing concurrent requests and load balancing</p></li>
<li><p>Monitoring system resource utilization</p></li>
</ul>
<p>The serving layer acts as the bridge between the LLM and applications while working on top of a hardware stack as shown in <a class="reference internal" href="#local-inference"><span class="std std-numref">Fig. 8.11</span></a>. Getting this layer right is crucial for building locally-served reliable AI-powered applications, as it directly impacts the end-user experience in terms of response times, reliability, and resource efficiency.</p>
<figure class="align-center" id="local-inference">
<a class="reference internal image-reference" href="../_images/local_inference.svg"><img alt="Local Inference Server" height="523" src="../_images/local_inference.svg" width="626" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.11 </span><span class="caption-text">Local Inference Server.</span><a class="headerlink" href="#local-inference" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Model inference can be performed on Open Source models using cloud solutions such as Groq, Cerebras Systems, and SambaNova Systems. Here, we limit our scope to Open Source solutions that enable inference on local machines which includes consumer hardware. We will cover the following:</p>
<ul class="simple">
<li><p><strong>LLama.cpp</strong>: A highly optimized C++ implementation for running LLMs on consumer hardware</p></li>
<li><p><strong>Llamafile</strong>: A self-contained executable format by Mozilla for easy model distribution and deployment</p></li>
<li><p><strong>Ollama</strong>: A tool that simplifies running and managing local LLMs with Docker-like commands</p></li>
</ul>
<p>Let’s explore each of these options in detail.</p>
<section id="llama-cpp">
<h4><a class="toc-backref" href="#id333" role="doc-backlink"><span class="section-number">8.3.1.1. </span>LLama.cpp</a><a class="headerlink" href="#llama-cpp" title="Permalink to this heading">¶</a></h4>
<p>LLama.cpp <span id="id24">[<a class="reference internal" href="#id182" title="Georgi Gerganov and contributors. Llama.cpp. GitHub Repository, 2024a. High-performance inference of LLaMA models in pure C/C++. URL: https://github.com/ggerganov/llama.cpp.">Gerganov and contributors, 2024a</a>]</span> is an MIT-licensed open source optimized implementation of the <strong>LLama</strong> model architecture designed to run efficiently on machines with limited memory.</p>
<p>Originally developed by Georgi Gerganov and today counting with hundreds of contributors, this C/C++ LLama version provides a simplified interface and advanced features that allow language models to run locally without overwhelming systems. With the ability to run in resource-constrained environments, LLama.cpp makes powerful language models more accessible and practical for a variety of applications.</p>
<p>In its “Manifesto” <span id="id25">[<a class="reference internal" href="#id156" title="Georgi Gerganov and others. Quantization of llama models - discussion. GitHub Discussion, 2023. Discussion thread about quantization techniques and tradeoffs in llama.cpp. URL: https://github.com/ggerganov/llama.cpp/discussions/205.">Gerganov and others, 2023</a>]</span>, the author highlights the significant potential in bringing AI from cloud to edge devices, emphasizing the importance of keeping development lightweight, experimental, and enjoyable rather than getting bogged down in complex engineering challenges. The author states a vision that emphasizes maintaining an exploratory, hacker-minded approach while building practical edge computing solutions highlighting the following core principles:</p>
<ul class="simple">
<li><p>“Will remain open-source”</p></li>
<li><p>Focuses on simplicity and efficiency in codebase</p></li>
<li><p>Emphasizes quick prototyping over premature optimization</p></li>
<li><p>Aims to stay adaptable given rapid AI model improvements</p></li>
<li><p>Values practical experimentation over complex engineering</p></li>
</ul>
<p>LLama.cpp implementation characteristics include:</p>
<ol class="arabic simple">
<li><p><strong>Memory Efficiency</strong>: The main advantage of LLama.cpp is its ability to reduce memory requirements, allowing users to run large language models at the edge for instance offering ease of model quantization.</p></li>
<li><p><strong>Computational Efficiency</strong>: Besides reducing memory usage, LLama.cpp also focuses on improving execution efficiency, using specific C++ code optimizations to accelerate the process.</p></li>
<li><p><strong>Ease of Implementation</strong>: Although it’s a lighter solution, LLama.cpp doesn’t sacrifice result quality. It maintains the ability to generate texts and perform NLP tasks with high precision.</p></li>
</ol>
<p><strong>GGUF</strong></p>
<p>GGUF (GPT-Generated Unified Format) <span id="id26">[<a class="reference internal" href="#id185" title="Georgi Gerganov and contributors. Gguf file format specification. GitHub Repository, 2024b. Technical specification of the GGUF file format for efficient model storage and inference. URL: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md.">Gerganov and contributors, 2024b</a>]</span> is the latest model format used by LLama.cpp, replacing the older GGML format. It was designed specifically for efficient inference of large language models on consumer hardware. The key features that make GGUF particularly valuable include <span id="id27">[<a class="reference internal" href="#id183" title="IBM Think. Gguf vs ggml: what's the difference? 2024. Comparison of GGUF and GGML model formats. URL: https://www.ibm.com/think/topics/gguf-versus-ggml.">IBM Think, 2024</a>]</span>:</p>
<ul class="simple">
<li><p>Improved quantization: GGUF supports multiple quantization levels to reduce model size while preserving performance. Common quantization schemes that are supported by GGUF include:</p>
<ul>
<li><p>2-bit quantization: Offers the highest compression, significantly reducing model size and inference speed, though with a potential impact on accuracy.</p></li>
<li><p>4-bit quantization: Balances compression and accuracy, making it suitable for many practical applications.</p></li>
<li><p>8-bit quantization: Provides good accuracy with moderate compression, widely used in various applications.</p></li>
</ul>
</li>
<li><p>Metadata support: The format includes standardized metadata about model architecture, tokenization, and other properties</p></li>
<li><p>Memory mapping: Enables efficient loading of large models by mapping them directly from disk rather than loading entirely into RAM</p></li>
<li><p>Architecture-specific optimizations: Takes advantage of CPU/GPU specific instructions for faster inference</p></li>
<li><p>Versioning support: Includes proper versioning to handle format evolution and backwards compatibility</p></li>
</ul>
<p>These capabilities make GGUF models significantly more practical for running LLMs locally compared to full-precision formats, often dramatically reducing memory requirements. Hugging Face hosts a growing collection of pre-converted GGUF models <span id="id28">[<a class="reference internal" href="#id184" title="HuggingFace. Gguf models on huggingface. Online Repository, 2024x. Collection of models in GGUF format for efficient local inference. URL: https://huggingface.co/models?search=gguf.">HuggingFace, 2024x</a>]</span> and provides a tool (ggml-org/gguf-my-repo) to convert existing models to GGUF format, making it easier for developers to access and deploy optimized versions of popular language models.</p>
<p><strong>Setup</strong></p>
<p>Please follow the instructions from the LLama.cpp <a class="reference external" href="https://github.com/ggerganov/llama.cpp">GitHub repository</a> <span id="id29">[<a class="reference internal" href="#id182" title="Georgi Gerganov and contributors. Llama.cpp. GitHub Repository, 2024a. High-performance inference of LLaMA models in pure C/C++. URL: https://github.com/ggerganov/llama.cpp.">Gerganov and contributors, 2024a</a>]</span> to install and compile the library.</p>
<p>Here, we will compile the library from source on a Linux machine with 8 jobs in parallel for enhanced performance (add the <code class="docutils literal notranslate"><span class="pre">-j</span></code> argument to run multiple jobs in parallel).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>cmake

cmake<span class="w"> </span>-B<span class="w"> </span>build
cmake<span class="w"> </span>--build<span class="w"> </span>build<span class="w"> </span>--config<span class="w"> </span>Release<span class="w"> </span>-j<span class="w"> </span><span class="m">8</span>
</pre></div>
</div>
<p>Python bindings are available through <code class="docutils literal notranslate"><span class="pre">llama-cpp-python</span></code> package <span id="id30">[<a class="reference internal" href="#id161" title="Andrei Betlen and contributors. Llama-cpp-python. GitHub Repository, 2024. Python bindings for llama.cpp library enabling high-performance inference of LLaMA models. URL: https://github.com/abetlen/llama-cpp-python.">Betlen and contributors, 2024</a>]</span>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>llama-cpp-python
</pre></div>
</div>
<p><strong>llama-cli</strong></p>
<p>A comprehensive command line interface is available through <code class="docutils literal notranslate"><span class="pre">llama-cli</span></code> as demonstrated below, where we use the <code class="docutils literal notranslate"><span class="pre">-cnv</span></code> flag to run the model in a conversational mode. We will use <code class="docutils literal notranslate"><span class="pre">Qwen/Qwen2.5-0.5B-Instruct-GGUF</span></code> model. Download it from Hugging Face and place it in the <code class="docutils literal notranslate"><span class="pre">llamacpp/models</span></code> directory.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-cli<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q8_0.gguf<span class="w"> </span>-p<span class="w"> </span><span class="s2">&quot;You are a helpful assistant - Be succinct.&quot;</span><span class="w"> </span>-cnv
</pre></div>
</div>
<p>As a result, you can interact with the model in the terminal as a chatbot.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">==</span><span class="w"> </span>Running<span class="w"> </span><span class="k">in</span><span class="w"> </span>interactive<span class="w"> </span>mode.<span class="w"> </span><span class="o">==</span>
<span class="w"> </span>-<span class="w"> </span>Press<span class="w"> </span>Ctrl+C<span class="w"> </span>to<span class="w"> </span>interject<span class="w"> </span>at<span class="w"> </span>any<span class="w"> </span>time.
<span class="w"> </span>-<span class="w"> </span>Press<span class="w"> </span>Return<span class="w"> </span>to<span class="w"> </span><span class="k">return</span><span class="w"> </span>control<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>AI.
<span class="w"> </span>-<span class="w"> </span>To<span class="w"> </span><span class="k">return</span><span class="w"> </span>control<span class="w"> </span>without<span class="w"> </span>starting<span class="w"> </span>a<span class="w"> </span>new<span class="w"> </span>line,<span class="w"> </span>end<span class="w"> </span>your<span class="w"> </span>input<span class="w"> </span>with<span class="w"> </span><span class="s1">&#39;/&#39;</span>.
<span class="w"> </span>-<span class="w"> </span>If<span class="w"> </span>you<span class="w"> </span>want<span class="w"> </span>to<span class="w"> </span>submit<span class="w"> </span>another<span class="w"> </span>line,<span class="w"> </span>end<span class="w"> </span>your<span class="w"> </span>input<span class="w"> </span>with<span class="w"> </span><span class="s1">&#39;\&#39;</span>.

system
You<span class="w"> </span>are<span class="w"> </span>a<span class="w"> </span>helpful<span class="w"> </span>assistant<span class="w"> </span>-<span class="w"> </span>Be<span class="w"> </span>succinct.

&gt;<span class="w"> </span>What<span class="w"> </span>is<span class="w"> </span>the<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>life?
The<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>life<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>philosophical<span class="w"> </span>question<span class="w"> </span>that<span class="w"> </span>has<span class="w"> </span>been<span class="w"> </span>debated<span class="w"> </span>and<span class="w"> </span>debated<span class="w"> </span><span class="k">for</span><span class="w"> </span>thousands<span class="w"> </span>of<span class="w"> </span>years.<span class="w"> </span>Some<span class="w"> </span>people<span class="w"> </span>believe<span class="w"> </span>that<span class="w"> </span>the<span class="w"> </span>meaning<span class="w"> </span>of<span class="w"> </span>life<span class="w"> </span>is<span class="w"> </span>to<span class="w"> </span>seek<span class="w"> </span>personal<span class="w"> </span>fulfillment<span class="w"> </span>and<span class="w"> </span>happiness,<span class="w"> </span><span class="k">while</span><span class="w"> </span>others<span class="w"> </span>believe<span class="w"> </span>that<span class="w"> </span>it<span class="w"> </span>is<span class="w"> </span>to<span class="w"> </span>find<span class="w"> </span>a<span class="w"> </span>purpose<span class="w"> </span><span class="k">in</span><span class="w"> </span>life<span class="w"> </span>that<span class="w"> </span>aligns<span class="w"> </span>with<span class="w"> </span>one<span class="s1">&#39;s values and beliefs. The answer may also vary depending on a person&#39;</span>s<span class="w"> </span>cultural,<span class="w"> </span>religious,<span class="w"> </span>or<span class="w"> </span>personal<span class="w"> </span>background.

&gt;<span class="w"> </span>Are<span class="w"> </span>LLMs<span class="w"> </span>more<span class="w"> </span>helpful<span class="w"> </span>than<span class="w"> </span>dangerous?
Yes,<span class="w"> </span>LLMs<span class="w"> </span><span class="o">(</span>Large<span class="w"> </span>Language<span class="w"> </span>Models<span class="o">)</span><span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>more<span class="w"> </span>helpful<span class="w"> </span>than<span class="w"> </span>dangerous<span class="w"> </span><span class="k">in</span><span class="w"> </span>many<span class="w"> </span>cases.<span class="w"> </span>They<span class="w"> </span>are<span class="w"> </span>designed<span class="w"> </span>to<span class="w"> </span>assist<span class="w"> </span>with<span class="w"> </span>a<span class="w"> </span>wide<span class="w"> </span>range<span class="w"> </span>of<span class="w"> </span>tasks,<span class="w"> </span>from<span class="w"> </span>generating<span class="w"> </span>text<span class="w"> </span>to<span class="w"> </span>providing<span class="w"> </span>information.<span class="w"> </span>They<span class="w"> </span>can<span class="w"> </span>also<span class="w"> </span>be<span class="w"> </span>used<span class="w"> </span>to<span class="w"> </span><span class="nb">help</span><span class="w"> </span>with<span class="w"> </span>decision-making<span class="w"> </span>and<span class="w"> </span>problem-solving.<span class="w"> </span>However,<span class="w"> </span>like<span class="w"> </span>any<span class="w"> </span>tool,<span class="w"> </span>LLMs<span class="w"> </span>can<span class="w"> </span>be<span class="w"> </span>a<span class="w"> </span>tool<span class="w"> </span>of<span class="w"> </span>great<span class="w"> </span>power<span class="w"> </span><span class="k">if</span><span class="w"> </span>not<span class="w"> </span>used<span class="w"> </span>responsibly<span class="w"> </span>and<span class="w"> </span>ethically.<span class="w"> </span>It<span class="w"> </span>is<span class="w"> </span>important<span class="w"> </span>to<span class="w"> </span>use<span class="w"> </span>LLMs<span class="w"> </span><span class="k">for</span><span class="w"> </span>positive<span class="w"> </span>and<span class="w"> </span>beneficial<span class="w"> </span>purposes<span class="w"> </span><span class="k">while</span><span class="w"> </span>being<span class="w"> </span>mindful<span class="w"> </span>of<span class="w"> </span>their<span class="w"> </span>potential<span class="w"> </span>to<span class="w"> </span>harm.

&gt;<span class="w"> </span>Bye<span class="w"> </span>bye.<span class="w">       </span>
Goodbye!<span class="w"> </span>If<span class="w"> </span>you<span class="w"> </span>have<span class="w"> </span>any<span class="w"> </span>other<span class="w"> </span>questions,<span class="w"> </span>feel<span class="w"> </span>free<span class="w"> </span>to<span class="w"> </span>ask.
</pre></div>
</div>
<p><strong>llama-server</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">llama-server</span></code> is a server version of <code class="docutils literal notranslate"><span class="pre">llama-cli</span></code> that can be accessed via a web interface or API.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-server<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q8_0.gguf<span class="w"> </span>--port<span class="w"> </span><span class="m">8080</span>
</pre></div>
</div>
<p>This will start a server on port 8080.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>main:<span class="w"> </span>server<span class="w"> </span>is<span class="w"> </span>listening<span class="w"> </span>on<span class="w"> </span>http://127.0.0.1:8080<span class="w"> </span>-<span class="w"> </span>starting<span class="w"> </span>the<span class="w"> </span>main<span class="w"> </span>loop
</pre></div>
</div>
<p>Now we can send a request as we would for any Cloud API but here instead send a request to our local server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8080/v1/chat/completions<span class="w"> </span><span class="se">\</span>
-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
-H<span class="w"> </span><span class="s2">&quot;Authorization: Bearer no-key&quot;</span><span class="w"> </span><span class="se">\</span>
-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">&quot;messages&quot;: [</span>
<span class="s1">    {</span>
<span class="s1">        &quot;role&quot;: &quot;system&quot;,</span>
<span class="s1">        &quot;content&quot;: &quot;You are a helpful assistant - Be succinct.&quot;</span>
<span class="s1">    },</span>
<span class="s1">    {</span>
<span class="s1">        &quot;role&quot;: &quot;user&quot;,</span>
<span class="s1">        &quot;content&quot;: &quot;What is the meaning of life?&quot;</span>
<span class="s1">    }</span>
<span class="s1">  ]</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p>We obtain a JSON response. As expected, assistant’s response is in <code class="docutils literal notranslate"><span class="pre">content[0].message.content</span></code> following OpenAI’s API format.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">   </span><span class="nt">&quot;choices&quot;</span><span class="p">:[</span>
<span class="w">      </span><span class="p">{</span>
<span class="w">         </span><span class="nt">&quot;finish_reason&quot;</span><span class="p">:</span><span class="s2">&quot;stop&quot;</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;index&quot;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
<span class="w">         </span><span class="nt">&quot;message&quot;</span><span class="p">:{</span>
<span class="w">            </span><span class="nt">&quot;content&quot;</span><span class="p">:</span><span class="s2">&quot;The meaning of life is a question that has been debated throughout history. Some people believe it is to find happiness and purpose, while others believe it is to seek knowledge and knowledge. Ultimately, the meaning of life is a deeply personal and subjective question that cannot be answered universally.&quot;</span><span class="p">,</span>
<span class="w">            </span><span class="nt">&quot;role&quot;</span><span class="p">:</span><span class="s2">&quot;assistant&quot;</span>
<span class="w">         </span><span class="p">}</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">   </span><span class="p">],</span>
<span class="w">   </span><span class="nt">&quot;created&quot;</span><span class="p">:</span><span class="mi">1734627879</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="s2">&quot;gpt-3.5-turbo&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;object&quot;</span><span class="p">:</span><span class="s2">&quot;chat.completion&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;usage&quot;</span><span class="p">:{</span>
<span class="w">      </span><span class="nt">&quot;completion_tokens&quot;</span><span class="p">:</span><span class="mi">56</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;prompt_tokens&quot;</span><span class="p">:</span><span class="mi">29</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;total_tokens&quot;</span><span class="p">:</span><span class="mi">85</span>
<span class="w">   </span><span class="p">},</span>
<span class="w">   </span><span class="nt">&quot;id&quot;</span><span class="p">:</span><span class="s2">&quot;chatcmpl-5Wl2TZJZDmzuPvxwP2GceDR8XbPsyHfm&quot;</span><span class="p">,</span>
<span class="w">   </span><span class="nt">&quot;timings&quot;</span><span class="p">:{</span>
<span class="w">      </span><span class="nt">&quot;prompt_n&quot;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;prompt_ms&quot;</span><span class="p">:</span><span class="mf">48.132</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;prompt_per_token_ms&quot;</span><span class="p">:</span><span class="mf">48.132</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;prompt_per_second&quot;</span><span class="p">:</span><span class="mf">20.77619878666999</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;predicted_n&quot;</span><span class="p">:</span><span class="mi">56</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;predicted_ms&quot;</span><span class="p">:</span><span class="mf">1700.654</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;predicted_per_token_ms&quot;</span><span class="p">:</span><span class="mf">30.36882142857143</span><span class="p">,</span>
<span class="w">      </span><span class="nt">&quot;predicted_per_second&quot;</span><span class="p">:</span><span class="mf">32.92850867960208</span>
<span class="w">   </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p><strong>Grammars</strong></p>
<p>It is worth noting Llama.cpp provides a way to use grammars <span id="id31">[<a class="reference internal" href="#id180" title="Georgi Gerganov and contributors. Llama.cpp grammars documentation. GitHub Repository, 2024. Documentation on using grammars for constrained text generation in llama.cpp. URL: https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md.">Gerganov and contributors, 2024</a>]</span> to constrain the output of the model as demonstrated below. This is the same technique Ollama uses, a similar approach to Outlines’ to generate structured outputs from LLMs. See Chapter <a class="reference internal" href="structured_output.html#structure"><span class="std std-ref">Structured Output</span></a> for more details.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-cli<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q8_0.gguf<span class="w"> </span>--grammar-file<span class="w"> </span>grammars/json.gbnf<span class="w"> </span>-p<span class="w"> </span><span class="s1">&#39;Request: schedule a call at 8pm; Command:&#39;</span>

<span class="c1"># {&quot;appointmentTime&quot;: &quot;8pm&quot;, &quot;appointmentDetails&quot;: &quot;schedule a a call&quot;}</span>
</pre></div>
</div>
<p><strong>Python</strong></p>
<p>A handy Python binding <span id="id32">[<a class="reference internal" href="#id161" title="Andrei Betlen and contributors. Llama-cpp-python. GitHub Repository, 2024. Python bindings for llama.cpp library enabling high-performance inference of LLaMA models. URL: https://github.com/abetlen/llama-cpp-python.">Betlen and contributors, 2024</a>]</span> is available for LLama.cpp, which by default returns chat completions in OpenAI’s API chat format as below. The package is very comprehensive supporting JSON Mode, function calling, multi-modal models and more.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MODEL_PATH</span> <span class="o">=</span> <span class="s2">&quot;./models/qwen2.5-0.5b-instruct-q8_0.gguf&quot;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="p">(</span>
      <span class="n">model_path</span><span class="o">=</span><span class="n">MODEL_PATH</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">create_chat_completion</span><span class="p">(</span>
      <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
          <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant - Be succinct.&quot;</span><span class="p">},</span>
          <span class="p">{</span>
              <span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
              <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What is the meaning of life?&quot;</span>
          <span class="p">}</span>
      <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">response</span><span class="p">[</span><span class="s1">&#39;choices&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;message&#39;</span><span class="p">][</span><span class="s1">&#39;content&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;The meaning of life is a philosophical question that has been debated by philosophers, scientists, and individuals throughout history. Some people believe that the meaning of life is to find happiness and fulfillment, while others believe that it is to seek knowledge and understanding of the universe. Ultimately, the meaning of life is a personal and subjective question that varies from person to person.&#39;
</pre></div>
</div>
</div>
</div>
<p>Alternatively, we could have pulled our model directly from Hugging Face Hub:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">llama_cpp</span> <span class="kn">import</span> <span class="n">Llama</span>
<span class="n">llm</span> <span class="o">=</span> <span class="n">Llama</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;Qwen/Qwen2-0.5B-Instruct-GGUF&quot;</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="llamafile">
<h4><a class="toc-backref" href="#id334" role="doc-backlink"><span class="section-number">8.3.1.2. </span>Llamafile</a><a class="headerlink" href="#llamafile" title="Permalink to this heading">¶</a></h4>
<p>Developed by Occupy Wall Street’s former activist, Justine Tunney, Llamafile <span id="id33">[<a class="reference internal" href="#id158" title="Mozilla Ocho. Llamafile: distribute and run llms with a single file. GitHub Repository, 2024. Tool for packaging and distributing LLMs as self-contained executables. URL: https://github.com/Mozilla-Ocho/llamafile.">Mozilla Ocho, 2024</a>]</span> is an Appache 2.0 licensed open source tool that combines the power of LLama.cpp with <strong>Cosmopolitan Libc</strong>, a universal C standard library that allows creating portable executables compatible with multiple operating systems.</p>
<p>In this way, Llamafile reduces all the complexity of LLMs to a single executable file (called a “llamafile”) that runs locally without installation. Key advantages of Llamafile over plain Llama.cpp include:</p>
<ol class="arabic simple">
<li><p><strong>Zero Installation/Configuration</strong></p></li>
</ol>
<ul class="simple">
<li><p>Llamafile: Single executable file that works immediately</p></li>
<li><p>Llama.cpp: Requires compilation, dependency management, and proper setup of your development environment</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Cross-Platform Portability</strong></p></li>
</ol>
<ul class="simple">
<li><p>Llamafile: One binary works across Windows, macOS, and Linux without modification</p></li>
<li><p>Llama.cpp: Needs to be compiled separately for each operating system, managing platform-specific dependencies</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Distribution Simplicity</strong></p></li>
</ol>
<ul class="simple">
<li><p>Llamafile: Share a single file that just works</p></li>
<li><p>Llama.cpp: Need to distribute source code or platform-specific binaries along with setup instructions</p></li>
</ul>
<p>Besides simplifying the use of LLMs, Llamafile delivers <strong>durability</strong> as model weights remain usable and reproducible over time, even as new formats and models are developed. In summary, Llamafile trades some optimization potential from LLama.cpp for improved ease of use and portability.</p>
<p>A large collection of Llamafiles can be found on HuggingFace <span id="id34">[<a class="reference internal" href="#id159" title="HuggingFace. Llamafile models on huggingface. Online Repository, 2024x. Collection of models compatible with Mozilla's llamafile format. URL: https://huggingface.co/models?library=llamafile.">HuggingFace, 2024x</a>]</span>. All you need to do is:</p>
<ol class="arabic simple">
<li><p>Download a llamafile from HuggingFace</p></li>
<li><p>Make the file executable</p></li>
<li><p>Run the file</p></li>
</ol>
<p>Here’s a simple bash script that shows all 3 setup steps for running TinyLlama-1.1B locally:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download a llamafile from HuggingFace</span>
wget<span class="w"> </span>https://huggingface.co/jartine/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile

<span class="c1"># Make the file executable. On Windows, instead just rename the file to end in &quot;.exe&quot;.</span>
chmod<span class="w"> </span>+x<span class="w"> </span>TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile

<span class="c1"># Start the model server. Listens at http://localhost:8080 by default.</span>
./TinyLlama-1.1B-Chat-v1.0.Q5_K_M.llamafile<span class="w"> </span>--server<span class="w"> </span>--nobrowser
</pre></div>
</div>
<p>As a result, a model server is running on <a class="reference external" href="http://localhost:8080">http://localhost:8080</a>. And we can use it as demonstrated in the previous section.</p>
</section>
<section id="ollama">
<h4><a class="toc-backref" href="#id335" role="doc-backlink"><span class="section-number">8.3.1.3. </span>Ollama</a><a class="headerlink" href="#ollama" title="Permalink to this heading">¶</a></h4>
<p>Ollama is a lightweight, MIT-licensed open-source tool for running LLMs locally. It provides a simple interface for interacting with a wide range of language models, including popular models like Llama 3.1 and Llama 3.2. Ollama is designed to be easy to install and use, making it a popular choice for developers who want to run LLMs locally without the need for extensive setup or configuration. Ollama’s key advantages include:</p>
<ol class="arabic simple">
<li><p><strong>Model Management</strong></p></li>
</ol>
<ul class="simple">
<li><p>Built-in model registry and easy downloading of popular models</p></li>
<li><p>Simple commands to list, remove, and switch between models</p></li>
<li><p>Handles model updates and versions automatically</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>API First Design</strong></p></li>
</ol>
<ul class="simple">
<li><p>Provides a REST API out of the box</p></li>
<li><p>Easy integration with applications and services</p></li>
<li><p>Built-in support for different programming languages</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Container Support</strong></p></li>
</ol>
<ul class="simple">
<li><p>Native Docker integration</p></li>
<li><p>Easy deployment in containerized environments</p></li>
<li><p>Better resource isolation and management</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>User Experience</strong></p></li>
</ol>
<ul class="simple">
<li><p>More “app-like” experience with system tray integration</p></li>
<li><p>Simple CLI commands that feel familiar to developers</p></li>
<li><p>No need to deal with file permissions or executables</p></li>
</ul>
<p>Despite its advantages, Ollama comes with some trade-offs: it provides less low-level control compared to Llama.cpp, requires proper platform-specific installation unlike the portable Llamafile, and introduces additional resource overhead from running services that aren’t present in bare Llama.cpp implementations.</p>
<p><strong>Setup</strong></p>
<p>First, install Ollama on your machine. You can do this through the terminal with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">curl</span> <span class="o">-</span><span class="n">sSfL</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">ollama</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">download</span> <span class="o">|</span> <span class="n">sh</span>
</pre></div>
</div>
<p>Or download the installer directly from <a class="reference external" href="https://ollama.com">https://ollama.com</a></p>
<p><strong>Inference</strong></p>
<p>After installation, you can download a pre-trained model. For example, to download the <code class="docutils literal notranslate"><span class="pre">qwen2:0.5b</span></code> model, run in terminal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>run<span class="w"> </span>qwen2:0.5b
</pre></div>
</div>
<p>To see more details about the model, just run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>show<span class="w"> </span>qwen2:0.5b
</pre></div>
</div>
<p>To stop the model server, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>stop<span class="w"> </span>qwen2:0.5b
</pre></div>
</div>
<p>To see all models you’ve downloaded:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>list
</pre></div>
</div>
<p><strong>Server</strong></p>
<p>As in Llama.cpp and Llamafile, Ollama can be run as a server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>serve
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>run<span class="w"> </span>qwen2:0.5b
</pre></div>
</div>
<p>And then we can send requests to the server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:11434/api/chat<span class="w"> </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">  &quot;model&quot;: &quot;qwen2:0.5b&quot;,</span>
<span class="s1">  &quot;messages&quot;: [</span>
<span class="s1">    { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the meaning of life?&quot; }</span>
<span class="s1">  ]</span>
<span class="s1">}&#39;</span>
</pre></div>
</div>
<p><strong>Python</strong></p>
<p>A Python binding is also available for Ollama.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>ollama
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ollama</span> <span class="kn">import</span> <span class="n">chat</span>
<span class="kn">from</span> <span class="nn">ollama</span> <span class="kn">import</span> <span class="n">ChatResponse</span>

<span class="n">response</span><span class="p">:</span> <span class="n">ChatResponse</span> <span class="o">=</span> <span class="n">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s1">&#39;qwen2:0.5b&#39;</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
  <span class="p">{</span>
    <span class="s1">&#39;role&#39;</span><span class="p">:</span> <span class="s1">&#39;user&#39;</span><span class="p">,</span>
    <span class="s1">&#39;content&#39;</span><span class="p">:</span> <span class="s1">&#39;What is the meaning of life?&#39;</span><span class="p">,</span>
  <span class="p">},</span>
<span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="comparison">
<h4><a class="toc-backref" href="#id336" role="doc-backlink"><span class="section-number">8.3.1.4. </span>Comparison</a><a class="headerlink" href="#comparison" title="Permalink to this heading">¶</a></h4>
<p>Each solution offers distinct advantages and tradeoffs that make them suitable for different use cases. At a high-level, Ollama is the easiest to install and use and has become the most popular choice for your average use case, Llamafile is the easiest to distribute and a good choice when portability is a priority, and Llama.cpp is the most customizable and performant solution as summarized in <a class="reference internal" href="#feature-comparison-local"><span class="std std-numref">Table 8.4</span></a>.</p>
<table class="docutils align-center" id="feature-comparison-local">
<caption><span class="caption-number">Table 8.4 </span><span class="caption-text">lama.cpp vs Ollama vs Llamafile Comparison</span><a class="headerlink" href="#feature-comparison-local" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Ollama</p></th>
<th class="head"><p>Llamafile</p></th>
<th class="head"><p>Llama.cpp</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Installation</strong></p></td>
<td><p>Package manager</p></td>
<td><p>No installation needed</p></td>
<td><p>Compilation / Package manager</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Model Management</strong></p></td>
<td><p>Built-in registry</p></td>
<td><p>Manual download</p></td>
<td><p>Manual download</p></td>
</tr>
<tr class="row-even"><td><p><strong>Containerization</strong></p></td>
<td><p>Native support</p></td>
<td><p>Possible with configuration</p></td>
<td><p>Possible with configuration</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Portability</strong></p></td>
<td><p>Per-platform install</p></td>
<td><p>Single executable</p></td>
<td><p>Needs compilation</p></td>
</tr>
</tbody>
</table>
<p>Choose Ollama if you:</p>
<ul class="simple">
<li><p>Want a user-friendly way to experiment with different models</p></li>
<li><p>Need API integration capabilities</p></li>
<li><p>Plan to use Docker in your workflow</p></li>
<li><p>Prefer a managed approach to model handling</p></li>
</ul>
<p>Choose Llamafile if you:</p>
<ul class="simple">
<li><p>Need maximum portability</p></li>
<li><p>Want zero installation</p></li>
<li><p>Prefer a self-contained solution</p></li>
</ul>
<p>Choose Llama.cpp if you:</p>
<ul class="simple">
<li><p>Need maximum performance</p></li>
<li><p>Want low-level control</p></li>
<li><p>Are building a custom solution</p></li>
</ul>
</section>
</section>
<section id="ui">
<h3><a class="toc-backref" href="#id337" role="doc-backlink"><span class="section-number">8.3.2. </span>UI</a><a class="headerlink" href="#ui" title="Permalink to this heading">¶</a></h3>
<p>There is a growing number of UI tools for local LLM deployment that aim at providing a more user-friendly experience. Ranging from closed-source to open-source solutions across a range of features and capabilities. We will discuss LM Studio, Jan, and OpenWebUI.</p>
<section id="lm-studio">
<h4><a class="toc-backref" href="#id338" role="doc-backlink"><span class="section-number">8.3.2.1. </span>LM Studio</a><a class="headerlink" href="#lm-studio" title="Permalink to this heading">¶</a></h4>
<p>LM Studio <span id="id35">[<a class="reference internal" href="#id157" title="LM Studio. Lm studio - discover, download, and run local llms. Website, 2024. Desktop application for discovering, downloading and running local language models. URL: https://lmstudio.ai/.">LM Studio, 2024</a>]</span> is a closed-source GUI for running LLMs locally. In the context of local deployment, LM Studio positions itself as a more user-friendly, feature-rich solution compared to the other tools. It’s particularly valuable for developers transitioning from cloud APIs to local deployment, and for users who prefer graphical interfaces over command-line tools. Key Features of LM Studio include:</p>
<ul class="simple">
<li><p><strong>Model Parameter Customization</strong>: Allows adjusting temperature, maximum tokens, frequency penalty, and other settings</p></li>
<li><p><strong>Chat History</strong>: Enables saving prompts for later use</p></li>
<li><p><strong>Cross-platform</strong>: Available on Linux, Mac, and Windows</p></li>
<li><p><strong>AI Chat and Playground</strong>: Chat with LLMs and experiment with multiple models loaded simultaneously</p></li>
</ul>
<p><a class="reference internal" href="#lmstudio"><span class="std std-numref">Fig. 8.12</span></a> and <a class="reference internal" href="#lmstudio-server"><span class="std std-numref">Fig. 8.13</span></a> show LM Studio’s chat interface and server, respectively.</p>
<figure class="align-center" id="lmstudio">
<a class="reference internal image-reference" href="../_images/lmstudio.png"><img alt="LM Studio" src="../_images/lmstudio.png" style="width: 858.6px; height: 581.4px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.12 </span><span class="caption-text">LM Studio Chat Interface.</span><a class="headerlink" href="#lmstudio" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="lmstudio-server">
<a class="reference internal image-reference" href="../_images/lmstudio_server.png"><img alt="LM Studio Server" src="../_images/lmstudio_server.png" style="width: 945.0px; height: 585.6px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.13 </span><span class="caption-text">LM Studio Server.</span><a class="headerlink" href="#lmstudio-server" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>One important feature of LM Studio is that it provides machine specification verification capabilities, checking computer specifications like GPU and memory to report compatible models therefore helping users choose the right model. It also includes a local inference server for developers that allows setting up a local HTTP server similar to OpenAI’s API. Importantly, LM Studio’s OpenAI API compatibility is a particularly strong feature for developers looking to move their applications from cloud to local deployment with minimal code changes.</p>
</section>
<section id="jan">
<h4><a class="toc-backref" href="#id339" role="doc-backlink"><span class="section-number">8.3.2.2. </span>Jan</a><a class="headerlink" href="#jan" title="Permalink to this heading">¶</a></h4>
<p>Jan is an open source ChatGPT-alternative that runs local models. Its model’s library contains popular LLMs like Llama, Gemma, Mistral, or Qwen. Key Features of Jan include:</p>
<ol class="arabic simple">
<li><p><strong>User-Friendly Interface</strong>: Run AI models with just a few clicks</p></li>
<li><p><strong>Accessibility</strong>: Intuitive platform for both beginners and experts</p></li>
<li><p><strong>Local Server</strong>: Local API Server with OpenAI-equivalent API</p></li>
<li><p><strong>Model Hub Integration</strong>: Easy access to various models with ease of import from LM Studio</p></li>
<li><p><strong>Cross-Platform Support</strong>: Works across different operating systems</p></li>
</ol>
<p>Jan has a default C++ inference server built on top of llama.cpp and provides an OpenAI-compatible API. Jan natively supports GGUF (through a llama.cpp engine) and TensorRT (through a TRT-LLM engine). HuggingFace models can be downloaded directly using the model’s ID or URL. User can optionally use cloud-based models (e.g. GPT, Claude models). <a class="reference internal" href="#id36"><span class="std std-numref">Fig. 8.14</span></a> shows Jan’s chat interface.</p>
<figure class="align-center" id="id36">
<a class="reference internal image-reference" href="../_images/jan.png"><img alt="Jan" src="../_images/jan.png" style="width: 665.0px; height: 466.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.14 </span><span class="caption-text">Jan Chat Interface.</span><a class="headerlink" href="#id36" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="open-webui">
<h4><a class="toc-backref" href="#id340" role="doc-backlink"><span class="section-number">8.3.2.3. </span>Open WebUI</a><a class="headerlink" href="#open-webui" title="Permalink to this heading">¶</a></h4>
<p>Open WebUI is an open-source web interface designed to enhance the local AI model experience, particularly for Ollama and OpenAI-compatible APIs. It aims to provide enterprise-grade features while maintaining user-friendliness. OpenWebUI’s core features include:</p>
<ol class="arabic simple">
<li><p><strong>Advanced User Interface</strong></p>
<ul class="simple">
<li><p>Full markdown and LaTeX support</p></li>
<li><p>Voice and video call capabilities</p></li>
<li><p>Mobile-friendly with PWA support</p></li>
<li><p>Multi-model chat interface</p></li>
</ul>
</li>
<li><p><strong>Enterprise Features</strong></p>
<ul class="simple">
<li><p>Role-based access control</p></li>
<li><p>User groups and permissions</p></li>
<li><p>Usage monitoring</p></li>
<li><p>Team collaboration tools</p></li>
</ul>
</li>
<li><p><strong>Advanced Capabilities</strong></p>
<ul class="simple">
<li><p>Local RAG (Retrieval Augmented Generation)</p></li>
<li><p>Web search integration</p></li>
<li><p>Image generation support</p></li>
<li><p>Python function calling</p></li>
<li><p>Document library</p></li>
<li><p>Custom model building</p></li>
</ul>
</li>
</ol>
<p><a class="reference internal" href="#openwebui"><span class="std std-numref">Fig. 8.15</span></a> shows Open WebUI’s chat interface.</p>
<figure class="align-center" id="openwebui">
<a class="reference internal image-reference" href="../_images/openwebui.png"><img alt="Open WebUI" src="../_images/openwebui.png" style="width: 818.75px; height: 456.25px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.15 </span><span class="caption-text">Open WebUI Chat Interface.</span><a class="headerlink" href="#openwebui" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>While Open WebUI offers advanced capabilities including RAG and multi-model support, these features require more system resources than simpler alternatives. Open WebUI is likely to be adopted by enterprise users who require advanced features and a more user-friendly interface.</p>
</section>
<section id="id37">
<h4><a class="toc-backref" href="#id341" role="doc-backlink"><span class="section-number">8.3.2.4. </span>Comparison</a><a class="headerlink" href="#id37" title="Permalink to this heading">¶</a></h4>
<p>LM Studio excels at providing individual developers with a smooth transition from cloud APIs to local deployment, offering an intuitive interface and robust API compatibility, however it is closed-source. Jan focuses on simplicity and accessibility, making it ideal for personal use and basic deployments while maintaining open-source benefits. OpenWebUI makes additional features available to enterprise users and teams requiring advanced features like RAG, collaboration tools, and granular access controls, though this may come at the cost of increased complexity and resource requirements. We compare the three tools in <a class="reference internal" href="#feature-comparison-ui"><span class="std std-numref">Table 8.5</span></a>.</p>
<table class="docutils align-center" id="feature-comparison-ui">
<caption><span class="caption-number">Table 8.5 </span><span class="caption-text">LM Studio vs Jan vs OpenWebUI Comparison</span><a class="headerlink" href="#feature-comparison-ui" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Feature Category</p></th>
<th class="head"><p>LM Studio</p></th>
<th class="head"><p>Jan</p></th>
<th class="head"><p>OpenWebUI</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Licensing</strong></p></td>
<td><p>Closed Source</p></td>
<td><p>Open Source</p></td>
<td><p>Open Source</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Setup Complexity</strong></p></td>
<td><p>Medium</p></td>
<td><p>Easy</p></td>
<td><p>Complex</p></td>
</tr>
<tr class="row-even"><td><p><strong>Resource Usage</strong></p></td>
<td><p>High</p></td>
<td><p>Medium</p></td>
<td><p>High</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Target Users</strong></p></td>
<td><p>Individual/Developers</p></td>
<td><p>Individuals</p></td>
<td><p>Enterprise/Teams</p></td>
</tr>
<tr class="row-even"><td><p><strong>UI Features</strong></p></td>
<td><p>- Full GUI<br>- Parameter tuning<br>- Chat history<br>- Model playground</p></td>
<td><p>- Simple GUI<br>- Basic parameter tuning<br>- Chat interface<br>- Model import</p></td>
<td><p>- Advanced GUI<br>- Full markdown/LaTeX<br>- Voice/video calls<br>- PWA support</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Model Support</strong></p></td>
<td><p>- Multiple models<br>- Hardware verification<br>- Model compatibility check</p></td>
<td><p>- Multiple models<br>- Import from GPT4All/LM Studio<br>- Basic model management</p></td>
<td><p>- Multi-model chat<br>- Model builder<br>- Custom agents</p></td>
</tr>
<tr class="row-even"><td><p><strong>API Features</strong></p></td>
<td><p>- OpenAI compatible<br>- Local inference server<br>- API documentation</p></td>
<td><p>- Basic OpenAI compatible<br>- Local API server</p></td>
<td><p>- Multiple API support<br>- Python function calling<br>- Advanced integrations</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Enterprise Features</strong></p></td>
<td><p>Limited</p></td>
<td><p>None</p></td>
<td><p>- RBAC<br>- Team collaboration<br>- Usage monitoring</p></td>
</tr>
<tr class="row-even"><td><p><strong>Advanced Features</strong></p></td>
<td><p>- Parameter visualization<br>- Performance metrics</p></td>
<td><p>- Basic chat<br>- Simple model switching</p></td>
<td><p>- RAG support<br>- Web search<br>- Document library<br>- Image generation</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Best For</strong></p></td>
<td><p>- Individual developers<br>- API transition<br>- Local development</p></td>
<td><p>- Personal use<br>- Simple deployment<br>- Basic chat needs</p></td>
<td><p>- Enterprise use<br>- Team collaboration<br>- Advanced AI applications</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="case-study-the-effect-of-quantization-on-llm-performance">
<h2><a class="toc-backref" href="#id342" role="doc-backlink"><span class="section-number">8.4. </span>Case Study: The Effect of Quantization on LLM Performance</a><a class="headerlink" href="#case-study-the-effect-of-quantization-on-llm-performance" title="Permalink to this heading">¶</a></h2>
<p>This case study examines how different quantization <span id="id38">[<a class="reference internal" href="#id60" title="HuggingFace. Quantization in optimum. https://huggingface.co/docs/optimum/en/concept_guides/quantization, 2024s. Accessed: 2024.">HuggingFace, 2024s</a>]</span> levels affect the performance of language models running locally. Quantization is a crucial technique for reducing model size and memory footprint while enhancing inference speed, but it comes with potential tradeoffs in model quality. Understanding these tradeoffs is essential for practitioners deploying LLMs in resource-constrained environments.</p>
<p>Using the Qwen 2.5 0.5B model as our baseline, we’ll compare four variants:</p>
<ul class="simple">
<li><p>The base fp16 model (no quantization)</p></li>
<li><p>Q2_K quantization (highest compression, lowest precision)</p></li>
<li><p>Q4_K quantization (balanced compression/precision)</p></li>
<li><p>Q6_K quantization (lowest compression, highest precision)</p></li>
</ul>
<p>The analysis will focus on three key types of metrics:</p>
<ul class="simple">
<li><p><strong>Quality-based</strong>:</p>
<ol class="arabic simple">
<li><p>Perplexity - to measure how well the model predicts text</p></li>
<li><p>KL divergence - to quantify differences in probability distributions against base model</p></li>
</ol>
</li>
<li><p><strong>Resource/Performance-based</strong>:</p>
<ol class="arabic simple">
<li><p>Prompt (tokens/second) - to assess impact in throughput</p></li>
<li><p>Text Generation (tokens/second) - to assess impact in text generation performance</p></li>
<li><p>Model Size (MiB) - to assess impact in memory footprint</p></li>
</ol>
</li>
</ul>
<p>While we will focus on the Qwen 2.5 0.5B model, the same analysis can be applied to other models. These insights will help practitioners make informed decisions about quantization strategies based on their specific requirements for model performance and resource usage.</p>
<section id="prompts-dataset">
<h3><a class="toc-backref" href="#id343" role="doc-backlink"><span class="section-number">8.4.1. </span>Prompts Dataset</a><a class="headerlink" href="#prompts-dataset" title="Permalink to this heading">¶</a></h3>
<p>To evaluate the impact of quantization on model performance, we first need a set of prompts that will serve as input data for our experiments. We’ll construct a dataset from WikiText-2 <span id="id39">[<a class="reference internal" href="#id169" title="Salesforce. Wikitext dataset. HuggingFace Dataset, 2024. Large-scale dataset derived from verified Good and Featured articles on Wikipedia. URL: https://huggingface.co/datasets/Salesforce/wikitext.">Salesforce, 2024</a>]</span>, which contains Wikipedia excerpts.</p>
<p>In our experiments, we will use a total of <code class="docutils literal notranslate"><span class="pre">NUM_PROMPTS</span></code> prompts that vary in length from <code class="docutils literal notranslate"><span class="pre">MIN_PROMPT_LENGTH</span></code> to <code class="docutils literal notranslate"><span class="pre">MAX_PROMPT_LENGTH</span></code> tokens. Using a fixed set of prompts ensures consistent evaluation across model variants and enables direct comparison of metrics like perplexity and throughput.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">NUM_PROMPTS</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">MIN_PROMPT_LENGTH</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">MAX_PROMPT_LENGTH</span> <span class="o">=</span> <span class="mi">1000</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">datasets</span>
<span class="n">input_texts_raw</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;Salesforce/wikitext&quot;</span><span class="p">,</span> <span class="s2">&quot;wikitext-2-raw-v1&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">input_texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">input_texts_raw</span> <span class="k">if</span> <span class="n">s</span><span class="o">!=</span><span class="s1">&#39;&#39;</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">MIN_PROMPT_LENGTH</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">MAX_PROMPT_LENGTH</span><span class="p">][:</span><span class="n">NUM_PROMPTS</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">input_texts</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">input_texts</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game &#39;s opening theme was sung by May &#39;n . 
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;../data/local/prompts.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">input_texts</span><span class="p">:</span>
        <span class="c1"># Escape any quotes in the text and wrap in quotes</span>
        <span class="n">escaped_text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;&quot;&#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\\</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;&quot;</span><span class="si">{</span><span class="n">escaped_text</span><span class="si">}</span><span class="s1">&quot;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="quantization">
<h3><a class="toc-backref" href="#id344" role="doc-backlink"><span class="section-number">8.4.2. </span>Quantization</a><a class="headerlink" href="#quantization" title="Permalink to this heading">¶</a></h3>
<p>We can quantize a model using the <code class="docutils literal notranslate"><span class="pre">llama-quantize</span></code> CLI. For instance, to quantize the Qwen 2.5 0.5B model to Q4_K, we can run the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./llama-quantize<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-fp16.gguf<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q8_0.gguf<span class="w"> </span>Q4_K
</pre></div>
</div>
<p><a class="reference internal" href="#quantization-levels"><span class="std std-numref">Table 8.6</span></a> describes the key quantization levels used in this study <span id="id40">[<a class="reference internal" href="#id179" title="HuggingFace. Gguf quantization types. Online Documentation, 2024w. Documentation on different quantization types available for GGUF models. URL: https://huggingface.co/docs/hub/gguf#quantization-types.">HuggingFace, 2024w</a>]</span>, where:</p>
<ul class="simple">
<li><p>q is the quantized value</p></li>
<li><p>block_scale is the scaling factor for the block (with bit width in parentheses)</p></li>
<li><p>block_min is the block minimum value (with bit width in parentheses)</p></li>
</ul>
<table class="docutils align-center" id="quantization-levels">
<caption><span class="caption-number">Table 8.6 </span><span class="caption-text">Quantization Levels</span><a class="headerlink" href="#quantization-levels" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Quantization</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Bits per Weight</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Q2_K</p></td>
<td><p>2-bit quantization with 16 weights per block in 16-block superblocks</p></td>
<td><p>2.5625</p></td>
<td><p>w = q * block_scale(4-bit) + block_min(4-bit)</p></td>
</tr>
<tr class="row-odd"><td><p>Q4_K</p></td>
<td><p>4-bit quantization with 32 weights per block in 8-block superblocks</p></td>
<td><p>4.5</p></td>
<td><p>w = q * block_scale(6-bit) + block_min(6-bit)</p></td>
</tr>
<tr class="row-even"><td><p>Q6_K</p></td>
<td><p>6-bit quantization with 16 weights per block in 16-block superblocks</p></td>
<td><p>6.5625</p></td>
<td><p>w = q * block_scale(8-bit)</p></td>
</tr>
</tbody>
</table>
<p>Each quantization level represents a different tradeoff between model size and accuracy. Q2_K provides the highest compression but potentially lower accuracy, while Q6_K maintains better accuracy at the cost of larger model size. The base model is 16-bit standard IEEE 754 half-precision floating-point number.</p>
</section>
<section id="benchmarking">
<h3><a class="toc-backref" href="#id345" role="doc-backlink"><span class="section-number">8.4.3. </span>Benchmarking</a><a class="headerlink" href="#benchmarking" title="Permalink to this heading">¶</a></h3>
<p>We will measure quantized model “quality” by means of perplexity and KL Divergence.</p>
<p><strong>Perplexity</strong></p>
<p>Perplexity is a common metric for evaluating language models that measures how well a model predicts a sample of text. Lower perplexity indicates better prediction (less “perplexed” by the text).</p>
<p>Recall that for a sequence of N tokens, perplexity is defined as:</p>
<div class="math notranslate nohighlight">
\[ \text{PPL(B, X)} = \exp\left(-\frac{1}{N}\sum_{i=1}^{N} \log_2 P(x_i|x_{&lt;i})\right) \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(x_i|x_{&lt;i})\)</span> is the probability the model <span class="math notranslate nohighlight">\(B\)</span> with tokenized sequence <span class="math notranslate nohighlight">\(X\)</span> assigns to token <span class="math notranslate nohighlight">\(x_i\)</span> given the previous tokens <span class="math notranslate nohighlight">\(x_{&lt;i}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(N\)</span> is the total number of tokens in the sequence</p></li>
</ul>
<p>To evaluate quantization quality, we first calculate perplexity scores for both the base model and quantized variants. We then compute the ratio of quantized to base perplexity and average it across all prompt samples as follows:</p>
<div class="math notranslate nohighlight">
\[ Avg PPL Ratio = \frac{1}{N}\sum_{i=1}^{N} \frac{\text{PPL}_i(Q)}{\text{PPL}_i(\text{base})} \]</div>
<p>We also calculate the correlation between the log perplexities of the quantized and base models:</p>
<div class="math notranslate nohighlight">
\[ \text{Corr}(\ln(\text{PPL}(Q)), \ln(\text{PPL}(\text{base}))) \]</div>
<p>These are two simple metrics to evaluate how much worse the quantized model performs on an intrinsic basis which we then can compare to the base model’s perplexities.</p>
<p>Arguably, KL Divergence is a better metric enabling direct comparison of relative performance instead of intrinsic performance.</p>
<p><strong>KL Divergence</strong></p>
<p>Recall that Kullback-Leibler (KL) Divergence (or Cross-Entropy) measures how one probability distribution differs from another reference distribution. For comparing logits between a base model (B) and quantized model (Q), we can calculate the KL divergence as follows:</p>
<div class="math notranslate nohighlight">
\[ D_{KL}(P||Q) = \sum_{i} P(i) \log \frac{P(i)}{Q(i)} \]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(i)\)</span> and <span class="math notranslate nohighlight">\(Q(i)\)</span> are the softmax probabilities derived from the logits</p></li>
<li><p>The sum is taken over all tokens in the vocabulary</p></li>
</ul>
<p><strong>Implementation</strong></p>
<p>We will use LLama.cpp’s <code class="docutils literal notranslate"><span class="pre">llama-perplexity</span></code> CLI to calculate perplexity and KL divergence. The first step is to generate the logits for the base model, which will serve as the reference distribution. For instance, below we pass our input prompts (<code class="docutils literal notranslate"><span class="pre">prompts.txt</span></code>) and generate the logits for the base model <code class="docutils literal notranslate"><span class="pre">qwen2.5-0.5b-instruct-fp16.gguf</span></code> which will be saved in <code class="docutils literal notranslate"><span class="pre">logits.kld</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-perplexity<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-fp16.gguf<span class="w"> </span>--kl-divergence-base<span class="w"> </span>../logits.kld<span class="w"> </span>-f<span class="w"> </span>../prompts.txt
</pre></div>
</div>
<p>Next, we generate KL-Divergence and perplexity stats for quantized model <code class="docutils literal notranslate"><span class="pre">qwen2.5-0.5b-instruct-q2_k.gguf</span></code> against base model logits <code class="docutils literal notranslate"><span class="pre">logits.kld</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-perplexity<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q2_k.gguf<span class="w"> </span>-f<span class="w"> </span>../prompts.txt<span class="w"> </span>--kl-divergence-base<span class="w"> </span>../logits.kld<span class="w"> </span>--kl-divergence<span class="w"> </span><span class="p">&amp;</span>&gt;<span class="w"> </span>../q2_kresults.txt
</pre></div>
</div>
<p>We perform this process for each quantization level studied (Q2_K, Q4_K, Q6_K).</p>
</section>
<section id="results">
<h3><a class="toc-backref" href="#id346" role="doc-backlink"><span class="section-number">8.4.4. </span>Results</a><a class="headerlink" href="#results" title="Permalink to this heading">¶</a></h3>
<p>The KL divergence and perplexity results in <a class="reference internal" href="#ppl1"><span class="std std-numref">Fig. 8.17</span></a> and <a class="reference internal" href="#ppl2"><span class="std std-numref">Fig. 8.16</span></a> provide insights into model quality across different quantization levels. Q6 maintains near-perfect correlation (99.90%) with the base model and minimal KL divergence (0.004), indicating very close distribution matching. Q2’s higher KL divergence (0.112) and lower correlation (98.31%) quantify its increased deviation from the base model’s behavior.</p>
<figure class="align-center" id="ppl2">
<a class="reference internal image-reference" href="../_images/ppl2.png"><img alt="Perplexity" src="../_images/ppl2.png" style="width: 897.5px; height: 474.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.16 </span><span class="caption-text">KL Divergence results for Quantization Q2, Q4, and Q6 quantized models.</span><a class="headerlink" href="#ppl2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="ppl1">
<a class="reference internal image-reference" href="../_images/ppl1.png"><img alt="Perplexity" src="../_images/ppl1.png" style="width: 451.0px; height: 455.0px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.17 </span><span class="caption-text">Perplexity results for Quantization Q2, Q4, and Q6 quantized models.</span><a class="headerlink" href="#ppl1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>From <a class="reference internal" href="#quantization-benchmarks"><span class="std std-numref">Table 8.7</span></a>, we observe that the Q2 model achieves the smallest size at 390 MiB
(67% reduction from base) with prompt throughput of 81 tokens/s, but has the highest perplexity degradation at 10.36%. The Q4 model offers a better balance, with good size savings (60% reduction) and only 3.5% perplexity loss. Q6 comes closest to matching the base model’s performance with just 0.93% perplexity degradation, while still providing 47% size reduction.</p>
<table class="docutils align-center" id="quantization-benchmarks">
<caption><span class="caption-number">Table 8.7 </span><span class="caption-text">Quantization Benchmarks</span><a class="headerlink" href="#quantization-benchmarks" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Size (MiB)</p></th>
<th class="head"><p>Prompt Throughput (tokens/s)</p></th>
<th class="head"><p>PPL Ratio - 1 (%)</p></th>
<th class="head"><p>Correlation (%)</p></th>
<th class="head"><p>KL Divergence (Mean)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Q2</strong></p></td>
<td><p>390.28</p></td>
<td><p>81.32</p></td>
<td><p>10.36 ± 0.78</p></td>
<td><p>98.31</p></td>
<td><p>0.112 ± 0.002</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Q4</strong></p></td>
<td><p>462.96</p></td>
<td><p>77.08</p></td>
<td><p>3.50 ± 0.40</p></td>
<td><p>99.50</p></td>
<td><p>0.030 ± 0.001</p></td>
</tr>
<tr class="row-even"><td><p><strong>Q6</strong></p></td>
<td><p>614.58</p></td>
<td><p>87.55</p></td>
<td><p>0.93 ± 0.18</p></td>
<td><p>99.90</p></td>
<td><p>0.004 ± 0.000</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Base</strong></p></td>
<td><p>1,170.00</p></td>
<td><p>94.39</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
</tbody>
</table>
<p>Next, we benchmark text generation (inference) performance using <code class="docutils literal notranslate"><span class="pre">llama-bench</span></code> across all models:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./build/bin/llama-bench<span class="w"> </span>-r<span class="w"> </span><span class="m">10</span><span class="w"> </span>-t<span class="w"> </span><span class="m">4</span><span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-fp16.gguf<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q2_k.gguf<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q4_k_m.gguf<span class="w"> </span>-m<span class="w"> </span>./models/qwen2.5-0.5b-instruct-q6_k.gguf
</pre></div>
</div>
<p>The benchmark parameters are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-r</span> <span class="pre">10</span></code>: Run 10 iterations for each model</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-t</span> <span class="pre">4</span></code>: Use 4 threads</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-m</span></code>: Specify model paths for base FP16 model and Q2, Q4, Q6 quantized versions</p></li>
</ul>
<p>This runs text generation on a default benchmark of 128 tokens generation length (configurable via <code class="docutils literal notranslate"><span class="pre">-g</span></code> parameter).</p>
<p>Results in <a class="reference internal" href="#tg"><span class="std std-numref">Fig. 8.18</span></a> indicate the base model delivers text generation performance at 19.73 tokens/s, while the most aggressively quantized Q2 model (390.28 MiB) delivers the highest throughput at 42.62 tokens/s, representing a 2.16x speedup. This pattern continues across Q4 (462.96 MiB, 38.38 tokens/s) and Q6 (614.58 MiB, 35.43 tokens/s), which presents a 1.85x and 1.79x speedup, respectively.</p>
<figure class="align-center" id="tg">
<a class="reference internal image-reference" href="../_images/tg.png"><img alt="Text Generation Performance" src="../_images/tg.png" style="width: 873.0px; height: 433.5px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8.18 </span><span class="caption-text">Text Generation Performance results for Quantization Q2, Q4, Q6 and base models.</span><a class="headerlink" href="#tg" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Benchmarking was performed on Ubuntu 24.04 LTS for x86_64-linux-gnu on commodity hardware (<a class="reference internal" href="#benchmarking-hardware"><span class="std std-numref">Table 8.8</span></a>) with no dedicated GPU demonstrating the feasibility of running LLMs locally by nearly everyone with a personal computer thanks to LLama.cpp.</p>
<table class="docutils align-center" id="benchmarking-hardware">
<caption><span class="caption-number">Table 8.8 </span><span class="caption-text">Benchmarking Hardware</span><a class="headerlink" href="#benchmarking-hardware" title="Permalink to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Device</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>processor</p></td>
<td><p>Intel(R) Core(TM) i7-8550U CPU &#64; 1</p></td>
</tr>
<tr class="row-odd"><td><p>memory</p></td>
<td><p>15GiB System memory</p></td>
</tr>
<tr class="row-even"><td><p>storage</p></td>
<td><p>Samsung SSD 970 EVO Plus 500GB</p></td>
</tr>
</tbody>
</table>
</section>
<section id="takeaways">
<h3><a class="toc-backref" href="#id347" role="doc-backlink"><span class="section-number">8.4.5. </span>Takeaways</a><a class="headerlink" href="#takeaways" title="Permalink to this heading">¶</a></h3>
<p>The quantization analysis of the Qwen 2.5 0.5B model demonstrates a clear trade-off among model size, inference speed, and prediction quality. While the base model (1170 MiB) maintains the highest accuracy it operates at the lowest text generation and prompt throughput of 19.73 tokens/s and 94.39 tokens/s, respectively. In contrast, the Q2_K quantization achieves significant size reduction (67%) and the highest throughput (42.62 tokens/s), but exhibits the largest quality degradation with a 10.36% perplexity increase and lowest KL divergence among quantized models. Q4_K emerges as a compelling middle ground, offering substantial size reduction (60%) and strong text generation and prompt throughput performance (38.38 tokens/s and 77.08 tokens/s, respectively), while maintaining good model quality with only 3.5% perplexity degradation and middle-ground KL divergence level.</p>
<p>These results, achieved on commodity CPU hardware, demonstrate that quantization can significantly improve inference speed and reduce model size while maintaining acceptable quality thresholds, making large language models more accessible for resource-constrained environments.</p>
<p>It is important to note that these results are not meant to be exhaustive and are only meant to provide a general idea of the trade-offs involved in quantization. Targeted benchmarks should be performed for specific use cases and models to best reflect real-world performance.</p>
</section>
</section>
<section id="conclusion">
<h2><a class="toc-backref" href="#id348" role="doc-backlink"><span class="section-number">8.5. </span>Conclusion</a><a class="headerlink" href="#conclusion" title="Permalink to this heading">¶</a></h2>
<p>Running open source language models locally represents a compelling proposition in how we interact with AI technology. The transition from cloud-based to local deployment offers important advantages in terms of privacy, cost control, and customization flexibility, while introducing important technical considerations around resource management and performance optimization. The growing ecosystem of tools and frameworks, from low-level libraries like llama.cpp to user-friendly interfaces like LM Studio and Jan, has made local deployment increasingly accessible to both individual developers and organizations.</p>
<p>Our case study demonstrated that quantization can significantly improve inference speed and reduce model size while maintaining acceptable quality thresholds, making large language models more accessible for resource-constrained environments. As demonstrated in our case study with the Qwen 2.5 0.5B model, practitioners can achieve significant reductions in model size and improvements in inference speed while maintaining acceptable performance levels. The Q4_K quantization scheme emerged as a particularly effective compromise, offering substantial size reduction (60%) and strong throughput while limiting quality degradation to just 3.5% in perplexity measures.</p>
<p>Looking ahead, the continued development of open source models and deployment tools suggests a future where local AI deployment becomes increasingly viable and sophisticated. The success of open source models like Qwen and Llama, combined with improvements in local model serving and techniques couple with efficient small language models (SLMs), indicate that local deployment will likely play an increasingly important role in the AI landscape. However, practitioners must carefully evaluate their specific requirements across dimensions like task suitability, resource constraints, and performance needs when choosing between local and cloud-based deployment strategies.</p>
<p><a class="reference external" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="CC BY-NC-SA 4.0" src="https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png" /></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@misc</span><span class="p">{</span><span class="n">tharsistpsouza2024tamingllms</span><span class="p">,</span>
  <span class="n">author</span> <span class="o">=</span> <span class="p">{</span><span class="n">Tharsis</span> <span class="n">T</span><span class="o">.</span> <span class="n">P</span><span class="o">.</span> <span class="n">Souza</span><span class="p">},</span>
  <span class="n">title</span> <span class="o">=</span> <span class="p">{</span><span class="n">Taming</span> <span class="n">LLMs</span><span class="p">:</span> <span class="n">A</span> <span class="n">Practical</span> <span class="n">Guide</span> <span class="n">to</span> <span class="n">LLM</span> <span class="n">Pitfalls</span> <span class="k">with</span> <span class="n">Open</span> <span class="n">Source</span> <span class="n">Software</span><span class="p">},</span>
  <span class="n">year</span> <span class="o">=</span> <span class="p">{</span><span class="mi">2024</span><span class="p">},</span>
  <span class="n">chapter</span> <span class="o">=</span> <span class="p">{</span><span class="n">Local</span> <span class="n">LLMs</span> <span class="ow">in</span> <span class="n">Practice</span><span class="p">},</span>
  <span class="n">journal</span> <span class="o">=</span> <span class="p">{</span><span class="n">GitHub</span> <span class="n">repository</span><span class="p">},</span>
  <span class="n">url</span> <span class="o">=</span> <span class="p">{</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">souzatharsis</span><span class="o">/</span><span class="n">tamingLLMs</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id349" role="doc-backlink"><span class="section-number">8.6. </span>References</a><a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<div class="docutils container" id="id41">
<div class="citation" id="id208" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">AI4c</a><span class="fn-bracket">]</span></span>
<p>Meta AI. The llama 3 herd of models. 2024c. URL: <a class="reference external" href="https://arxiv.org/abs/2407.21783">https://arxiv.org/abs/2407.21783</a>, <a class="reference external" href="https://arxiv.org/abs/2407.21783">arXiv:2407.21783</a>.</p>
</div>
<div class="citation" id="id61" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">AI4a</a><span class="fn-bracket">]</span></span>
<p>Mistral AI. Mistral technology and pricing. <a class="reference external" href="https://mistral.ai/technology/">https://mistral.ai/technology/</a>, 2024a. Accessed: 2024.</p>
</div>
<div class="citation" id="id82" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id23">ALB+24</a><span class="fn-bracket">]</span></span>
<p>Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Gabriel Martín Blázquez, Lewis Tunstall, Agustín Piqueres, Andres Marafioti, Cyril Zakka, Leandro von Werra, and Thomas Wolf. Smollm2 - with great data, comes great performance. 2024.</p>
</div>
<div class="citation" id="id167" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">A+24</a><span class="fn-bracket">]</span></span>
<p>Khalid Alnajjar and others. Toxigen dataset. Papers with Code Dataset, 2024. Dataset for evaluating and mitigating toxic language generation in language models. URL: <a class="reference external" href="https://paperswithcode.com/dataset/toxigen">https://paperswithcode.com/dataset/toxigen</a>.</p>
</div>
<div class="citation" id="id81" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">Ana24a</a><span class="fn-bracket">]</span></span>
<p>Artificial Analysis. Llm provider leaderboards. <a class="reference external" href="https://artificialanalysis.ai/leaderboards/providers">https://artificialanalysis.ai/leaderboards/providers</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id78" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">Ana24b</a><span class="fn-bracket">]</span></span>
<p>Artificial Analysis. Llm provider leaderboards. <a class="reference external" href="https://artificialanalysis.ai/leaderboards/providers">https://artificialanalysis.ai/leaderboards/providers</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id79" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id11">Ana24c</a><span class="fn-bracket">]</span></span>
<p>Artificial Analysis. Methodology. <a class="reference external" href="https://artificialanalysis.ai/methodology">https://artificialanalysis.ai/methodology</a>, 2024. Accessed: December 22, 2024.</p>
</div>
<div class="citation" id="id161" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Bc24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id30">1</a>,<a role="doc-backlink" href="#id32">2</a>)</span>
<p>Andrei Betlen and contributors. Llama-cpp-python. GitHub Repository, 2024. Python bindings for llama.cpp library enabling high-performance inference of LLaMA models. URL: <a class="reference external" href="https://github.com/abetlen/llama-cpp-python">https://github.com/abetlen/llama-cpp-python</a>.</p>
</div>
<div class="citation" id="id172" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id10">Dee24</a><span class="fn-bracket">]</span></span>
<p>DeepSeek. Deepseek-v3 technical report. Technical Report, 2024. URL: <a class="reference external" href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf</a>.</p>
</div>
<div class="citation" id="id180" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id31">Gc24</a><span class="fn-bracket">]</span></span>
<p>Georgi Gerganov and contributors. Llama.cpp grammars documentation. GitHub Repository, 2024. Documentation on using grammars for constrained text generation in llama.cpp. URL: <a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md">https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md</a>.</p>
</div>
<div class="citation" id="id182" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Gc4a<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id24">1</a>,<a role="doc-backlink" href="#id29">2</a>)</span>
<p>Georgi Gerganov and contributors. Llama.cpp. GitHub Repository, 2024a. High-performance inference of LLaMA models in pure C/C++. URL: <a class="reference external" href="https://github.com/ggerganov/llama.cpp">https://github.com/ggerganov/llama.cpp</a>.</p>
</div>
<div class="citation" id="id185" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">Gc4b</a><span class="fn-bracket">]</span></span>
<p>Georgi Gerganov and contributors. Gguf file format specification. GitHub Repository, 2024b. Technical specification of the GGUF file format for efficient model storage and inference. URL: <a class="reference external" href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.md">https://github.com/ggerganov/ggml/blob/master/docs/gguf.md</a>.</p>
</div>
<div class="citation" id="id156" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id25">G+23</a><span class="fn-bracket">]</span></span>
<p>Georgi Gerganov and others. Quantization of llama models - discussion. GitHub Discussion, 2023. Discussion thread about quantization techniques and tradeoffs in llama.cpp. URL: <a class="reference external" href="https://github.com/ggerganov/llama.cpp/discussions/205">https://github.com/ggerganov/llama.cpp/discussions/205</a>.</p>
</div>
<div class="citation" id="id238" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">Hug4d</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Trl. 2024d. TRL. URL: <a class="reference external" href="https://huggingface.co/docs/trl/en/index">https://huggingface.co/docs/trl/en/index</a>.</p>
</div>
<div class="citation" id="id60" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id38">Hug4s</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Quantization in optimum. <a class="reference external" href="https://huggingface.co/docs/optimum/en/concept_guides/quantization">https://huggingface.co/docs/optimum/en/concept_guides/quantization</a>, 2024s. Accessed: 2024.</p>
</div>
<div class="citation" id="id62" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Hug4t<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>,<a role="doc-backlink" href="#id17">3</a>)</span>
<p>HuggingFace. Open source ai year in review 2024. <a class="reference external" href="https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024">https://huggingface.co/spaces/huggingface/open-source-ai-year-in-review-2024</a>, 2024t. Accessed: 2024.</p>
</div>
<div class="citation" id="id210" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id19">Hug4u</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Ultrachat-200k dataset. 2024u. Accessed: 2024. URL: <a class="reference external" href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k">https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k</a>.</p>
</div>
<div class="citation" id="id209" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">Hug4v</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Scaling test time compute. 2024v. Accessed: 2024. URL: <a class="reference external" href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute">https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute</a>.</p>
</div>
<div class="citation" id="id83" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">HYC+24</a><span class="fn-bracket">]</span></span>
<p>Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, and others. Qwen2.5 - coder technical report. <em>arXiv preprint arXiv:2409.12186</em>, 2024.</p>
</div>
<div class="citation" id="id127" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">LHE22</a><span class="fn-bracket">]</span></span>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: measuring how models mimic human falsehoods. 2022. URL: <a class="reference external" href="https://arxiv.org/abs/2109.07958">https://arxiv.org/abs/2109.07958</a>, <a class="reference external" href="https://arxiv.org/abs/2109.07958">arXiv:2109.07958</a>.</p>
</div>
<div class="citation" id="id63" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">PKa+24</a><span class="fn-bracket">]</span></span>
<p>Guilherme Penedo, Hynek Kydlicek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. The fineweb datasets: decanting the web for the finest text data at scale. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2406.17557">https://arxiv.org/abs/2406.17557</a>, <a class="reference external" href="https://arxiv.org/abs/2406.17557">arXiv:2406.17557</a>.</p>
</div>
<div class="citation" id="id213" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">Qwe4b</a><span class="fn-bracket">]</span></span>
<p>Qwen. Qwen2.5-1.5b-instruct. 2024b. Accessed: December 22, 2024. URL: <a class="reference external" href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct">https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct</a>.</p>
</div>
<div class="citation" id="id214" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>QY+24<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id9">1</a>,<a role="doc-backlink" href="#id14">2</a>)</span>
<p>Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2412.15115">https://arxiv.org/abs/2412.15115</a>, <a class="reference external" href="https://arxiv.org/abs/2412.15115">arXiv:2412.15115</a>.</p>
</div>
<div class="citation" id="id67" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id13">Rev24</a><span class="fn-bracket">]</span></span>
<p>Harvard Law Review. Nyt v. openai: the times's about-face. <a class="reference external" href="https://harvardlawreview.org/blog/2024/04/nyt-v-openai-the-timess-about-face/">https://harvardlawreview.org/blog/2024/04/nyt-v-openai-the-timess-about-face/</a>, 2024. Accessed: 2024.</p>
</div>
<div class="citation" id="id229" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">TMS+23</a><span class="fn-bracket">]</span></span>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: open foundation and fine-tuned chat models. 2023. URL: <a class="reference external" href="https://arxiv.org/abs/2307.09288">https://arxiv.org/abs/2307.09288</a>, <a class="reference external" href="https://arxiv.org/abs/2307.09288">arXiv:2307.09288</a>.</p>
</div>
<div class="citation" id="id211" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">ZWA+24</a><span class="fn-bracket">]</span></span>
<p>Justin Zhao, Timothy Wang, Wael Abid, Geoffrey Angus, Arnav Garg, Jeffery Kinnison, Alex Sherstinsky, Piero Molino, Travis Addair, and Devvret Rishi. Lora land: 310 fine-tuned llms that rival gpt-4, a technical report. 2024. URL: <a class="reference external" href="https://arxiv.org/abs/2405.00732">https://arxiv.org/abs/2405.00732</a>, <a class="reference external" href="https://arxiv.org/abs/2405.00732">arXiv:2405.00732</a>.</p>
</div>
<div class="citation" id="id179" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id40">HuggingFace4w</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Gguf quantization types. Online Documentation, 2024w. Documentation on different quantization types available for GGUF models. URL: <a class="reference external" href="https://huggingface.co/docs/hub/gguf#quantization-types">https://huggingface.co/docs/hub/gguf#quantization-types</a>.</p>
</div>
<div class="citation" id="id184" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id28">HuggingFace4xa</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Gguf models on huggingface. Online Repository, 2024x. Collection of models in GGUF format for efficient local inference. URL: <a class="reference external" href="https://huggingface.co/models?search=gguf">https://huggingface.co/models?search=gguf</a>.</p>
</div>
<div class="citation" id="id159" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id34">HuggingFace4xb</a><span class="fn-bracket">]</span></span>
<p>HuggingFace. Llamafile models on huggingface. Online Repository, 2024x. Collection of models compatible with Mozilla's llamafile format. URL: <a class="reference external" href="https://huggingface.co/models?library=llamafile">https://huggingface.co/models?library=llamafile</a>.</p>
</div>
<div class="citation" id="id183" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">IBMThink24</a><span class="fn-bracket">]</span></span>
<p>IBM Think. Gguf vs ggml: what's the difference? 2024. Comparison of GGUF and GGML model formats. URL: <a class="reference external" href="https://www.ibm.com/think/topics/gguf-versus-ggml">https://www.ibm.com/think/topics/gguf-versus-ggml</a>.</p>
</div>
<div class="citation" id="id157" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id35">LMStudio24</a><span class="fn-bracket">]</span></span>
<p>LM Studio. Lm studio - discover, download, and run local llms. Website, 2024. Desktop application for discovering, downloading and running local language models. URL: <a class="reference external" href="https://lmstudio.ai/">https://lmstudio.ai/</a>.</p>
</div>
<div class="citation" id="id162" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">MetaAI4c</a><span class="fn-bracket">]</span></span>
<p>Meta AI. Llama-2-70b-chat-hf. HuggingFace Model, 2024c. 70 billion parameter chat model from Meta's Llama 2 family. URL: <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-70b-chat-hf">https://huggingface.co/meta-llama/Llama-2-70b-chat-hf</a>.</p>
</div>
<div class="citation" id="id158" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id33">MozillaOcho24</a><span class="fn-bracket">]</span></span>
<p>Mozilla Ocho. Llamafile: distribute and run llms with a single file. GitHub Repository, 2024. Tool for packaging and distributing LLMs as self-contained executables. URL: <a class="reference external" href="https://github.com/Mozilla-Ocho/llamafile">https://github.com/Mozilla-Ocho/llamafile</a>.</p>
</div>
<div class="citation" id="id169" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id39">Salesforce24</a><span class="fn-bracket">]</span></span>
<p>Salesforce. Wikitext dataset. HuggingFace Dataset, 2024. Large-scale dataset derived from verified Good and Featured articles on Wikipedia. URL: <a class="reference external" href="https://huggingface.co/datasets/Salesforce/wikitext">https://huggingface.co/datasets/Salesforce/wikitext</a>.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

          </div>
          <div class="page-nav">
            <div class="inner"><ul class="page-nav">
  <li class="prev">
    <a href="alignment.html"
       title="previous chapter">← <span class="section-number">7. </span>Preference-Based Alignment</a>
  </li>
  <li class="next">
    <a href="cost.html"
       title="next chapter"><span class="section-number">9. </span>The Falling Cost Paradox →</a>
  </li>
</ul><div class="footer" role="contentinfo">
      &#169; Copyright Tharsis T. P. Souza, 2024.
    <br>
    Created using <a href="http://sphinx-doc.org/">Sphinx</a> 6.2.1 with <a href="https://github.com/schettino72/sphinx_press_theme">Press Theme</a> 0.9.1.
</div>
            </div>
          </div>
      </page>
    </div></div>
    
    
  </body>
</html>